<!DOCTYPE html>
<html lang="pt-br">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="../../../site/style.css">
    <title>Neural Networks and Learning Machines / Introdu√ß√£o</title>
</head>

<header class="breadcrumb" id="top">
    <nav>
        <a href="../../../index.html">Voltar √† p√°gina inicial</a>
        <br/>
        <p>Networks and Learning Machines | 3<sup>a</sup> Edi√ß√£o</p>
        <p>Simon Haykin</p>
    </nav>
</header>

<body>
    <main>
        <h1># 1.2</h1>
        <p><img src="https://img.shields.io/badge/Fase-Escrevendo-grey?labelColor=5F9EA0"></p>
        
        <h2>Introdu√ß√£o (p. 1/46)</h2>

        <article>
            <h3>Defini√ß√£o de rede neural [1]</h3>
            <p> Desde o in√≠cio, o estudo das redes neurais artificiais foi pautado pela observa√ß√£o de que tanto o c√©rebro humano quanto os computadores convencionais s√£o sistemas de processamento de informa√ß√µes[2] e, por conseguinte, realizam trabalho computacional[3]. N√£o apenas seu funcionamento √© bastante distinto, mas a capacidade computacional do c√©rebro - n√£o s√≥ humano, mas tamb√©m o de outros animais - supera, em muito, a dos computadores digitais.</p>
            <p>O c√©rebro √© um sistema de processamento de informa√ß√µes complexo, n√£o linear e paralelo[4]. Suas estruturas fundamentais s√£o os neur√¥nios, organizados de modo a realizar tarefas computacionais, a exemplo do reconhecimento de padr√µes. Portanto, assim no c√©rebro como nas redes artificiais, os neur√¥nios constituem as unidades de processamento da informa√ß√£o.</p>
            <p>A plasticidade[5], caracter√≠stica que permite a adapta√ß√£o do indiv√≠duo ao ambiente em que est√° inserido, tamb√©m √© importante para as redes neurais artificiais.</p>
            <p>Em linhas gerais, pode-se dizer que uma rede neural artificial √© um modelo computacional inspirado no modo como o c√©rebro realiza o processamento de informa√ß√µes. Nas palavras de Haykin (2009, p. 2), ‚Äú[‚Ä¶] a neural network is a machine that is designed to model the way in which the brain performs a particular task or function of interest‚Äù.</p>
            <p>J√° como defini√ß√£o formal, o autor d√° √†s redes neurais, vistas como uma m√°quina adaptativa[6], o seguinte conceito:</p>
            <blockquote>
            <p>A neural network is a massively parallel distributed processor made up of simple processing units that has a natural propensity for storing experiential knowledge and making it available for use. It resembles the brain in two respects:</p>
            <ol>
            <li>Knowledge is acquired by the network from its environment through a learning process.</li>
            <li>Interneuron connection strengths, known as synaptic weights, are used to store the ac-<br>
            quired knowledge. (HAYKIN, 2009, p. 2)</li>
            </ol>
            </blockquote>
            <p>Em portugu√™s, conforme defini√ß√£o dada na edi√ß√£o anterior do livro:</p>
            <blockquote>
            <p>Uma rede neural √© um processador maci√ßamente paralelamente distribu√≠do constitu√≠do de unidades de processamento simples, que tem uma propens√£o natural para armazenar conhecimento experimental e torn√°-lo dispon√≠vel para uso. Ela se assemelha ao c√©rebro em dois aspectos:</p>
            <ol>
            <li>O conhecimento √© adquirido pela rede a partir de seu ambiente atrav√©s de um processo de aprendizagem.</li>
            <li>For√ßas de conex√£o entre os neur√¥nios, conhecidas como pesos sin√°pticos, s√£o utilizadas para armazenar o conhecimento adquirido. (HAYKIN, 2001, p. 28)</li>
            </ol>
            </blockquote>
            <p>Assim, na medida do poss√≠vel, as redes neurais artificiais assemelham-se e s√£o modeladas √† luz do c√©rebro humano [interconex√£o de unidades computacionais simples - neur√¥nios - assim no c√©rebro biol√≥gico, como nas redes neurais artificiais].</p>
            <p>O processo de aprendizagem - <strong>algoritmo de aprendizagem</strong> - tradicionalmente aplicado em redes neurais consiste na <strong>modifica√ß√£o dos pesos sin√°pticos</strong> das conex√µes neuronais e deve necessariamente resultar em <strong>atividade computacional √∫til</strong>, ou seja, o alcance do objetivo almejado.</p>
            <p>A rede neural pode alterar sua pr√≥pria estrutura (topologia).</p>
            <p>Conforme o autor, a t√©cnica de modifica√ß√£o dos pesos sin√°pticos guarda muita similaridade com a teoria dos filtros adaptativos lineares (<em>linear adaptive filter theory</em>) [7].</p>
            
            <h4>Benef√≠cios das redes neurais</h4>
            <p>A capacidade computacional das redes neurais √© resultado de sua estrutura massiva e paralelamente distribu√≠da, bem como da habilidade de aprender e generalizar. ‚ÄúA generaliza√ß√£o se refere ao fato de a rede neural produzir sa√≠das adequadas para entradas que n√£o estavam presentes durante o treinamento (aprendizagem). Estas duas capacidades de processamento de informa√ß√£o [aprender e generalizar] tornam poss√≠vel para as redes neurais resolver problemas complexos (de grande escala) que s√£o atualmente intrat√°veis.‚Äù (HAYKIN, 2001, p. 28) [da 2 edi√ß√£o, em portugu√™s].</p>
            <blockquote>
            <p>A generaliza√ß√£o se refere ao fato de a rede neural produzir sa√≠das adequadas para entradas que n√£o estavam presentes durante o treinamento (aprendizagem) (HAYKIN, 2001, p. 28).</p>
            </blockquote>
            <blockquote>
            <p>Generalization refers to the neural network‚Äôs production of reasonable outputs for inputs not encountered during training (learning) (HAYKIN, 2009, p. 2).</p>
            </blockquote>
            <p>S√£o produzidas sa√≠das (<em>outputs</em>) adequadas para as entradas (<em>inputs</em>) fornecidas - <em>good approximate solutions</em>, do original em ingl√™s (Haykin, 2009). Atualmente, o problema grande e complexo deve ser decomposto em problemas menores, os quais redes neurais de prop√≥sito espec√≠fico t√™m capacidade de resolver.</p>
            <p>Principais capacidades e propriedades das redes neurais artificiais (p. 2/6):</p>
            <ul>
            <li>N√£o linearidade (embora individualmente os neur√¥nios artificiais possam ser lineares ou n√£o lineares);</li>
            <li>Mapeamento de entrada e sa√≠da (na aprendizagem supervisionada);</li>
            <li>Adaptabilidade (capacidade de modificar os pesos sin√°pticos);
            <ul>
            <li><strong>Dilema estabilidade x plasticidade:</strong> ‚ÄúPara aproveitar todos os benef√≠cios da adaptabilidade, as constantes de tempo principais do sistema devem ser grandes o suficiente para que o sistema ignore perturba√ß√µes esp√∫rias mas ainda assim serem suficientemente pequenas para responder a mudan√ßas significativas no ambiente‚Äù (HAYKIN, 2001, p. 30).</li>
            </ul>
            </li>
            <li>Resposta a evid√™ncias (n√£o apenas classificar o padr√£o adequadamente, mas informar o n√≠vel de confiabilidade da escolha, rejeitando ambiguidade);</li>
            <li>Informa√ß√£o contextualizada (‚ÄúO conhecimento √© representado pela pr√≥pria estrutura e estado de ativa√ß√£o de uma rede neural.‚Äù (HAYKIN, 2001, p. 30));</li>
            <li>Toler√¢ncia a falhas (devido √† sua estrutura massiva e paralelamente distribu√≠da);</li>
            <li>Implementa√ß√£o em VSLI (<em>very-large-scale-integration</em>) [8];</li>
            <li>Uniformidade de an√°lise e projeto (&quot;[‚Ä¶] as redes neurais desfrutam de universalidade como processadores de informa√ß√£o&quot; (HAYKIN, 2001, p. 30));</li>
            <li>Analogia neurobiol√≥gica (motivadas por estruturas biol√≥gicas do c√©rebro humano).</li>
            </ul>
        </article>

        <article>
            <h3>O c√©rebro humano[9]</h3>
            <p>O sistema nervoso humano pode ser visto como um sistema de tr√™s est√°gios, do qual fazem parte c√©rebro, receptores e atuadores. Os est√≠mulos - entrada (<em>input</em>) do sistema - s√£o captados e convertidos em sinais el√©tricos pelos receptores; o c√©rebro continuamente recebe e processa esses sinais; os atuadores convertem os sinais recebidos de modo a gerar as a√ß√µes (respostas) apropriadas, que constituem a sa√≠da (<em>output</em>) do sistema. A informa√ß√£o captada √© transmitida em um √∫nico sentido - receptor &gt; c√©rebro &gt; atuador -, mas o sistema se retroalimenta no sentido oposto dessa transmiss√£o - atuador &gt; c√©rebro &gt; receptor (<em>feedback</em>).</p>
            <p>Foi o trabalho de Santiago Ram√≥n y Cajal que, em 1911, introduziu o neur√¥nio como estrutura fundamental do c√©rebro. Os neur√¥nios s√£o cinco a seis ordens de grandeza mais lentos do que os circuitos digitais e, ainda assim, o c√©rebro √© muito mais eficiente do que computadores em termos de efici√™ncia energ√©tica. Provavelmente, isso √© resultado da enorme quantidade de c√©lulas neuronais e √† massiva interconex√£o entre elas - a efici√™ncia energ√©tica do c√©rebro humano √© de $10^{-16}$ joules por opera√ß√£o por segundo. Eles se apresentam sob diversas formas e tamanhos, sendo que um dos mais comuns √© a denominada c√©lula piramidal, e se organizam na anatomia cerebral em diversos n√≠veis e em pequena ou grande escala.</p>
            <p>Essa estrutura hier√°rquica vai, em menor n√≠vel, das mol√©culas respons√°veis pelas sinapses ao pr√≥prio sistema nervoso central, √∫ltimo n√≠vel hier√°rquico. A estrutura completa √© apresentada por Haykin (2009, p. 9), na figura 3, da seguinte forma: mol√©culas &gt; sinapses &gt; microcircuitos neurais &gt; √°rvore dendr√≠tica &gt; neur√¥nio &gt; circuitos locais &gt; circuitos regionais &gt; sistema nervoso central.</p>
            <blockquote>
            <p>The synapses represent the most fundamental level, depending on molecules and ions for their action. [‚Ä¶] A neural microcircuit refers to an assembly of synapses organized into patterns of connectivity to produce a functional operation of interest. A neural microcircuit may be likened to a silicon chip made up of an assembly of transistors. At the next level of complexity, we have local circuits [‚Ä¶] made up of neurons with similar or different properties; there neural assemblies perform operations characteristic of a localized region in the brain. They are followed by interregional circuits made up of pathways, columns, and topographic maps, which involve multiple regions located in different parts of the brain.<br>
            Topographic maps are organized to respond to incoming sensory information. These maps are often arranged in sheets, [‚Ä¶] stacked in adjacent layers in such a way that stimuli from corresponding points in space lie above or below each other. [‚Ä¶] different sensory inputs [‚Ä¶] are mapped onto corresponding areas of the cerebral cortex in an orderly fashion. At the final level of complexity, the topographic maps and other interregional circuits mediate specific types of behavior in the central nervous system. (HAYKIN, 2009, p. 7/9)</p>
            </blockquote>
            <p>O equivalente, na 2 edi√ß√£o, em portugu√™s:</p>
            <blockquote>
            <p>As sinapses representam o n√≠vel mais fundamental, dependente de mol√©culas e √≠ons para sua a√ß√£o. [‚Ä¶] Um microcircuito neural se refere a um agrupamento de sinapses organizadas em padr√µes de conectividade para produzir uma opera√ß√£o funcional de interesse. Um microcircuito neural pode ser comparado a um circuito de sil√≠cio constitu√≠do por um agrupamento de transistores. [‚Ä¶] No n√≠vel seguinte de complexidade n√≥s temos circuitos locais [‚Ä¶] constitu√≠dos por neur√¥nios com propriedades similares ou diferentes; estes agrupamentos neurais realizam opera√ß√µes caracter√≠sticas de uma regi√£o localizada no c√©rebro. Eles s√£o seguidos por circuitos inter-regionais constitu√≠dos por caminhos, colunas e mapas topogr√°ficos, que envolvem regi√µes m√∫ltiplas localizadas em partes diferentes do c√©rebro.<br>
            Os mapas topogr√°ficos s√£o organizados para responder √† informa√ß√£o sensorial incidente. Estes mapas s√£o frequentemente arranjados em folhas, [‚Ä¶] empilhados em camadas adjacentes de tal modo que est√≠mulos advindos de pontos correspondentes no espa√ßo se localizem acima ou abaixo de cada um deles. [‚Ä¶] diferentes entradas sensoriais [‚Ä¶] s√£o mapeadas sobre √°reas correspondentes do c√≥rtex cerebral de forma ordenada. No n√≠vel final de complexidade, os mapas topogr√°ficos e outros circuitos inter-regionais medeiam tipos espec√≠ficos de comportamento no sistema nervoso central. (HAYKIN, 2001, p. 33/36)</p>
            </blockquote>
            <p>As sinapses s√£o unidades estruturais e funcionais elementares respons√°veis por intermediar a comunica√ß√£o entre neur√¥nios[10] e, em maioria, s√£o qu√≠micas. Nelas, um sinal el√©trico pr√©-sin√°ptico √© transformado em sinal qu√≠mico pela libera√ß√£o de neurotransmissores, e depois se converte novamente em sinal el√©trico, p√≥s-sin√°ptico (Shepherd; Koch, 1990 <em>apud</em> Haykin, 2009). ‚ÄúNas descri√ß√µes tradicionais da organiza√ß√£o neural, assume-se que uma sinapse √© uma conex√£o simples que pode impor ao neur√¥nio receptivo excita√ß√£o ou inibi√ß√£o, mas n√£o ambas.‚Äù (HAYKIN, 2001, p. 33). <strong>O surgimento de novas sinapses ou a modifica√ß√£o das j√° existentes s√£o os mecanismos respons√°veis pela plasticidade cerebral e, consequentemente, pela aprendizagem.</strong></p>
            <p>A sa√≠da (<em>output</em>) do processamento neuronal s√£o, no mais das vezes, impulsos el√©tricos denominados <strong>potenciais de a√ß√£o</strong>[11] ou <strong><em>spikes</em></strong> - na 2 edi√ß√£o, em portugu√™s, o termo √© traduzido como <strong>impulso</strong>.</p>
        </article>

        <article>
            <h3>Modelos de neur√¥nio [artificial]</h3>
            <p>Em sua forma mais rudimentar, o modelo de um neur√¥nio artificial possui tr√™s elementos b√°sicos: um <strong>conjunto de sinapses ou elos de liga√ß√£o (<em>synapses or connecting links</em>)</strong>, cada um com seu pr√≥prio peso ou for√ßa (peso sin√°ptico/<em>synaptic weight</em>) [12]; um <strong>somador (<em>adder</em>)</strong>; e uma <strong>fun√ß√£o de ativa√ß√£o (<em>activation function</em>)</strong>.</p>
            <p>Eis a ilustra√ß√£o do modelo (HAYKIN, 2009, p. 11):</p>
            <img class="figura" src="../../../fichamentos/neural-networks-and-learning-machines-simon-haykin/images/03_neuronio_artificial_nao_linear_basico.png" alt="Modelo de neur√¥nio artificial">
            <p>Depreende-se que um sinal de <strong>entrada</strong> - representado pela letra $x$ - √© direcionado √† sinapse - representada pela letra $j$ -, que est√° conectada ao neur√¥nio - representado pela letra $k$ - e que, por sua vez, possui um peso sin√°ptico - representado pela letra $w$. Noutras palavras, considerados quaisquer √≠ndices, diz-se que ‚Äú[‚Ä¶] um sinal $x_j$ na entrada da sinapse $j$ conectada ao neur√¥nio $k$ √© multiplicada pelo peso sin√°ptico $w_{kj}$. [‚Ä¶] O primeiro √≠ndice se refere ao neur√¥nio em quest√£o e o segundo se refere ao terminal de entrada da sinapse √† qual o peso se refere.‚Äù (HAYKIN, 2001, p. 36).</p>
            <p>O somat√≥rio dos sinais de entrada ponderados pelos pesos sin√°pticos de cada neur√¥nio pode, ou n√£o, executar a fun√ß√£o de ativa√ß√£o, que visa ‚Äúrestringir a amplitude da sa√≠da de um neur√¥nio. A fun√ß√£o de ativa√ß√£o √© tamb√©m referida como <em>fun√ß√£o restritiva</em> j√° que restringe (limita) o intervalo permiss√≠vel de amplitude do sinal de sa√≠da a um valor finito. Tipicamente, o intervalo normalizado da amplitude da sa√≠da de um neur√¥nio √© escrito como o intervalo unit√°rio fechado [0, 1] ou alternativamente [-1, 1]‚Äù (HAYKIN, 2001, p. 37). Na terminologia em ingl√™s, a dita fun√ß√£o restritiva √© denominada <em>squashing function</em> (HAYKIN, 2009, p. 10).</p>
            <p>O resultado - sa√≠da/<em>output</em> - do c√°lculo realizado pelo somador (na figura, <em>summing junction</em>) descreve um <strong>combinador linear (<em>linear combiner</em>)</strong>, representado pela letra $u$ (n√£o aparece no modelo acima), que consiste na soma dos valores da entrada do sistema multiplicada pelo peso sin√°ptico do neur√¥nio.</p>
            <p>Pode ser considerado no c√°lculo o <strong><em>bias</em></strong> (<strong>vi√©s</strong>, em tradu√ß√£o literal), que ‚Äútem o efeito de aumentar ou diminuir a entrada l√≠quida da fun√ß√£o de ativa√ß√£o, dependendo se ele √© positivo ou negativo, respectivamente‚Äù (HAYKIN, 2001, p. 37). No modelo acima, √© representado por $b_k$, do que se infere que <strong>cada neur√¥nio pode ter um vi√©s espec√≠fico</strong>. Tem-se, ainda, que o <em>bias</em> aplica uma <strong>transforma√ß√£o afim</strong> (<em>affine transformation</em>, em ingl√™s) <strong>√† sa√≠da do somador, que equivale √† entrada l√≠quida da fun√ß√£o de ativa√ß√£o.</strong></p>
            <p>A sa√≠da do combinador linear - $u_k$ -, ap√≥s a incid√™ncia do <em>bias</em> - $b_k$ -, se for o caso, √© a <strong>entrada l√≠quida (<em>net input</em>) da fun√ß√£o de ativa√ß√£o</strong>. Esta, por sua vez, √© representada na imagem por $\phi(\cdot)$ e sua aplica√ß√£o resulta na sa√≠da (<em>output</em>) do pr√≥prio neur√¥nio, representada pela letra $y$.</p>
            <p>Portanto, <strong>para o neur√¥nio $k$, sua sa√≠da √© dada por $y_k = \phi(u_k + b_k)$, cujos par√¢metros s√£o a sa√≠da do combinador linear ($u_k$) e o <em>bias</em> do neur√¥nio ($b_k$).</strong></p>
            <p>At√© aqui, a representa√ß√£o matem√°tica do neur√¥nio $k$ √© dada por:</p>
            <p>$$<br>
            u_k = \sum_{j=1}^{m} w_{kj}x_j<br>
            $$</p>
            <p>" ">e</p>
            <p>" ">$$<br>
            y_k = \phi(u_k + b_k)<br>
            $$</p>
            <p>A equa√ß√£o</p>
            <p>$$<br>
            v_k = u_k + b_k<br>
            $$</p>
            <p>mostra que o <strong>campo local induzido (<em>induced local field</em>) ou potencial de ativa√ß√£o (<em>activation potential</em>)</strong>[13], representado pela letra $v$, do neur√¥nio $k$ tem correla√ß√£o com a sa√≠da do combinador linear - $u_k$ - e o <em>bias</em> - $b_k$, de modo a deslocar o gr√°fico da equa√ß√£o a depender se o valor do <em>bias</em> √© positivo ou negativo.</p>
            
            <h4>Tipos de fun√ß√µes de ativa√ß√£o</h4>
            <h5>Fun√ß√£o de limiar ou limite (<em>threshold function</em>)</h5>
            <p>√â empregada no modelo de McCulloch-Pitts, no qual ‚Äúa sa√≠da de um neur√¥nio assume o valor de 1, se o campo local induzido daquele neur√¥nio √© n√£o-negativo, e 0 caso contr√°rio.‚Äù (HAYKIN, 2001, p. 39). √â um modelo determin√≠stico.</p>
            <p>Em nota√ß√£o matem√°tica, tem-se que</p>
            <p>$$<br>
            \phi(v) = \left{<br>
            \begin{array}{ll}<br>
            1, &amp; \text{se } v \geq 0 \<br>
            0, &amp; \text{se } v &lt; 0<br>
            \end{array}<br>
            \right.<br>
            $$</p>
            <p>, e para a sa√≠da $y_k$ do neur√¥nio</p>
            <p>$$<br>
            y(k) = \left{<br>
            \begin{array}{ll}<br>
            1, &amp; \text{se } v \geq 0 \<br>
            0, &amp; \text{se } v &lt; 0<br>
            \end{array}<br>
            \right.<br>
            $$</p>
            <p>Para esse neur√¥nio, o campo local induzido ou potencial de ativa√ß√£o √© dado por</p>
            <p>$$<br>
            v_k = \sum_{j=1}^{m} w_{kj}x_j + b_k<br>
            $$</p>

            <h5>Fun√ß√£o sigmoide</h5>
            <p>√â a mais comumente utilizada na constru√ß√£o de redes neurais. ‚ÄúEla √© definida como uma fun√ß√£o estritamente crescente que exige um balanceamento adequado entre comportamento linear e n√£o-linear‚Äù (HAYKIN, 2001, p. 40), de modo que pode assumir valores cont√≠nuos no intervalo $[0, 1]$. Essa fun√ß√£o √© diferenci√°vel.</p>
            <p>Nos casos em que seja interessante que os valores da fun√ß√£o de ativa√ß√£o se estenda de $-1$ a $+1$, em vez de $0$ a $+1$, tem-se a denominada <strong>fun√ß√£o sinal (<em>signum function</em>)</strong>, que √© definida por</p>
            <p>$$<br>
            \phi(v) = \left{<br>
            \begin{array}{}<br>
            1, &amp; \text{se } v &gt; 0 \<br>
            0, &amp; \text{se } v = 0 \<br>
            -1, &amp; \text{se } v &lt; 0<br>
            \end{array}<br>
            \right.<br>
            $$</p>
            <p>, e sua fun√ß√£o sigmoide (fun√ß√£o tangente hiperb√≥lica) √© dada por</p>
            <p>$$<br>
            \phi(v) = tanh(v)<br>
            $$</p>

            <h4>Modelo estoc√°stico de neur√¥nio</h4>
            <p>Adi√ß√£o de uma vari√°vel aleat√≥ria ao modelo de McCulloch-Pitts, que √© determin√≠stico, de modo a torn√°-lo estoc√°stico, isto √©, probabil√≠stico. Isso porque um modelo estoc√°stico de neur√¥nio tenta predizer poss√≠veis resultados (sa√≠das) levando em considera√ß√£o a exist√™ncia de um ou mais par√¢metros vari√°veis ao longo do tempo. Ao contr√°rio, em um modelo determin√≠stico, as sa√≠das devem ser sempre as mesmas para os mesmos valores de entrada.</p>
            <p>Se $x$ representar o estado de um neur√¥nio, $v$ o seu potencial de ativa√ß√£o e $P(v)$ a probabilidade de disparo, isto √©, de que ocorra mudan√ßa de estado, tem-se que</p>
            <p>$$<br>
            x = \left{<br>
            \begin{array}{ll}<br>
            +1 &amp; \text{com probabilidade } P(v) \<br>
            -1 &amp; \text{com probabilidade } 1 - P(v)<br>
            \end{array}<br>
            \right.<br>
            $$</p>
            <p>, e</p>
            <p>$$<br>
            P(v) = \frac{1}{1 + exp(-v/T)}<br>
            $$</p>
            <p>, em que $T$ √© a dita <strong><em>pseudotemperatura</em></strong>, ‚Äú[‚Ä¶] utilizada para controlar o n√≠vel de ru√≠do e portanto a incerteza de disparar [‚Ä¶] como um par√¢metro que controle as flutua√ß√µes t√©rmicas que representam os efeitos do ru√≠do sin√°ptico. Note que quanto $T \rightarrow 0$, o neur√¥nio estoc√°stico [‚Ä¶] se reduz a uma forma sem ru√≠do (i.e., determin√≠stica), que √© o modelo de McCulloch-Pitts.‚Äù (HAYKIN, 2001, p. 41).</p>
        </article>

        <article>
            <h3>Redes neurais como grafos dirigidos</h3>
            <h3>Feedback</h3>
            <h3>Arquiteturas de redes neurais</h3>
            <h3>Representa√ß√£o do conhecimento</h3>
            <h4>Regras</h4>
            <h4>Informa√ß√£o pr√©via no projeto de uma rede neural</h4>
            <h4>Invari√¢ncias no projeto de uma rede neural</h4>
            <h3>Processos de aprendizagem</h3>
            <h3>Tipos de aprendizagem</h3>
        </article>

        <hr>

        <article>
            <h2>Principais conceitos/defini√ß√µes/ideias extra√≠dos do texto original</h2>
            <ul>
                <li><p><strong>Defini√ß√£o de rede neural</strong></p>
                    <ul>
                        <li>O c√©rebro √© um sistema de processamento de informa√ß√µes complexo, n√£o linear e paralelo, com capacidade computacional muito superior √† dos computadores digitais convencionais.</li>
                        <li>As redes neurais artificiais s√£o inspiradas no c√©rebro humano e a ele se assemelham na medida em que compostas por neur√¥nios artificiais interconectados.</li>
                        <li>A plasticidade √© uma caracter√≠stica importante tanto no c√©rebro quanto nas redes neurais artificiais e √© ela que assegura a adapta√ß√£o do indiv√≠duo ao ambiente e a capacidade de aprender.</li>
                        <li>Em redes biol√≥gicas ou artificiais, os neur√¥nios s√£o a unidade de processamento de informa√ß√£o e, portanto, respons√°veis pelo trabalho computacional.</li>
                        <li>Uma rede neural artificial modela o modo como o c√©rebro biol√≥gico realiza determinada tarefa ou fun√ß√£o.</li>
                        <li>O processo de aprendizagem tradicional consiste na modifica√ß√£o dos pesos sin√°pticos da rede neural artificial.</li>
                    </ul>

                    <li><p><strong>Benef√≠cios das redes neurais</strong></p>
                        <ul>
                            <li>Massiva e paralelamente distribu√≠da.</li>
                            <li>Habilidade de aprender e generalizar.</li>
                            <li>Produz sa√≠das (<em>outputs</em>) adequadas para as entradas (<em>inputs</em>) fornecidas.</li>
                            <li>Principais caracter√≠sticas:
                                <ul>
                                    <li>N√£o linearidade (embora individualmente os neur√¥nios artificiais possam ser lineares ou n√£o lineares);</li>
                                    <li>Mapeamento de entrada e sa√≠da (na aprendizagem supervisionada);</li>
                                    <li>Adaptabilidade (capacidade de modificar os pesos sin√°pticos);</li>
                                    <li>Resposta a evid√™ncias (n√£o apenas classificar o padr√£o adequadamente, mas informar o n√≠vel de confiabilidade da escolha, rejeitando ambiguidade);</li>
                                    <li>Informa√ß√£o contextualizada (‚ÄúO conhecimento √© representado pela pr√≥pria estrutura e estado de ativa√ß√£o de uma rede neural.‚Äù (HAYKIN, 2001, p. 30));</li>
                                    <li>Toler√¢ncia a falhas (devido √† sua estrutura massiva e paralelamente distribu√≠da);</li>
                                    <li>Implementa√ß√£o em VSLI (<em>very-large-scale-integration</em>) [6];</li>
                                    <li>Uniformidade de an√°lise e projeto (&quot;[‚Ä¶] as redes neurais desfrutam de universalidade como processadores de informa√ß√£o&quot; (HAYKIN, 2001, p. 30));</li>
                                    <li>Analogia neurobiol√≥gica (motivadas por estruturas biol√≥gicas do c√©rebro humano).</li>
                                </ul>
                            </li>
                        </ul>
                    </li>
                
                <li><p><strong>O c√©rebro humano</strong></p>
                    <ul>
                        <li>Uma das formas de se enxergar o sistema nervoso do ser humano √© como um sistema de tr√™s est√°gios composto por c√©rebro, receptores e atuadores. Os sinais de entrada (<em>input</em>) desse sistema s√£o os est√≠mulos externos e/ou internos que s√£o captados pelos receptores e, submetidos ao dito sistema, tem-se como sa√≠da (<em>output</em>) alguma resposta que pode variar desde a mera percep√ß√£o a um movimento corporal.</li>
                        <li>Os neur√¥nios, estruturas fundamentais do c√©rebro humano, comunicam-se entre si por meio do processo denominado transmiss√£o sin√°ptica. A sinapse √© o local onde essa comunica√ß√£o ocorre e pode ser, basicamente, el√©trica ou qu√≠mica.
                            <ul>
                                <li>O surgimento de novas conex√µes entre os neur√¥nios ou a modifica√ß√£o das j√° existentes s√£o os mecanismos respons√°veis pela plasticidade cerebral e, consequentemente, pela aprendizagem.</li>
                                <li>O potencial de a√ß√£o (<em>spike</em> ou impulso) √© a sa√≠da (<em>output</em>) do processamento neuronal.</li>
                            </ul>
                        </li>
                        <li>Anatomicamente, o c√©rebro se organiza em n√≠veis hier√°rquicos.</li>
                        <li>Regi√µes espec√≠ficas do c√©rebro s√£o respons√°veis por determinadas fun√ß√µes, mas n√£o necessariamente de forma isolada, pois h√° casos em que um √∫nico resultado decorre da ativa√ß√£o de mais de uma regi√£o cerebral, que interagem entre si para produzi-lo.
                            <ul>
                                <li>Isso ocorre porque o c√©rebro √© vastamente interconectado.</li>
                            </ul>
                        </li>
                    </ul>
                </li>

                <li><p><strong>Modelos de neur√¥nio</strong></p>
                    <ul>
                        <li>Elementos b√°sicos: um conjunto de sinapses ou elos de liga√ß√£o (<em>synapses or connecting links</em>), cada um com seu pr√≥prio peso ou for√ßa; um somador (<em>adder</em>); e uma fun√ß√£o de ativa√ß√£o (<em>activation function</em>). Pode ser aplicado outro elemento chamado vi√©s (<em>bias</em>), igualmente relativo a cada neur√¥nio.
                            <ul>
                                <li>Cada neur√¥nio pode ter seu pr√≥prio <em>bias</em>.</li>
                                <li>Cada neur√¥nio tem o seu campo local induzido (<em>induced local field</em>) ou potencial de ativa√ß√£o (<em>activation potential</em>), que √© correlacionado com a sa√≠da do combinador linear e o <em>bias</em>.</li>
                            </ul>
                        </li>

                        <li><strong>Tipos de fun√ß√£o de ativa√ß√£o</strong>
                            <ul>
                                <li>Fun√ß√£o de limiar ou limite (<em>threshold function</em>)
                                    <ul>
                                    <li>Modelo de McCulloch-Pitts</li>
                                    <li>Determin√≠stico</li>
                                    </ul>
                                </li>
                                <li>Fun√ß√£o sigmoide
                                    <ul>
                                        <li>Mais utilizada na constru√ß√£o de redes neurais.</li>
                                    </ul>
                                </li>
                            </ul>
                        </li>

                        <li><strong>Modelo estoc√°stico de neur√¥nio</strong>
                            <ul>
                                <li>Probabil√≠stico</li>
                            </ul>
                        </li>
                    </ul>
                </li>
            </ul>
        </article>

        <hr>

        <article>
            <h2>Notas</h2>
            <p>[1] Uma rede neural artificial (ou apenas rede neural) √© um modelo preditivo baseado na din√¢mica do c√©rebro, que tem uma s√©rie de neur√¥nios conectados. Cada um deles analisa as sa√≠das dos outros neur√¥nios ligados nele, faz um c√°lculo e, em seguida, dispara (se o valor calculado exceder um limite) ou n√£o (se n√£o). Portanto, as redes neurais artificiais s√£o formadas por neur√¥nios artificiais que executam c√°lculos semelhantes a partir de entradas. As redes neurais resolvem uma ampla variedade de problemas, como reconhecimento de caracteres manuscritos e detec√ß√£o facial, e s√£o muito usadas no aprendizado profundo, um dos subcampos mais inovadores do data science. No entanto, a maioria das redes neurais s√£o ‚Äúcaixas-pretas‚Äù ‚Äî analisar seus detalhes n√£o explica como elas resolvem os problemas. Al√©m disso, √© dif√≠cil treinar grandes redes neurais. Para a maioria dos problemas t√≠picos do in√≠cio de carreira de um cientista de dados, elas n√£o s√£o a melhor op√ß√£o. (GRUS, 2021, p. 245)</p>
            <p>[2] Vide complemento #3 (dado, informa√ß√£o, conhecimento e compet√™ncias).</p>
            <p>[3] A palavra computar significa ‚Äúfazer o c√¥mputo de; calcular; or√ßar; contar; processar atrav√©s de computadores‚Äù. Fonte: <a href="https://dicionario.priberam.org/computar">Dicion√°rio Priberam da L√≠ngua Portuguesa</a>.</p>
            <p>[4] Sobre o <strong>processamento em paralelo</strong>, das Neuroci√™ncias colhe-se que, ao que parece, os comportamentos humanos de maior complexidade n√£o decorrem da sinaliza√ß√£o de um √∫nico neur√¥nio, mas pela a√ß√£o de muitos n√£o necessariamente localizados na mesma regi√£o cortical (Kandel et al., 2014).</p>
            <blockquote>
                <p>O envolvimento de v√°rios grupos neurais ou rotas para transmitir uma informa√ß√£o similar √© chamado <em>processamento em paralelo</em>. O processamento em paralelo tamb√©m ocorre em uma √∫nica via quando diferentes neur√¥nios nessa via executa a√ß√µes similares simultaneamente. [‚Ä¶] O campo da ci√™ncia computacional conhecido como intelig√™ncia artificial originalmente usou o processamento serial para simular os processos cognitivos do enc√©falo [‚Ä¶] Esses modelos seriais executavam muitas tarefas de forma adequada, inclusive jogar xadrez. Entretanto, eram muito ruims em outras tarefas que o enc√©falo faz quase instantaneamente, com o reconhecimento de faces ou a compreens√£o do discurso. [‚Ä¶] Nesses modelos [redes neurais], elementos do sistema processam a informa√ß√£o simultaneamente usando conex√µes de pr√≥-a√ß√£o e de retroalimenta√ß√£o. √â interessante observar que, em sistemas com circuitos de retroalimenta√ß√£o √© a atividade din√¢mica do sistema que determina o desfecho do processamento, n√£o as afer√™ncias ou condi√ß√µes iniciais. Modelos de redes neurais capturam bem a arquitetura altamente recorrente da maioria dos circuitos neurais reais e tamb√©m a capacidade do enc√©falo de funcionar na aus√™ncia de uma afer√™ncia sensorial espec√≠fica vinda de fora do corpo [‚Ä¶] Modelos de redes neurais tamb√©m demonstram que a an√°lise de elementos individuais de um sistema pode n√£o ser suficiente para decodificar a <em>mensagem dos potenciais de a√ß√£o</em>. De acordo com tal vis√£o de redes neurais, o que faz o enc√©falo ser um deslumbrante √≥rg√£o que processa a informa√ß√£o n√£o √© a complexidade de seus neur√¥nios, mas o fato de ter muitos elementos interconectados de v√°rias formas complexas.&quot; (KANDEL et al., 2014, p. 32/33)</p>
            </blockquote>
            <p>[5] Vide complemento #1 (plasticidade).</p>
            <p>[6] Nesse contexto, diz-se que uma rede neural √© uma <strong>m√°quina adaptativa</strong> exatamente pela capacidade aprendizado (autoajuste) e adapta√ß√£o a partir dos dados de entrada, o que permite o ajuste de par√¢metros (pesos sin√°pticos) para obter o melhor desempenho na tarefa.</p>
            <p>[7] Sobre <strong>filtros adaptativos (<em>adaptive filter</em>)</strong>:</p>
            <blockquote>
                <p>The term ‚Äúfilter‚Äù is often used to describe a device in the form of a piece of physical hardware or software that is applied to a set of noisy data in order to extract information about a prescribed quantity of interest. [‚Ä¶] In any event, we may use a filter to perform three basic information-processing tasks:</p>
                <ol>
                    <li><em>Filtering</em>, which means the extraction of information about a quantity of interest at a time $t$ by using data measured up to and including time $t$.</li>
                    <li><em>Smoothing</em>, which differs from filtering in that information about the quantity of interest need not to be available at time $t$, and data measured later than time $t$ can be used in obtaining this information. This means that in the case of smoothing there is a <em>delay</em> in producing the result of interest. Since in the smoothing process we are able to use data obtained not only up to time $t$ but also data obtained after time $t$, we would expect smoothing to be more accurate in some sense than filtering.</li>
                    <li><em>Prediction</em>, which is the forecasting side of information processing. The aim here is to derive information about what the quantity of interest will be like at some time ${t + T}$ in the future, for some ${T &gt; 0}$, by using data measured up to and including time $t$.<br> We may classify filters into linear and nonlinear. A filter is said to be <em>linear</em> if the filtered, smoothed, or predicted quantity at the output of the device is <em>a linear function of the observations applied to the filter input</em>. Otherwise, the filter is nonlinear. [‚Ä¶] By such a device [an adaptive filter] we mean one that is self-designing in that the adaptive filter relies for its operation on a recursive algorithm, which makes it possible for the filter to perform satisfactorily in an environment where complete knowledge of the relevant signal characteristics is not available. (HAYKIN, 1995, p. 1/3)</li>
                </ol>
            </blockquote>
            <p>[8] VLSI, do ingl√™s <em>Very Large Scale Integration</em>, √© um processo de fabrica√ß√£o de circuitos eletr√¥nicos integrados com alt√≠ssima quantidade de transistores em um √∫nico chip.</p>
            <p>[9] [11] Vide complemento <a href="">sistema nervoso, enc√©falo e neur√¥nio</a> para consulta mais detalhada sobre o assunto, a partir de refer√™ncias da literatura de Neuroci√™ncias.</p>
            <p>[10] A sinapse √© uma regi√£o espec√≠fica, ao passo que ao processo de comunica√ß√£o entre neur√¥nios d√°-se o nome de transmiss√£o sin√°ptica (Kandel et al., 2004). Nesse sentido, ‚Äúo local especializado em que um neur√¥nio se comunica com outro √© chamado de sinapse [‚Ä¶]‚Äù (KANDEL et al., 2014, p. 157).</p>
            <p>[12] Em se tratando de peso sin√°ptico, tamb√©m referido como for√ßa sin√°ptica, ‚Äú[‚Ä¶] muitos modelos neurais est√£o equipados com processos din√¢micos que [se]reorganizam continuamente [‚Ä¶] criam ou eliminam neur√¥nios ou suas conex√µes [‚Ä¶] ajustam as for√ßas de conex√µes sin√°pticas existentes ou mudam outras propriedades dos neur√¥nios. [‚Ä¶] O termo peso sin√°ptico frequentemente √© utilizado para se referir √† for√ßa de determinada conex√£o sin√°ptica, enquanto o termo matriz de pesos sin√°pticos aplica-se ao conjunto de todos os pesos sin√°pticos em uma rede. A for√ßa da sinapse do neur√¥nio $j$ sobre o neur√¥nio $i$ √© descrita como $W_{ij}$. Esse √© o elemento da matriz de pesos localizado na intersec√ß√£o da linha $i$ com a coluna $j$.‚Äù (KANDEL et al., 2014, p. 1387). O fen√¥meno da modifica√ß√£o dos pesos sin√°pticos √© consequ√™ncia da plasticidade - <strong>regra de plasticidade sin√°ptica</strong> -, que Kandel et al. (2014) argumenta que n√£o deve se confundida com a <strong>regra da aprendizagem</strong>, embora sejam comumente utilizadas como sin√¥nimas, pois aprendizado √© a ‚Äú[‚Ä¶] express√£o do comportamento de uma rede e n√£o de uma √∫nica sinapse‚Äù (KANDEL et al., 2014, p. 1387). Nesse sentido, a <strong>regra ou plasticidade <em>hebbiana</em></strong>, em alus√£o a Donald Hebb, ensina que ‚Äú[‚Ä¶] as sinapses s√£o modificadas com base na atividade temporalmente cont√≠gua dos neur√¥nios pr√© e p√≥s-sin√°pticos‚Äù (KANDEL et al., 2014, p. 1387). Conforme Bear et al. (, p. 878), ‚ÄúDonald Hebb prop√¥s que cada sinapse individual se torna um pouco mais forte quando participa com sucesso no disparo de um neur√¥nio p√≥s-sin√°ptico‚Äù, embora haja trabalhos no sentido de que a mudan√ßa do peso sin√°ptico esteja mais proximamente relacionada ao neur√¥nio pr√©-sin√°ptico, apenas (Gallinaro; Scholl; Clopath, 2023).</p>
            <p>[13] Em refer√™ncia ao potencial de a√ß√£o da c√©lula biol√≥gica.</p>
        </article>

        <hr>
        <article>
            <h2>Refer√™ncias complementares consultadas durante o fichamento deste cap√≠tulo</h2>
            <p>BEAR, Mark F.; CONNORS, Barry W.; PARADISO, Michael A. <strong>Neuroci√™ncias: desvendando o sistema nervoso</strong>. Trad. Carla Dalmaz et al. 4. ed. Porto Alegre: Artmed, 2017.</p>
            <p>GALLINARO, J√∫lia V.; SCHOLL, Benjamin; CLOPATH, Claudia. 2023. <strong>Synaptic weights that correlate with presynaptic selectivity increase decoding performance.</strong> PLOS Computational Biology 19(8): e1011362. <a href="https://doi.org/10.1371/journal.pcbi.1011362">https://doi.org/10.1371/journal.pcbi.1011362</a>.</p>
            <p>GRUS, Joel. <strong>Data science do zero: no√ß√µes fundamentais com Python.</strong> Trad. Welington Nascimento. 2 ed. E-book. Rio de Janeiro: Alta Books, 2021.</p>
            <p>HAYKIN, Simon. <strong>Adaptive filter theory.</strong> 3 ed. Upper Saddle River, NJ: Prentice Hall, 1995.</p>
            <p>KANDEL, Eric R.; SCHWARTZ, James H.; JESSELL, Thomas M.; SIEGELBAUM, Steven A.; HUDSPETH, A. J. <strong>Princ√≠pios de neuroci√™ncias</strong>. Trad. Ana L√∫cia Severo Rodrigues et al. 5. ed. Porto Alegre: AMGH, 2014.</p>
            <p>VERY LARGE SCALE INTEGRATION. In: WIKIPEDIA. Dispon√≠vel em <a href="https://en.wikipedia.org/wiki/Very_Large_Scale_Integration">https://en.wikipedia.org/wiki/Very_Large_Scale_Integration</a>. Acesso em 22 jan. 2024.</p>
        </article>

        <a class="emoji" href="#top">
            [ üîù ]
        </a>

    </main>
</body>

<footer>
    <p>Projeto NeuroBit</p>
        <p>
            <a class="social" href="https://github.com/paulorobertovrc" target="_blank">
                <svg width="24px" height="24px" viewBox="0 0 20 20" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" fill="#ffffff"><g id="SVGRepo_bgCarrier" stroke-width="0"></g><g id="SVGRepo_tracerCarrier" stroke-linecap="round" stroke-linejoin="round"></g><g id="SVGRepo_iconCarrier"> <title>github [#142]</title> <desc>Created with Sketch.</desc> <defs> </defs> <g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"> <g id="Dribbble-Light-Preview" transform="translate(-140.000000, -7559.000000)" fill="#fff"> <g id="icons" transform="translate(56.000000, 160.000000)"> <path d="M94,7399 C99.523,7399 104,7403.59 104,7409.253 C104,7413.782 101.138,7417.624 97.167,7418.981 C96.66,7419.082 96.48,7418.762 96.48,7418.489 C96.48,7418.151 96.492,7417.047 96.492,7415.675 C96.492,7414.719 96.172,7414.095 95.813,7413.777 C98.04,7413.523 100.38,7412.656 100.38,7408.718 C100.38,7407.598 99.992,7406.684 99.35,7405.966 C99.454,7405.707 99.797,7404.664 99.252,7403.252 C99.252,7403.252 98.414,7402.977 96.505,7404.303 C95.706,7404.076 94.85,7403.962 94,7403.958 C93.15,7403.962 92.295,7404.076 91.497,7404.303 C89.586,7402.977 88.746,7403.252 88.746,7403.252 C88.203,7404.664 88.546,7405.707 88.649,7405.966 C88.01,7406.684 87.619,7407.598 87.619,7408.718 C87.619,7412.646 89.954,7413.526 92.175,7413.785 C91.889,7414.041 91.63,7414.493 91.54,7415.156 C90.97,7415.418 89.522,7415.871 88.63,7414.304 C88.63,7414.304 88.101,7413.319 87.097,7413.247 C87.097,7413.247 86.122,7413.234 87.029,7413.87 C87.029,7413.87 87.684,7414.185 88.139,7415.37 C88.139,7415.37 88.726,7417.2 91.508,7416.58 C91.513,7417.437 91.522,7418.245 91.522,7418.489 C91.522,7418.76 91.338,7419.077 90.839,7418.982 C86.865,7417.627 84,7413.783 84,7409.253 C84,7403.59 88.478,7399 94,7399" id="github-[#142]"> </path> </g> </g> </g> </g></svg>
            </a>
            <a class="social" href="https://www.instagram.com/pauloroberto.dev" target="_blank">
                <svg width="24px" height="24px" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" stroke="#FFFFFF"><g id="SVGRepo_bgCarrier" stroke-width="0"></g><g id="SVGRepo_tracerCarrier" stroke-linecap="round" stroke-linejoin="round"></g><g id="SVGRepo_iconCarrier"> <path fill-rule="evenodd" clip-rule="evenodd" d="M12 18C15.3137 18 18 15.3137 18 12C18 8.68629 15.3137 6 12 6C8.68629 6 6 8.68629 6 12C6 15.3137 8.68629 18 12 18ZM12 16C14.2091 16 16 14.2091 16 12C16 9.79086 14.2091 8 12 8C9.79086 8 8 9.79086 8 12C8 14.2091 9.79086 16 12 16Z" fill="#FFFFFF"></path> <path d="M18 5C17.4477 5 17 5.44772 17 6C17 6.55228 17.4477 7 18 7C18.5523 7 19 6.55228 19 6C19 5.44772 18.5523 5 18 5Z" fill="#FFFFFF"></path> <path fill-rule="evenodd" clip-rule="evenodd" d="M1.65396 4.27606C1 5.55953 1 7.23969 1 10.6V13.4C1 16.7603 1 18.4405 1.65396 19.7239C2.2292 20.8529 3.14708 21.7708 4.27606 22.346C5.55953 23 7.23969 23 10.6 23H13.4C16.7603 23 18.4405 23 19.7239 22.346C20.8529 21.7708 21.7708 20.8529 22.346 19.7239C23 18.4405 23 16.7603 23 13.4V10.6C23 7.23969 23 5.55953 22.346 4.27606C21.7708 3.14708 20.8529 2.2292 19.7239 1.65396C18.4405 1 16.7603 1 13.4 1H10.6C7.23969 1 5.55953 1 4.27606 1.65396C3.14708 2.2292 2.2292 3.14708 1.65396 4.27606ZM13.4 3H10.6C8.88684 3 7.72225 3.00156 6.82208 3.0751C5.94524 3.14674 5.49684 3.27659 5.18404 3.43597C4.43139 3.81947 3.81947 4.43139 3.43597 5.18404C3.27659 5.49684 3.14674 5.94524 3.0751 6.82208C3.00156 7.72225 3 8.88684 3 10.6V13.4C3 15.1132 3.00156 16.2777 3.0751 17.1779C3.14674 18.0548 3.27659 18.5032 3.43597 18.816C3.81947 19.5686 4.43139 20.1805 5.18404 20.564C5.49684 20.7234 5.94524 20.8533 6.82208 20.9249C7.72225 20.9984 8.88684 21 10.6 21H13.4C15.1132 21 16.2777 20.9984 17.1779 20.9249C18.0548 20.8533 18.5032 20.7234 18.816 20.564C19.5686 20.1805 20.1805 19.5686 20.564 18.816C20.7234 18.5032 20.8533 18.0548 20.9249 17.1779C20.9984 16.2777 21 15.1132 21 13.4V10.6C21 8.88684 20.9984 7.72225 20.9249 6.82208C20.8533 5.94524 20.7234 5.49684 20.564 5.18404C20.1805 4.43139 19.5686 3.81947 18.816 3.43597C18.5032 3.27659 18.0548 3.14674 17.1779 3.0751C16.2777 3.00156 15.1132 3 13.4 3Z" fill="#FFFFFF"></path> </g></svg>
            </a>
            <a class="social" href="https://www.linkedin.com/in/paulorobertovrc" target="_blank">
                <svg width="27px" height="27px" viewBox="0 0 21 21" fill="none" xmlns="http://www.w3.org/2000/svg"><g id="SVGRepo_bgCarrier" stroke-width="0"></g><g id="SVGRepo_tracerCarrier" stroke-linecap="round" stroke-linejoin="round"></g><g id="SVGRepo_iconCarrier"> <path d="M18.72 3.99997H5.37C5.19793 3.99191 5.02595 4.01786 4.86392 4.07635C4.70189 4.13484 4.55299 4.22471 4.42573 4.34081C4.29848 4.45692 4.19537 4.59699 4.12232 4.75299C4.04927 4.909 4.0077 5.07788 4 5.24997V18.63C4.01008 18.9901 4.15766 19.3328 4.41243 19.5875C4.6672 19.8423 5.00984 19.9899 5.37 20H18.72C19.0701 19.9844 19.4002 19.8322 19.6395 19.5761C19.8788 19.32 20.0082 18.9804 20 18.63V5.24997C20.0029 5.08247 19.9715 4.91616 19.9078 4.76122C19.8441 4.60629 19.7494 4.466 19.6295 4.34895C19.5097 4.23191 19.3672 4.14059 19.2108 4.08058C19.0544 4.02057 18.8874 3.99314 18.72 3.99997ZM9 17.34H6.67V10.21H9V17.34ZM7.89 9.12997C7.72741 9.13564 7.5654 9.10762 7.41416 9.04768C7.26291 8.98774 7.12569 8.89717 7.01113 8.78166C6.89656 8.66615 6.80711 8.5282 6.74841 8.37647C6.6897 8.22474 6.66301 8.06251 6.67 7.89997C6.66281 7.73567 6.69004 7.57169 6.74995 7.41854C6.80986 7.26538 6.90112 7.12644 7.01787 7.01063C7.13463 6.89481 7.2743 6.80468 7.42793 6.74602C7.58157 6.68735 7.74577 6.66145 7.91 6.66997C8.07259 6.66431 8.2346 6.69232 8.38584 6.75226C8.53709 6.8122 8.67431 6.90277 8.78887 7.01828C8.90344 7.13379 8.99289 7.27174 9.05159 7.42347C9.1103 7.5752 9.13699 7.73743 9.13 7.89997C9.13719 8.06427 9.10996 8.22825 9.05005 8.3814C8.99014 8.53456 8.89888 8.6735 8.78213 8.78931C8.66537 8.90513 8.5257 8.99526 8.37207 9.05392C8.21843 9.11259 8.05423 9.13849 7.89 9.12997ZM17.34 17.34H15V13.44C15 12.51 14.67 11.87 13.84 11.87C13.5822 11.8722 13.3313 11.9541 13.1219 12.1045C12.9124 12.2549 12.7546 12.4664 12.67 12.71C12.605 12.8926 12.5778 13.0865 12.59 13.28V17.34H10.29V10.21H12.59V11.21C12.7945 10.8343 13.0988 10.5225 13.4694 10.3089C13.84 10.0954 14.2624 9.98848 14.69 9.99997C16.2 9.99997 17.34 11 17.34 13.13V17.34Z" fill="#FFFFFF"></path> </g></svg>
            </a>
        </p>
</footer>
