<!DOCTYPE html>
<html lang="pt-br">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="../../../site/style.css">
    <title>NeuroBit | @pauloroberto.dev</title>
    <link rel="shortcut icon" href="../../../imagens/favicon.png" type="image/x-icon">
</head>

<header class="breadcrumb" id="top">
    <nav>
        <a href="ia-am-faceli.html">Voltar à página anterior</a>
        <br/>
        <br/>
        <div class="header-info-container">
            <div class="container-esquerdo">
                <p class="numeracao-sequencial">2</p>
            </div>
            <div>
                <p class="titulo">Inteligência Artificial: Uma Abordagem de Aprendizado de Máquina</p>
                <p class="descricao">Katti Faceli et al.</p>
            </div>
        </div>
    </nav>
</header>

<body>
    <main>
        <h1># 2.5 Capítulo 5 (p. 64/77)</h1>
        <p><img src="https://img.shields.io/badge/Status-Estudando-grey?labelColor=31A8B8"></p>

        <article>
            <div>
                <h2>Parte 2 | Modelos preditivos (capítulos 4 a 10)</h2>
                <h3> 5. Métodos probabilísticos</h3>
                <div>
                    <h4 id="subtitulo_1_nota_1">5.1 Brevíssimas considerações sobre probabilidade <sup><a class="linknotas" href="#nota_1">[1]</a></sup></h4>
                    <p id="subtitulo_1_nota_2_3">
                        Em linhas gerais, a probabilidade pode ser definida como <strong>medida da frequência relativa</strong><sup><a class="linknotas" href="#nota_2">[2]</a></sup> de um evento<sup><a class="linknotas" href="#nota_3">[3]</a></sup> — isto é, a quantidade de vezes em que um determinado resultado ocorre ao longo de uma série experimental, desde que as condições nas quais foi observado se mantenham constantes — ou <strong>medida de crença</strong> de um indivíduo, decorrente de subjetivismo pessoal (Ross, 2010). Ela diz respeito a formas de quantificar a incerteza associada à previsão de resultados de eventos aleatórios e, sob uma perspectiva mais ampla, também à quantidade de surpresa esperada ao observar essas ocorrências e à quantidade de informação intrínseca a tais observações (Ross, 2010).
                    </p>
                    <p>
                        <blockquote>
                            "Parece razoável supor que a quantidade de surpresa causada pela informação de que E ocorreu deve depender da probabilidade de E. [...] [Logo,] a surpresa que alguém sente ao saber da ocorrência do evento E depende somente da probabilidade de E, [...] [então] não há surpresa ao ouvirmos que um evento cuja ocorrência é certa tenha de fato ocorrido [...] [, pois] quanto mais improvável é a ocorrência de um evento, maior é a surpresa causada por sua ocorrência." (ROSS, 2010, p. 501/502).
                        </blockquote>
                    </p>
                    <p id="subtitulo_1_nota_4_5">
                        Nesse sentido, a quantificação tanto da surpresa quanto da incerteza traduzem a mesma imprevisibilidade inerente ao contexto probabilístico e, exatamente por consubstanciarem enfoques distintos de uma mesma linha de raciocínio, podem ser vistas como a entropia<sup><a class="linknotas" href="#nota_4">[4]</a></sup> de uma variável aleatória<sup><a class="linknotas" href="#nota_5">[5]</a></sup> (Ross, 2010) ou, em outras palavras, de um evento qualquer. "De fato, na teoria da informação, [...] é interpretada como a quantidade média de informação recebida quando o valor de [uma variável aleatória] X é observado." (ROSS, 2010, p. 504). O <strong>valor esperado</strong> ou a <strong>esperança</strong> de uma variável aleatória "[...] é uma média ponderada dos possíveis valores que [a variável aleatória] X pode receber, com cada valor sendo ponderado pela probabilidade de que [a variável aleatória] X seja igual a esse valor." (ROSS, 2010, p. 160).
                    </p>
                    <p>
                        Consoante a teoria da informação, a entropia de um sistema pode ser compreendida como uma métrica da [im]previsibilidade da ocorrência de determinado evento, tal que quanto maior a quantidade de informações disponíveis para ajustar a expectativa relacionada à observação, tão menor será a entropia e a incerteza associadas; consequentemente, por outro lado, se houver poucas informações prévias para auxiliar na valoração da probabilidade, o aumento da entropia consequentemente importará em maior incerteza. Isso acontece porque ela é inversamente proporcional à interpretabilidade dos dados: cenários de maior entropia reduzem o valor informacional porque aumentam a desorganização (confusão), enquanto os de menor entropia favorecem a organização — e reduzem a confusão — dos dados, tornando-os mais interpretáveis e previsíveis.
                    </p>
                    <p>
                        Não por outro motivo, "<em>uncertainty represents the reliability of our inferences.</em>" (DAVIS et al., 2020, p. 3).
                    </p>
                    <p>
                        Entretanto, é valioso observar que, "frequentemente, quando realizamos um experimento, estamos interessados principalmente em alguma função do resultado e não no resultado em si." (ROSS, 2010, p. 151).
                    </p>
                </div>
                <div>
                    <h4 id="subtitulo_2_nota_6_7">5.2 Teorema de Bayes, métodos probabilísticos bayesianos e aprendizado bayesiano <sup><a class="linknotas" href="#nota_6">[6]</a></sup></h4>
                    <p>
                        A <strong>probabilidade condicional</strong><sup><a class="linknotas" href="#nota_7">[7]</a></sup> avalia a probabilidade de que um evento ocorra, dada a ocorrência ou não de outro, já conhecido, a qual deve ser considerada para obter a probabilidade total. O cálculo da probabilidade condicionada é especialmente útil quando se trabalha com dados incompletos ou imprecisos.
                    </p>
                    <p>
                        Os métodos probabilísticos bayesianos, sustentados no Teorema de Bayes, "[...] assumem que a probabilidade de um evento A, que pode ser uma classe [...], dado um evento B, que pode ser o conjunto de valores dos atributos de entrada [...], não depende apenas da relação entre A e B, mas também da probabilidade de observar A independentemente de observar B (Mitchell, 1997)." (FACELI et al., 2023, p. 64). No que pertine ao aprendizado de máquina, ele "[...] fornece uma maneira de calcular a probabilidade de um evento ou objeto pertencer a uma classe P(B|A) utilizando a probabilidade <em>a priori</em> da classe, P(A), a probabilidade de observar vários objetos com os mesmos valores de atributos que pertencem à classe, P(B|A), e a probabilidade de ocorrência desses objetos, P(B)." (FACELI et al., 2023, p. 64), e pode ser utilizado para resolver problemas em cenários probabilísticos.
                    </p>
                    <p>
                        Noutras palavras, o Teorema de Bayes expressa a probabilidade <em>a posteriori</em>, que é a probabilidade <em>a priori</em>, inicialmente conhecida, sopesada pelas informações adicionais disponíveis — na verdade, pela verossimilhança da hipótese mais provável à luz das novas evidências, que servem para atualizar ou refinar a probabilidade inicial, em vez de desconstruí-la ou substituí-la por completo.
                    </p>
                    <p>
                        "No aprendizado bayesiano, o valor de uma variável aleatória tem uma probabilidade associada. [...] O teorema de Bayes é usado para calcular a probabilidade <em>a posteriori</em> de um evento, dadas sua probabilidade <em>a priori</em> e a verossimilhança do novo dado." (FACELI et al., 2023, p. 66).
                    </p>
                    <p>
                        A função que calcula a probabilidade condicionada e separa os exemplos em classes distintas é chamada <strong>função discriminante</strong>. "Dependendo das hipóteses propostas, diferentes funções discriminantes são obtidas, levando a diferentes classificadores" (FACELI et al., 2023, p. 66), e uma das formas de obtê-la é por meio da estimativa MAP (<em>Maximum A Posteriori</em>), que busca maximizar a eficiência preditiva ao combinar as informações disponíveis <em>a priori</em> com a hipótese mais provável (verossímil) dada a nova evidência observada.
                    </p>
                </div>
                <div>
                    <h4>5.3 Classificador <em>naive</em> Bayes</h4>
                    <p>
                        É um algoritmo que, aplicando o Teorema de Bayes, classifica objetos a partir do cálculo da probabilidade individual de cada atributo, assumindo serem independentes entre si e em relação à classe. Presume-se que a probabilidade de que o exemplo, assim considerado como um conjunto de atributos, pertença à classe é proporcional ao produto das probabilidades dos atributos individualmente considerados, isto é, de que cada um deles ocorra em objetos daquela classe. Essa característica faz com que não seja uma boa opção em cenários que envolvam atributos interdependentes ou em que seja importante a busca e/ou análise de correlações.
                    </p>
                    <p>
                        No tocante à implementação, as probabilidades são calculadas durante o treinamento por meio do uso extensivo de contadores: um para a probabilidade <em>a priori</em> de cada classe; outros tantos quantos forem os atributos qualitativos por classe; e para os atributos quantitativos, se previamente discretizados, deverá haver um contador por intervalo para cada classe. Alternativamente, é possível supor que os atributos quantitativos possuam determinada distribuição, comumente a normal/gaussiana, o que a literatura aponta menos eficiente em comparação com a discretização (Dougherty et al., 1995; Domingos e Pazzini, 1997, apud Faceli et al., 2023), porque não necessariamente tal assunção será verdadeira.
                    </p>
                    <p>
                        De modo geral, são destacados dentre os aspectos positivos do algoritmo a eficiência na indução do modelo, facilidade de implementação, bom desempenho em cenários variados, ainda que haja alguma correlação entre atributos, boa interpretabilidade de resultados, pois "[...] resume a variabilidade do conjunto de dados em tabelas de contingência, e assume que estas são suficientes para distinguir entre as classes" (FACELI et al., 2023, p. 71), e a capacidade de lidar com dados incompletos ou imprecisos, "isso porque [supondo um problema de classificação binária] o atributo contribuirá igualmente na previsão das duas classes e os outros atributos é que determinarão a classificação final." (FACELI et al., 2023, p. 71).
                    </p>
                    <p>
                        Já como aspectos negativos, destaca-se a sensibilidade à presença de atributos redundantes, que exercerão maior influência sobre a classificação porque "[...] o NB desconsidera a relação entre os atributos, tratando-os como independentes" (FACELI et al., 2023, p. 71) e as peculiaridades inerentes aos atributos quantitativos. Ademais, "frequentemente, os valores de probabilidade obtidos pelo NB não são realistas. Contudo, eles fornecem um bom ranqueamento, de maneira que a regra do máximo <em>a posteriori</em> pode ser aplicada com sucesso." (FACELI et al., 2023, p. 71).
                    </p>
                    <p>
                        Há diversas implementações alternativas desse algoritmo, desenvolvidas para contornar as limitações e otimizar o desempenho do classificador, como o <em>naive</em> Bayes hierárquico, a árvore de <em>naive</em> Bayes, o semi-<em>naive</em> Bayes, o <em>naive</em> Bayes construtivo, o Bayes Flexível e o Linear Bayes, os quais podem apresentar desempenho superior, especialmente, em cenários específicos.
                    </p>
                </div>
                <div>
                    <h4>5.4 Redes Bayesianas</h4>
                    <p>
                        Os modelos gráficos probabilísticos, dentre os quais as redes Bayesianas, "[...] utilizam o conceito de independência condicional entre variáveis para obter um equilíbrio entre o número de parâmetros a calcular e a representação de dependências entre as variáveis. Esses modelos representam a distribuição de probabilidade conjunta de um grupo de variáveis aleatórias em um domínio específico" (FACELI et al., 2023, p. 72) e podem ser empregados em tarefas que vão "[...] desde [a] previsão, em que se pretende obter o resultado mais provável para os dados de entrada, até o diagnóstico, em que se pretende obter as causas mais prováveis para os efeitos observados." (FACELI et al., 2023, p. 75).
                    </p>
                    <p id="subtitulo_4_nota_8_9">
                        Nesse sentido, duas <strong>variáveis aleatórias</strong> são <strong>independentes</strong></strong><sup><a class="linknotas" href="#nota_8">[8]</a></sup> se a probabilidade de uma não influenciar a de outra — ou seja, o valor de uma não serve para sugerir o de outra —; <strong>condicionalmente independentes</strong> se essa condição subsistir na presença de uma terceira variável — isto é, as probabilidades condicionais não se influenciam mutuamente, de modo que a probabilidade da primeira, dada a segunda ou dada esta e a terceira, é a mesma; e, contrariamente, são <strong>dependentes</strong> aquelas que não forem independentes (Ross, 2010), ou seja, quando a probabilidade de ocorrência de uma mudar a da outra</strong><sup><a class="linknotas" href="#nota_9">[9]</a></sup>.
                    </p>
                </div>
            </div>
            <a class="emoji" href="#top">[ 🔝 ]</a>
        </article>
        <hr>
        <article>
            <h2>Principais tópicos</h2>
            <ul>
                <li>
                    <strong>Probabilidade</strong>
                    <ul>
                        <li>
                            Dois pontos de vista
                            <ul>
                                <li>Medida da frequência relativa de um evento</li>
                                <li>Medida da crença de um indivíduo em relação a um evento</li>
                            </ul>
                        </li>
                        <li>
                            <strong>Quantificação:</strong>
                            <ul>
                                <li>da incerteza associada à [im]previsibilidade de eventos aleatórios</li>
                                <li>da surpresa esperada ao observar tais eventos</li>
                                <li>da informação intrínseca à observação</li>
                            </ul>
                        </li>
                        <li>
                            <strong>Conceitos abordados</strong>
                            <ul>
                                <li>Espaço amostral, evento e probabilidade</li>
                                <li>Esperança ou valor esperado</li>
                                <li>
                                    <strong>Entropia</strong>
                                    <ul>
                                        <li>Medida da desordem de um sistema</li>
                                        <li>Organização, interpretabilidade e valor informacional dos dados</li>
                                        <li>Relacionada com a incerteza e a surpresa em cenários probabilísticos</li>
                                    </ul>
                                </li>
                                <li>
                                    <strong>Variável aleatória</strong>
                                    <ul>
                                        <li>Função definida no espaço amostral</li>
                                        <li>Discretas e contínuas</li>
                                        <li>Independentes, condicionalmente independentes e dependentes</li>
                                    </ul>
                                </li>
                            </ul>
                        </li>
                    </ul>
                </li>
                <li>
                    <strong>Teorema de Bayes</strong>
                    <ul>
                        <li>É uma forma de calcular a probabilidade de ocorrência de um evento A dado um evento B que já ocorreu (P(A|B)), considerando <strong>(a)</strong> a probabilidade <em>a priori</em> de A (P(A)), isto é, a probabilidade de que A ocorra independentemente de B; <strong>(b)</strong> a probabilidade condicional de B dada a ocorrência de A (P(B|A)); e <strong>(c)</strong> a probabilidade total de B (P(B)), que consiste <strong>(c.1)</strong> novamente na probabilidade <em>a priori</em> de A e na probabilidade de B dado A, acrescida <strong>(c.2)</strong> da probabilidade de que A não ocorra, mas B ocorra independentemente de A.
                        </li>
                        <li>
                            É uma forma de ajustar a probabilidade de A em relação a B, considerando novas evidências de B, sem descartar a probabilidade inicial de A, que ao invés disso é atualizada pela probabilidade de B dado A e pela probabilidade total de B.
                        </li>
                    </ul>
                </li>
                <li>
                    <strong>Aprendizado bayesiano</strong>
                    <ul>
                        <li>
                            O valor de uma variável aleatória é estimado a partir da probabilidade <em>a priori</em> da classe (evento) e da probabilidade de que novos objetos pertençam àquela classe, considerando as informações disponíveis (verossimilhança).
                        </li>
                        <li>Dados imprecisos e incompletos</li>
                        <li>Função discriminante</li>
                    </ul>
                </li>
                <li>
                    <strong>Classificador <em>naive</em> Bayes</strong>
                    <ul>
                        <li>A classificação resulta do produto das probabilidades individuais de cada atributo, que se presumem independentes entre si e em relação à classe</li>
                        <li>Incapaz de lidar com atributos interdependentes</li>
                    </ul>
                </li>
            </ul>
            <p><a class="emoji" href="#top">[ 🔝 ]</a></p>
        </article>
        <hr>
        <article>
            <h2>Referências complementares</h2>
            <p>
                AUDY, Jorge L N.; ANDRADE, Gilberto K.; CIDRAL, Alexandre. <strong>Fundamentos de sistemas de informação</strong>. E-book. Porto Alegre: Bookman, 2007.
            </p>
            <p>
                DAVIS, Josiah; ZHU, Jason; OLDFATHER, Jeremy; MACDONALD, Samual; TRZASKOWSKI, Maciej; KELSEN, Max. 2020. <strong>Quantifying uncertainty in deep learning systems</strong>. Disponível em <https://d1.awsstatic.com/APG/quantifying-uncertainty-in-deep-learning-systems.pdf>. Acesso em 18 mai. 2024.
            </p>
            <p>
                ROSS, Sheldon. <strong>Probabilidade: um curso moderno com aplicações</strong>. Trad. Alberto Resende De Conti. 8 ed. Porto Alegre: Bookman, 2010.
            </p>
            <a class="emoji" href="#top">[ 🔝 ]</a>
        </article>
        <hr>
        <article>
            <h2>Notas</h2>
            <p id="nota_1">[1] Este tópico é autoral, portanto, não consta no texto original do livro.<a class="linkdiscreto" href="#subtitulo_1_nota_1"> [ 🔙 ]</a></p>
            <p id="nota_2">[2] Como uma métrica de frequência relativa, pode ser definida nos termos de "[...] um experimento, cujo espaço amostral é S, [que] seja realizado repetidamente em condições exatamente iguais. Para cada evento E do espaço amostral S, definimos n(E) como o número de vezes que o evento E ocorre nas n primeiras repetições do experimento. Então, P(E), a probabilidade do evento E, é definida como [...] a proporção (limite) de tempo em que E ocorre [...] ou como a frequência limite de E." (ROSS, 2010, p. 44).<a class="linkdiscreto" href="#subtitulo_1_nota_2_3"> [ 🔙 ]</a></p>
            <p id="nota_3">[3] Denomina-se <strong>espaço amostral</strong> (S) o conjunto que presumidamente contenha a totalidade de resultados possíveis de um experimento, mesmo os incertos; e <strong>evento</strong> (E) qualquer subconjunto de S, tal que, "se o resultado do experimento estiver contido em E, então dizemos que E ocorreu." (ROSS, 2010, p. 40).<a class="linkdiscreto" href="#subtitulo_1_nota_2_3"> [ 🔙 ]</a></p>
            <p id="nota_4">[4] Na teoria da informação, a <strong>entropia</strong> é uma propriedade que "[...] mede o grau de desordem de um sistema, e a forma de combater essa desordem se dá através da informação." (AUDY; CIDRAL, 2007, p. 37).<a class="linkdiscreto" href="#subtitulo_1_nota_4_5"> [ 🔙 ]</a></p>
            <p id="nota_5">[5] <strong>Variáveis aleatórias</strong> são "[...] grandezas de interesse, ou, mais formalmente, [...] funções reais definidas no espaço amostral [...]. Como o valor da variável aleatória é determinado pelo resultado do experimento, podemos atribuir probabilidades aos possíveis valores da variável aleatória" (ROSS, 2010, p. 151), que podem ser <strong>discretas</strong> — há uma quantidade máxima (finita) e [infinitamente] contável de valores possíveis — ou [absolutamente] <strong>contínuas</strong> — há infinitos e incontáveis valores que podem ser assumidos ou, inversamente, cuja probabilidade de que assumam determinado valor específico é nula (Ross, 2010).<a class="linkdiscreto" href="#subtitulo_1_nota_4_5"> [ 🔙 ]</a></p>
            <p id="nota_6">[6] No livro, equivalente à introdução do capítulo 5 e à seção 5.1 (aprendizado bayesiano).<a class="linkdiscreto" href="#subtitulo_2_nota_6_7"> [ 🔙 ]</a></p>
            <p id="nota_7">[7] A "[...] probabilidade condicional de que E ocorra dado que F ocorreu [...] é representada por P(E | F). [...] [Noutras palavras,] se o evento F ocorrer, então, para que E ocorra, é necessário que a ocorrência real seja um ponto tanto em E quanto em F; isto é, ela deve estar em EF. Agora, como sabemos que F ocorreu, tem-se que F se torna nosso novo, ou reduzido, espaço amostral; com isso, a probabilidade de que o evento EF ocorra será igual à probabilidade de EF relativa à probabilidade de F." (ROSS, 2010, p. 82).<a class="linkdiscreto" href="#subtitulo_2_nota_6_7"> [ 🔙 ]</a></p>
            <p id="nota_8">[8] Diz-se que duas variáveis aleatórias "[...] X e Y são independentes se o conhecimento do valor de um não mudar a distribuição do outro. Variáveis aleatórias que não são independentes são chamadas de dependentes." (ROSS, 2010, p. 293). Observe-se que a "independência é uma relação simétrica. As variáveis aleatórias X e Y são independentes se sua função densidade conjunta (ou função de probabilidade conjunta, no caso discreto) é o produto de suas funções densidade (ou de probabilidade) individuais. Portanto, dizer que X é independente de Y é equivalente a dizer que Y é independente de X — ou somente que X e Y são independentes. Como resultado, ao considerar se X é independente ou não de Y em situações em que não é intuitivo saber que o valor de Y não muda as probabilidades relacionadas a X, pode ser útil inverter os papeis de X e Y e perguntar se Y é independente de X." (ROSS, 2010, p. 304).<a class="linkdiscreto" href="#subtitulo_4_nota_8_9"> [ 🔙 ]</a></p>
            <p id="nota_9">[9] Sobre <strong>eventos independentes</strong>, a P(E|F), isto é, "[...] a probabilidade condicional de E dado F, não é geralmente igual [...] [à] probabilidade incondicional de E [P(E)]. Em outras palavras, o conhecimento de que F ocorreu geralmente muda a chance de ocorrência de E. Nos casos especiais em que P(E|F) é de fato igual a P(E), dizemos que E é independente de F. Isto é, <strong>E é independente de F se o conhecimento de que F ocorreu não mudar a probabilidade de ocorrência de E.</strong>" (ROSS, 2010, p. 106, destaquei).<a class="linkdiscreto" href="#subtitulo_4_nota_8_9"> [ 🔙 ]</a></p>
            <a class="emoji" href="#top">[ 🔝 ]</a>
        </article>
    </main>
</body>

<footer>
    <p>Projeto NeuroBit</p>
        <p class="linkdiscreto">
            <a class="social linkdiscreto" href="https://github.com/paulorobertovrc" target="_blank">
                <svg width="24px" height="24px" viewBox="0 0 20 20" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" fill="#ffffff"><g id="SVGRepo_bgCarrier" stroke-width="0"></g><g id="SVGRepo_tracerCarrier" stroke-linecap="round" stroke-linejoin="round"></g><g id="SVGRepo_iconCarrier"> <title>github [#142]</title> <desc>Created with Sketch.</desc> <defs> </defs> <g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"> <g id="Dribbble-Light-Preview" transform="translate(-140.000000, -7559.000000)" fill="#fff"> <g id="icons" transform="translate(56.000000, 160.000000)"> <path d="M94,7399 C99.523,7399 104,7403.59 104,7409.253 C104,7413.782 101.138,7417.624 97.167,7418.981 C96.66,7419.082 96.48,7418.762 96.48,7418.489 C96.48,7418.151 96.492,7417.047 96.492,7415.675 C96.492,7414.719 96.172,7414.095 95.813,7413.777 C98.04,7413.523 100.38,7412.656 100.38,7408.718 C100.38,7407.598 99.992,7406.684 99.35,7405.966 C99.454,7405.707 99.797,7404.664 99.252,7403.252 C99.252,7403.252 98.414,7402.977 96.505,7404.303 C95.706,7404.076 94.85,7403.962 94,7403.958 C93.15,7403.962 92.295,7404.076 91.497,7404.303 C89.586,7402.977 88.746,7403.252 88.746,7403.252 C88.203,7404.664 88.546,7405.707 88.649,7405.966 C88.01,7406.684 87.619,7407.598 87.619,7408.718 C87.619,7412.646 89.954,7413.526 92.175,7413.785 C91.889,7414.041 91.63,7414.493 91.54,7415.156 C90.97,7415.418 89.522,7415.871 88.63,7414.304 C88.63,7414.304 88.101,7413.319 87.097,7413.247 C87.097,7413.247 86.122,7413.234 87.029,7413.87 C87.029,7413.87 87.684,7414.185 88.139,7415.37 C88.139,7415.37 88.726,7417.2 91.508,7416.58 C91.513,7417.437 91.522,7418.245 91.522,7418.489 C91.522,7418.76 91.338,7419.077 90.839,7418.982 C86.865,7417.627 84,7413.783 84,7409.253 C84,7403.59 88.478,7399 94,7399" id="github-[#142]"> </path> </g> </g> </g> </g></svg>
            </a>
            <a class="social linkdiscreto" href="https://www.instagram.com/pauloroberto.dev" target="_blank">
                <svg width="24px" height="24px" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" stroke="#FFFFFF"><g id="SVGRepo_bgCarrier" stroke-width="0"></g><g id="SVGRepo_tracerCarrier" stroke-linecap="round" stroke-linejoin="round"></g><g id="SVGRepo_iconCarrier"> <path fill-rule="evenodd" clip-rule="evenodd" d="M12 18C15.3137 18 18 15.3137 18 12C18 8.68629 15.3137 6 12 6C8.68629 6 6 8.68629 6 12C6 15.3137 8.68629 18 12 18ZM12 16C14.2091 16 16 14.2091 16 12C16 9.79086 14.2091 8 12 8C9.79086 8 8 9.79086 8 12C8 14.2091 9.79086 16 12 16Z" fill="#FFFFFF"></path> <path d="M18 5C17.4477 5 17 5.44772 17 6C17 6.55228 17.4477 7 18 7C18.5523 7 19 6.55228 19 6C19 5.44772 18.5523 5 18 5Z" fill="#FFFFFF"></path> <path fill-rule="evenodd" clip-rule="evenodd" d="M1.65396 4.27606C1 5.55953 1 7.23969 1 10.6V13.4C1 16.7603 1 18.4405 1.65396 19.7239C2.2292 20.8529 3.14708 21.7708 4.27606 22.346C5.55953 23 7.23969 23 10.6 23H13.4C16.7603 23 18.4405 23 19.7239 22.346C20.8529 21.7708 21.7708 20.8529 22.346 19.7239C23 18.4405 23 16.7603 23 13.4V10.6C23 7.23969 23 5.55953 22.346 4.27606C21.7708 3.14708 20.8529 2.2292 19.7239 1.65396C18.4405 1 16.7603 1 13.4 1H10.6C7.23969 1 5.55953 1 4.27606 1.65396C3.14708 2.2292 2.2292 3.14708 1.65396 4.27606ZM13.4 3H10.6C8.88684 3 7.72225 3.00156 6.82208 3.0751C5.94524 3.14674 5.49684 3.27659 5.18404 3.43597C4.43139 3.81947 3.81947 4.43139 3.43597 5.18404C3.27659 5.49684 3.14674 5.94524 3.0751 6.82208C3.00156 7.72225 3 8.88684 3 10.6V13.4C3 15.1132 3.00156 16.2777 3.0751 17.1779C3.14674 18.0548 3.27659 18.5032 3.43597 18.816C3.81947 19.5686 4.43139 20.1805 5.18404 20.564C5.49684 20.7234 5.94524 20.8533 6.82208 20.9249C7.72225 20.9984 8.88684 21 10.6 21H13.4C15.1132 21 16.2777 20.9984 17.1779 20.9249C18.0548 20.8533 18.5032 20.7234 18.816 20.564C19.5686 20.1805 20.1805 19.5686 20.564 18.816C20.7234 18.5032 20.8533 18.0548 20.9249 17.1779C20.9984 16.2777 21 15.1132 21 13.4V10.6C21 8.88684 20.9984 7.72225 20.9249 6.82208C20.8533 5.94524 20.7234 5.49684 20.564 5.18404C20.1805 4.43139 19.5686 3.81947 18.816 3.43597C18.5032 3.27659 18.0548 3.14674 17.1779 3.0751C16.2777 3.00156 15.1132 3 13.4 3Z" fill="#FFFFFF"></path> </g></svg>
            </a>
            <a class="social linkdiscreto" href="https://www.linkedin.com/in/paulorobertovrc" target="_blank">
                <svg width="27px" height="27px" viewBox="0 0 21 21" fill="none" xmlns="http://www.w3.org/2000/svg"><g id="SVGRepo_bgCarrier" stroke-width="0"></g><g id="SVGRepo_tracerCarrier" stroke-linecap="round" stroke-linejoin="round"></g><g id="SVGRepo_iconCarrier"> <path d="M18.72 3.99997H5.37C5.19793 3.99191 5.02595 4.01786 4.86392 4.07635C4.70189 4.13484 4.55299 4.22471 4.42573 4.34081C4.29848 4.45692 4.19537 4.59699 4.12232 4.75299C4.04927 4.909 4.0077 5.07788 4 5.24997V18.63C4.01008 18.9901 4.15766 19.3328 4.41243 19.5875C4.6672 19.8423 5.00984 19.9899 5.37 20H18.72C19.0701 19.9844 19.4002 19.8322 19.6395 19.5761C19.8788 19.32 20.0082 18.9804 20 18.63V5.24997C20.0029 5.08247 19.9715 4.91616 19.9078 4.76122C19.8441 4.60629 19.7494 4.466 19.6295 4.34895C19.5097 4.23191 19.3672 4.14059 19.2108 4.08058C19.0544 4.02057 18.8874 3.99314 18.72 3.99997ZM9 17.34H6.67V10.21H9V17.34ZM7.89 9.12997C7.72741 9.13564 7.5654 9.10762 7.41416 9.04768C7.26291 8.98774 7.12569 8.89717 7.01113 8.78166C6.89656 8.66615 6.80711 8.5282 6.74841 8.37647C6.6897 8.22474 6.66301 8.06251 6.67 7.89997C6.66281 7.73567 6.69004 7.57169 6.74995 7.41854C6.80986 7.26538 6.90112 7.12644 7.01787 7.01063C7.13463 6.89481 7.2743 6.80468 7.42793 6.74602C7.58157 6.68735 7.74577 6.66145 7.91 6.66997C8.07259 6.66431 8.2346 6.69232 8.38584 6.75226C8.53709 6.8122 8.67431 6.90277 8.78887 7.01828C8.90344 7.13379 8.99289 7.27174 9.05159 7.42347C9.1103 7.5752 9.13699 7.73743 9.13 7.89997C9.13719 8.06427 9.10996 8.22825 9.05005 8.3814C8.99014 8.53456 8.89888 8.6735 8.78213 8.78931C8.66537 8.90513 8.5257 8.99526 8.37207 9.05392C8.21843 9.11259 8.05423 9.13849 7.89 9.12997ZM17.34 17.34H15V13.44C15 12.51 14.67 11.87 13.84 11.87C13.5822 11.8722 13.3313 11.9541 13.1219 12.1045C12.9124 12.2549 12.7546 12.4664 12.67 12.71C12.605 12.8926 12.5778 13.0865 12.59 13.28V17.34H10.29V10.21H12.59V11.21C12.7945 10.8343 13.0988 10.5225 13.4694 10.3089C13.84 10.0954 14.2624 9.98848 14.69 9.99997C16.2 9.99997 17.34 11 17.34 13.13V17.34Z" fill="#FFFFFF"></path> </g></svg>
            </a>
        </p>
</footer>
