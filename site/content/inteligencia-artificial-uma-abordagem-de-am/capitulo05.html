<!DOCTYPE html>
<html lang="pt-br">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="../../../site/style.css">
    <title>NeuroBit | @pauloroberto.dev</title>
    <link rel="shortcut icon" href="../../../imagens/favicon.png" type="image/x-icon">
</head>

<header class="breadcrumb" id="top">
    <nav>
        <a href="ia-am-faceli.html">Voltar √† p√°gina anterior</a>
        <br/>
        <br/>
        <div class="header-info-container">
            <div class="container-esquerdo">
                <p class="numeracao-sequencial">2</p>
            </div>
            <div>
                <p class="titulo">Intelig√™ncia Artificial: Uma Abordagem de Aprendizado de M√°quina</p>
                <p class="descricao">Katti Faceli et al.</p>
            </div>
        </div>
    </nav>
</header>

<body>
    <main>
        <h1># 2.5 Cap√≠tulo 5 (p. 64/77)</h1>
        <p><img src="https://img.shields.io/badge/Status-Estudando-grey?labelColor=31A8B8"></p>

        <article>
            <div>
                <h2>Parte 2 | Modelos preditivos (cap√≠tulos 4 a 10)</h2>
                <h3> 5. M√©todos probabil√≠sticos</h3>
                <div>
                    <h4 id="subtitulo_1_nota_1">5.1 Brev√≠ssimas considera√ß√µes sobre probabilidade <sup><a class="linknotas" href="#nota_1">[1]</a></sup></h4>
                    <p id="subtitulo_1_nota_2_3">
                        Em linhas gerais, a probabilidade pode ser definida como <strong>medida da frequ√™ncia relativa</strong><sup><a class="linknotas" href="#nota_2">[2]</a></sup> de um evento<sup><a class="linknotas" href="#nota_3">[3]</a></sup> ‚Äî isto √©, a quantidade de vezes em que um determinado resultado ocorre ao longo de uma s√©rie experimental, desde que as condi√ß√µes nas quais foi observado se mantenham constantes ‚Äî ou <strong>medida de cren√ßa</strong> de um indiv√≠duo, decorrente de subjetivismo pessoal (Ross, 2010). Ela diz respeito a formas de quantificar a incerteza associada √† previs√£o de resultados de eventos aleat√≥rios e, sob uma perspectiva mais ampla, tamb√©m √† quantidade de surpresa esperada ao observar essas ocorr√™ncias e √† quantidade de informa√ß√£o intr√≠nseca a tais observa√ß√µes (Ross, 2010).
                    </p>
                    <p>
                        <blockquote>
                            "Parece razo√°vel supor que a quantidade de surpresa causada pela informa√ß√£o de que E ocorreu deve depender da probabilidade de E. [...] [Logo,] a surpresa que algu√©m sente ao saber da ocorr√™ncia do evento E depende somente da probabilidade de E, [...] [ent√£o] n√£o h√° surpresa ao ouvirmos que um evento cuja ocorr√™ncia √© certa tenha de fato ocorrido [...] [, pois] quanto mais improv√°vel √© a ocorr√™ncia de um evento, maior √© a surpresa causada por sua ocorr√™ncia." (ROSS, 2010, p. 501/502).
                        </blockquote>
                    </p>
                    <p id="subtitulo_1_nota_4_5">
                        Nesse sentido, a quantifica√ß√£o tanto da surpresa quanto da incerteza traduzem a mesma imprevisibilidade inerente ao contexto probabil√≠stico e, exatamente por consubstanciarem enfoques distintos de uma mesma linha de racioc√≠nio, podem ser vistas como a entropia<sup><a class="linknotas" href="#nota_4">[4]</a></sup> de uma vari√°vel aleat√≥ria<sup><a class="linknotas" href="#nota_5">[5]</a></sup> (Ross, 2010) ou, em outras palavras, de um evento qualquer. "De fato, na teoria da informa√ß√£o, [...] √© interpretada como a quantidade m√©dia de informa√ß√£o recebida quando o valor de [uma vari√°vel aleat√≥ria] X √© observado." (ROSS, 2010, p. 504). O <strong>valor esperado</strong> ou a <strong>esperan√ßa</strong> de uma vari√°vel aleat√≥ria "[...] √© uma m√©dia ponderada dos poss√≠veis valores que [a vari√°vel aleat√≥ria] X pode receber, com cada valor sendo ponderado pela probabilidade de que [a vari√°vel aleat√≥ria] X seja igual a esse valor." (ROSS, 2010, p. 160).
                    </p>
                    <p>
                        Consoante a teoria da informa√ß√£o, a entropia de um sistema pode ser compreendida como uma m√©trica da [im]previsibilidade da ocorr√™ncia de determinado evento, tal que quanto maior a quantidade de informa√ß√µes dispon√≠veis para ajustar a expectativa relacionada √† observa√ß√£o, t√£o menor ser√° a entropia e a incerteza associadas; consequentemente, por outro lado, se houver poucas informa√ß√µes pr√©vias para auxiliar na valora√ß√£o da probabilidade, o aumento da entropia consequentemente importar√° em maior incerteza. Isso acontece porque ela √© inversamente proporcional √† interpretabilidade dos dados: cen√°rios de maior entropia reduzem o valor informacional porque aumentam a desorganiza√ß√£o (confus√£o), enquanto os de menor entropia favorecem a organiza√ß√£o ‚Äî e reduzem a confus√£o ‚Äî dos dados, tornando-os mais interpret√°veis e previs√≠veis.
                    </p>
                    <p>
                        N√£o por outro motivo, "<em>uncertainty represents the reliability of our inferences.</em>" (DAVIS et al., 2020, p. 3).
                    </p>
                    <p>
                        Entretanto, √© valioso observar que, "frequentemente, quando realizamos um experimento, estamos interessados principalmente em alguma fun√ß√£o do resultado e n√£o no resultado em si." (ROSS, 2010, p. 151).
                    </p>
                </div>
                <div>
                    <h4 id="subtitulo_2_nota_6_7">5.2 Teorema de Bayes, m√©todos probabil√≠sticos bayesianos e aprendizado bayesiano <sup><a class="linknotas" href="#nota_6">[6]</a></sup></h4>
                    <p>
                        A <strong>probabilidade condicional</strong><sup><a class="linknotas" href="#nota_7">[7]</a></sup> avalia a probabilidade de que um evento ocorra, dada a ocorr√™ncia ou n√£o de outro, j√° conhecido, a qual deve ser considerada para obter a probabilidade total. O c√°lculo da probabilidade condicionada √© especialmente √∫til quando se trabalha com dados incompletos ou imprecisos.
                    </p>
                    <p>
                        Os m√©todos probabil√≠sticos bayesianos, sustentados no Teorema de Bayes, "[...] assumem que a probabilidade de um evento A, que pode ser uma classe [...], dado um evento B, que pode ser o conjunto de valores dos atributos de entrada [...], n√£o depende apenas da rela√ß√£o entre A e B, mas tamb√©m da probabilidade de observar A independentemente de observar B (Mitchell, 1997)." (FACELI et al., 2023, p. 64). No que pertine ao aprendizado de m√°quina, ele "[...] fornece uma maneira de calcular a probabilidade de um evento ou objeto pertencer a uma classe P(B|A) utilizando a probabilidade <em>a priori</em> da classe, P(A), a probabilidade de observar v√°rios objetos com os mesmos valores de atributos que pertencem √† classe, P(B|A), e a probabilidade de ocorr√™ncia desses objetos, P(B)." (FACELI et al., 2023, p. 64), e pode ser utilizado para resolver problemas em cen√°rios probabil√≠sticos.
                    </p>
                    <p>
                        Noutras palavras, o Teorema de Bayes expressa a probabilidade <em>a posteriori</em>, que √© a probabilidade <em>a priori</em>, inicialmente conhecida, sopesada pelas informa√ß√µes adicionais dispon√≠veis ‚Äî na verdade, pela verossimilhan√ßa da hip√≥tese mais prov√°vel √† luz das novas evid√™ncias, que servem para atualizar ou refinar a probabilidade inicial, em vez de desconstru√≠-la ou substitu√≠-la por completo.
                    </p>
                    <p>
                        "No aprendizado bayesiano, o valor de uma vari√°vel aleat√≥ria tem uma probabilidade associada. [...] O teorema de Bayes √© usado para calcular a probabilidade <em>a posteriori</em> de um evento, dadas sua probabilidade <em>a priori</em> e a verossimilhan√ßa do novo dado." (FACELI et al., 2023, p. 66).
                    </p>
                    <p>
                        A fun√ß√£o que calcula a probabilidade condicionada e separa os exemplos em classes distintas √© chamada <strong>fun√ß√£o discriminante</strong>. "Dependendo das hip√≥teses propostas, diferentes fun√ß√µes discriminantes s√£o obtidas, levando a diferentes classificadores" (FACELI et al., 2023, p. 66), e uma das formas de obt√™-la √© por meio da estimativa MAP (<em>Maximum A Posteriori</em>), que busca maximizar a efici√™ncia preditiva ao combinar as informa√ß√µes dispon√≠veis <em>a priori</em> com a hip√≥tese mais prov√°vel (veross√≠mil) dada a nova evid√™ncia observada.
                    </p>
                </div>
                <div>
                    <h4>5.3 Classificador <em>naive</em> Bayes</h4>
                    <p>
                        √â um algoritmo que, aplicando o Teorema de Bayes, classifica objetos a partir do c√°lculo da probabilidade individual de cada atributo, assumindo serem independentes entre si e em rela√ß√£o √† classe. Presume-se que a probabilidade de que o exemplo, assim considerado como um conjunto de atributos, perten√ßa √† classe √© proporcional ao produto das probabilidades dos atributos individualmente considerados, isto √©, de que cada um deles ocorra em objetos daquela classe. Essa caracter√≠stica faz com que n√£o seja uma boa op√ß√£o em cen√°rios que envolvam atributos interdependentes ou em que seja importante a busca e/ou an√°lise de correla√ß√µes.
                    </p>
                    <p>
                        No tocante √† implementa√ß√£o, as probabilidades s√£o calculadas durante o treinamento por meio do uso extensivo de contadores: um para a probabilidade <em>a priori</em> de cada classe; outros tantos quantos forem os atributos qualitativos por classe; e para os atributos quantitativos, se previamente discretizados, dever√° haver um contador por intervalo para cada classe. Alternativamente, √© poss√≠vel supor que os atributos quantitativos possuam determinada distribui√ß√£o, comumente a normal/gaussiana, o que a literatura aponta menos eficiente em compara√ß√£o com a discretiza√ß√£o (Dougherty et al., 1995; Domingos e Pazzini, 1997, apud Faceli et al., 2023), porque n√£o necessariamente tal assun√ß√£o ser√° verdadeira.
                    </p>
                    <p>
                        De modo geral, s√£o destacados dentre os aspectos positivos do algoritmo a efici√™ncia na indu√ß√£o do modelo, facilidade de implementa√ß√£o, bom desempenho em cen√°rios variados, ainda que haja alguma correla√ß√£o entre atributos, boa interpretabilidade de resultados, pois "[...] resume a variabilidade do conjunto de dados em tabelas de conting√™ncia, e assume que estas s√£o suficientes para distinguir entre as classes" (FACELI et al., 2023, p. 71), e a capacidade de lidar com dados incompletos ou imprecisos, "isso porque [supondo um problema de classifica√ß√£o bin√°ria] o atributo contribuir√° igualmente na previs√£o das duas classes e os outros atributos √© que determinar√£o a classifica√ß√£o final." (FACELI et al., 2023, p. 71).
                    </p>
                    <p>
                        J√° como aspectos negativos, destaca-se a sensibilidade √† presen√ßa de atributos redundantes, que exercer√£o maior influ√™ncia sobre a classifica√ß√£o porque "[...] o NB desconsidera a rela√ß√£o entre os atributos, tratando-os como independentes" (FACELI et al., 2023, p. 71) e as peculiaridades inerentes aos atributos quantitativos. Ademais, "frequentemente, os valores de probabilidade obtidos pelo NB n√£o s√£o realistas. Contudo, eles fornecem um bom ranqueamento, de maneira que a regra do m√°ximo <em>a posteriori</em> pode ser aplicada com sucesso." (FACELI et al., 2023, p. 71).
                    </p>
                    <p>
                        H√° diversas implementa√ß√µes alternativas desse algoritmo, desenvolvidas para contornar as limita√ß√µes e otimizar o desempenho do classificador, como o <em>naive</em> Bayes hier√°rquico, a √°rvore de <em>naive</em> Bayes, o semi-<em>naive</em> Bayes, o <em>naive</em> Bayes construtivo, o Bayes Flex√≠vel e o Linear Bayes, os quais podem apresentar desempenho superior, especialmente, em cen√°rios espec√≠ficos.
                    </p>
                </div>
                <div>
                    <h4>5.4 Redes Bayesianas</h4>
                    <p>
                        Os modelos gr√°ficos probabil√≠sticos, dentre os quais as redes Bayesianas, "[...] utilizam o conceito de independ√™ncia condicional entre vari√°veis para obter um equil√≠brio entre o n√∫mero de par√¢metros a calcular e a representa√ß√£o de depend√™ncias entre as vari√°veis. Esses modelos representam a distribui√ß√£o de probabilidade conjunta de um grupo de vari√°veis aleat√≥rias em um dom√≠nio espec√≠fico" (FACELI et al., 2023, p. 72) e podem ser empregados em tarefas que v√£o "[...] desde [a] previs√£o, em que se pretende obter o resultado mais prov√°vel para os dados de entrada, at√© o diagn√≥stico, em que se pretende obter as causas mais prov√°veis para os efeitos observados." (FACELI et al., 2023, p. 75).
                    </p>
                    <p id="subtitulo_4_nota_8_9">
                        Nesse sentido, duas <strong>vari√°veis aleat√≥rias</strong> s√£o <strong>independentes</strong></strong><sup><a class="linknotas" href="#nota_8">[8]</a></sup> se a probabilidade de uma n√£o influenciar a de outra ‚Äî ou seja, o valor de uma n√£o serve para sugerir o de outra ‚Äî; <strong>condicionalmente independentes</strong> se essa condi√ß√£o subsistir na presen√ßa de uma terceira vari√°vel ‚Äî isto √©, as probabilidades condicionais n√£o se influenciam mutuamente, de modo que a probabilidade da primeira, dada a segunda ou dada esta e a terceira, √© a mesma; e, contrariamente, s√£o <strong>dependentes</strong> aquelas que n√£o forem independentes (Ross, 2010), ou seja, quando a probabilidade de ocorr√™ncia de uma mudar a da outra</strong><sup><a class="linknotas" href="#nota_9">[9]</a></sup>.
                    </p>
                </div>
            </div>
            <a class="emoji" href="#top">[ üîù ]</a>
        </article>
        <hr>
        <article>
            <h2>Principais t√≥picos</h2>
            <ul>
                <li>
                    <strong>Probabilidade</strong>
                    <ul>
                        <li>
                            Dois pontos de vista
                            <ul>
                                <li>Medida da frequ√™ncia relativa de um evento</li>
                                <li>Medida da cren√ßa de um indiv√≠duo em rela√ß√£o a um evento</li>
                            </ul>
                        </li>
                        <li>
                            <strong>Quantifica√ß√£o:</strong>
                            <ul>
                                <li>da incerteza associada √† [im]previsibilidade de eventos aleat√≥rios</li>
                                <li>da surpresa esperada ao observar tais eventos</li>
                                <li>da informa√ß√£o intr√≠nseca √† observa√ß√£o</li>
                            </ul>
                        </li>
                        <li>
                            <strong>Conceitos abordados</strong>
                            <ul>
                                <li>Espa√ßo amostral, evento e probabilidade</li>
                                <li>Esperan√ßa ou valor esperado</li>
                                <li>
                                    <strong>Entropia</strong>
                                    <ul>
                                        <li>Medida da desordem de um sistema</li>
                                        <li>Organiza√ß√£o, interpretabilidade e valor informacional dos dados</li>
                                        <li>Relacionada com a incerteza e a surpresa em cen√°rios probabil√≠sticos</li>
                                    </ul>
                                </li>
                                <li>
                                    <strong>Vari√°vel aleat√≥ria</strong>
                                    <ul>
                                        <li>Fun√ß√£o definida no espa√ßo amostral</li>
                                        <li>Discretas e cont√≠nuas</li>
                                        <li>Independentes, condicionalmente independentes e dependentes</li>
                                    </ul>
                                </li>
                            </ul>
                        </li>
                    </ul>
                </li>
                <li>
                    <strong>Teorema de Bayes</strong>
                    <ul>
                        <li>√â uma forma de calcular a probabilidade de ocorr√™ncia de um evento A dado um evento B que j√° ocorreu (P(A|B)), considerando <strong>(a)</strong> a probabilidade <em>a priori</em> de A (P(A)), isto √©, a probabilidade de que A ocorra independentemente de B; <strong>(b)</strong> a probabilidade condicional de B dada a ocorr√™ncia de A (P(B|A)); e <strong>(c)</strong> a probabilidade total de B (P(B)), que consiste <strong>(c.1)</strong> novamente na probabilidade <em>a priori</em> de A e na probabilidade de B dado A, acrescida <strong>(c.2)</strong> da probabilidade de que A n√£o ocorra, mas B ocorra independentemente de A.
                        </li>
                        <li>
                            √â uma forma de ajustar a probabilidade de A em rela√ß√£o a B, considerando novas evid√™ncias de B, sem descartar a probabilidade inicial de A, que ao inv√©s disso √© atualizada pela probabilidade de B dado A e pela probabilidade total de B.
                        </li>
                    </ul>
                </li>
                <li>
                    <strong>Aprendizado bayesiano</strong>
                    <ul>
                        <li>
                            O valor de uma vari√°vel aleat√≥ria √© estimado a partir da probabilidade <em>a priori</em> da classe (evento) e da probabilidade de que novos objetos perten√ßam √†quela classe, considerando as informa√ß√µes dispon√≠veis (verossimilhan√ßa).
                        </li>
                        <li>Dados imprecisos e incompletos</li>
                        <li>Fun√ß√£o discriminante</li>
                    </ul>
                </li>
                <li>
                    <strong>Classificador <em>naive</em> Bayes</strong>
                    <ul>
                        <li>A classifica√ß√£o resulta do produto das probabilidades individuais de cada atributo, que se presumem independentes entre si e em rela√ß√£o √† classe</li>
                        <li>Incapaz de lidar com atributos interdependentes</li>
                    </ul>
                </li>
            </ul>
            <p><a class="emoji" href="#top">[ üîù ]</a></p>
        </article>
        <hr>
        <article>
            <h2>Refer√™ncias complementares</h2>
            <p>
                AUDY, Jorge L N.; ANDRADE, Gilberto K.; CIDRAL, Alexandre. <strong>Fundamentos de sistemas de informa√ß√£o</strong>. E-book. Porto Alegre: Bookman, 2007.
            </p>
            <p>
                DAVIS, Josiah; ZHU, Jason; OLDFATHER, Jeremy; MACDONALD, Samual; TRZASKOWSKI, Maciej; KELSEN, Max. 2020. <strong>Quantifying uncertainty in deep learning systems</strong>. Dispon√≠vel em <https://d1.awsstatic.com/APG/quantifying-uncertainty-in-deep-learning-systems.pdf>. Acesso em 18 mai. 2024.
            </p>
            <p>
                ROSS, Sheldon. <strong>Probabilidade: um curso moderno com aplica√ß√µes</strong>. Trad. Alberto Resende De Conti. 8 ed. Porto Alegre: Bookman, 2010.
            </p>
            <a class="emoji" href="#top">[ üîù ]</a>
        </article>
        <hr>
        <article>
            <h2>Notas</h2>
            <p id="nota_1">[1] Este t√≥pico √© autoral, portanto, n√£o consta no texto original do livro.<a class="linkdiscreto" href="#subtitulo_1_nota_1"> [ üîô ]</a></p>
            <p id="nota_2">[2] Como uma m√©trica de frequ√™ncia relativa, pode ser definida nos termos de "[...] um experimento, cujo espa√ßo amostral √© S, [que] seja realizado repetidamente em condi√ß√µes exatamente iguais. Para cada evento E do espa√ßo amostral S, definimos n(E) como o n√∫mero de vezes que o evento E ocorre nas n primeiras repeti√ß√µes do experimento. Ent√£o, P(E), a probabilidade do evento E, √© definida como [...] a propor√ß√£o (limite) de tempo em que E ocorre [...] ou como a frequ√™ncia limite de E." (ROSS, 2010, p. 44).<a class="linkdiscreto" href="#subtitulo_1_nota_2_3"> [ üîô ]</a></p>
            <p id="nota_3">[3] Denomina-se <strong>espa√ßo amostral</strong> (S) o conjunto que presumidamente contenha a totalidade de resultados poss√≠veis de um experimento, mesmo os incertos; e <strong>evento</strong> (E) qualquer subconjunto de S, tal que, "se o resultado do experimento estiver contido em E, ent√£o dizemos que E ocorreu." (ROSS, 2010, p. 40).<a class="linkdiscreto" href="#subtitulo_1_nota_2_3"> [ üîô ]</a></p>
            <p id="nota_4">[4] Na teoria da informa√ß√£o, a <strong>entropia</strong> √© uma propriedade que "[...] mede o grau de desordem de um sistema, e a forma de combater essa desordem se d√° atrav√©s da informa√ß√£o." (AUDY; CIDRAL, 2007, p. 37).<a class="linkdiscreto" href="#subtitulo_1_nota_4_5"> [ üîô ]</a></p>
            <p id="nota_5">[5] <strong>Vari√°veis aleat√≥rias</strong> s√£o "[...] grandezas de interesse, ou, mais formalmente, [...] fun√ß√µes reais definidas no espa√ßo amostral [...]. Como o valor da vari√°vel aleat√≥ria √© determinado pelo resultado do experimento, podemos atribuir probabilidades aos poss√≠veis valores da vari√°vel aleat√≥ria" (ROSS, 2010, p. 151), que podem ser <strong>discretas</strong> ‚Äî h√° uma quantidade m√°xima (finita) e [infinitamente] cont√°vel de valores poss√≠veis ‚Äî ou [absolutamente] <strong>cont√≠nuas</strong> ‚Äî h√° infinitos e incont√°veis valores que podem ser assumidos ou, inversamente, cuja probabilidade de que assumam determinado valor espec√≠fico √© nula (Ross, 2010).<a class="linkdiscreto" href="#subtitulo_1_nota_4_5"> [ üîô ]</a></p>
            <p id="nota_6">[6] No livro, equivalente √† introdu√ß√£o do cap√≠tulo 5 e √† se√ß√£o 5.1 (aprendizado bayesiano).<a class="linkdiscreto" href="#subtitulo_2_nota_6_7"> [ üîô ]</a></p>
            <p id="nota_7">[7] A "[...] probabilidade condicional de que E ocorra dado que F ocorreu [...] √© representada por P(E | F). [...] [Noutras palavras,] se o evento F ocorrer, ent√£o, para que E ocorra, √© necess√°rio que a ocorr√™ncia real seja um ponto tanto em E quanto em F; isto √©, ela deve estar em EF. Agora, como sabemos que F ocorreu, tem-se que F se torna nosso novo, ou reduzido, espa√ßo amostral; com isso, a probabilidade de que o evento EF ocorra ser√° igual √† probabilidade de EF relativa √† probabilidade de F." (ROSS, 2010, p. 82).<a class="linkdiscreto" href="#subtitulo_2_nota_6_7"> [ üîô ]</a></p>
            <p id="nota_8">[8] Diz-se que duas vari√°veis aleat√≥rias "[...] X e Y s√£o independentes se o conhecimento do valor de um n√£o mudar a distribui√ß√£o do outro. Vari√°veis aleat√≥rias que n√£o s√£o independentes s√£o chamadas de dependentes." (ROSS, 2010, p. 293). Observe-se que a "independ√™ncia √© uma rela√ß√£o sim√©trica. As vari√°veis aleat√≥rias X e Y s√£o independentes se sua fun√ß√£o densidade conjunta (ou fun√ß√£o de probabilidade conjunta, no caso discreto) √© o produto de suas fun√ß√µes densidade (ou de probabilidade) individuais. Portanto, dizer que X √© independente de Y √© equivalente a dizer que Y √© independente de X ‚Äî ou somente que X e Y s√£o independentes. Como resultado, ao considerar se X √© independente ou n√£o de Y em situa√ß√µes em que n√£o √© intuitivo saber que o valor de Y n√£o muda as probabilidades relacionadas a X, pode ser √∫til inverter os papeis de X e Y e perguntar se Y √© independente de X." (ROSS, 2010, p. 304).<a class="linkdiscreto" href="#subtitulo_4_nota_8_9"> [ üîô ]</a></p>
            <p id="nota_9">[9] Sobre <strong>eventos independentes</strong>, a P(E|F), isto √©, "[...] a probabilidade condicional de E dado F, n√£o √© geralmente igual [...] [√†] probabilidade incondicional de E [P(E)]. Em outras palavras, o conhecimento de que F ocorreu geralmente muda a chance de ocorr√™ncia de E. Nos casos especiais em que P(E|F) √© de fato igual a P(E), dizemos que E √© independente de F. Isto √©, <strong>E √© independente de F se o conhecimento de que F ocorreu n√£o mudar a probabilidade de ocorr√™ncia de E.</strong>" (ROSS, 2010, p. 106, destaquei).<a class="linkdiscreto" href="#subtitulo_4_nota_8_9"> [ üîô ]</a></p>
            <a class="emoji" href="#top">[ üîù ]</a>
        </article>
    </main>
</body>

<footer>
    <p>Projeto NeuroBit</p>
        <p class="linkdiscreto">
            <a class="social linkdiscreto" href="https://github.com/paulorobertovrc" target="_blank">
                <svg width="24px" height="24px" viewBox="0 0 20 20" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" fill="#ffffff"><g id="SVGRepo_bgCarrier" stroke-width="0"></g><g id="SVGRepo_tracerCarrier" stroke-linecap="round" stroke-linejoin="round"></g><g id="SVGRepo_iconCarrier"> <title>github [#142]</title> <desc>Created with Sketch.</desc> <defs> </defs> <g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"> <g id="Dribbble-Light-Preview" transform="translate(-140.000000, -7559.000000)" fill="#fff"> <g id="icons" transform="translate(56.000000, 160.000000)"> <path d="M94,7399 C99.523,7399 104,7403.59 104,7409.253 C104,7413.782 101.138,7417.624 97.167,7418.981 C96.66,7419.082 96.48,7418.762 96.48,7418.489 C96.48,7418.151 96.492,7417.047 96.492,7415.675 C96.492,7414.719 96.172,7414.095 95.813,7413.777 C98.04,7413.523 100.38,7412.656 100.38,7408.718 C100.38,7407.598 99.992,7406.684 99.35,7405.966 C99.454,7405.707 99.797,7404.664 99.252,7403.252 C99.252,7403.252 98.414,7402.977 96.505,7404.303 C95.706,7404.076 94.85,7403.962 94,7403.958 C93.15,7403.962 92.295,7404.076 91.497,7404.303 C89.586,7402.977 88.746,7403.252 88.746,7403.252 C88.203,7404.664 88.546,7405.707 88.649,7405.966 C88.01,7406.684 87.619,7407.598 87.619,7408.718 C87.619,7412.646 89.954,7413.526 92.175,7413.785 C91.889,7414.041 91.63,7414.493 91.54,7415.156 C90.97,7415.418 89.522,7415.871 88.63,7414.304 C88.63,7414.304 88.101,7413.319 87.097,7413.247 C87.097,7413.247 86.122,7413.234 87.029,7413.87 C87.029,7413.87 87.684,7414.185 88.139,7415.37 C88.139,7415.37 88.726,7417.2 91.508,7416.58 C91.513,7417.437 91.522,7418.245 91.522,7418.489 C91.522,7418.76 91.338,7419.077 90.839,7418.982 C86.865,7417.627 84,7413.783 84,7409.253 C84,7403.59 88.478,7399 94,7399" id="github-[#142]"> </path> </g> </g> </g> </g></svg>
            </a>
            <a class="social linkdiscreto" href="https://www.instagram.com/pauloroberto.dev" target="_blank">
                <svg width="24px" height="24px" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" stroke="#FFFFFF"><g id="SVGRepo_bgCarrier" stroke-width="0"></g><g id="SVGRepo_tracerCarrier" stroke-linecap="round" stroke-linejoin="round"></g><g id="SVGRepo_iconCarrier"> <path fill-rule="evenodd" clip-rule="evenodd" d="M12 18C15.3137 18 18 15.3137 18 12C18 8.68629 15.3137 6 12 6C8.68629 6 6 8.68629 6 12C6 15.3137 8.68629 18 12 18ZM12 16C14.2091 16 16 14.2091 16 12C16 9.79086 14.2091 8 12 8C9.79086 8 8 9.79086 8 12C8 14.2091 9.79086 16 12 16Z" fill="#FFFFFF"></path> <path d="M18 5C17.4477 5 17 5.44772 17 6C17 6.55228 17.4477 7 18 7C18.5523 7 19 6.55228 19 6C19 5.44772 18.5523 5 18 5Z" fill="#FFFFFF"></path> <path fill-rule="evenodd" clip-rule="evenodd" d="M1.65396 4.27606C1 5.55953 1 7.23969 1 10.6V13.4C1 16.7603 1 18.4405 1.65396 19.7239C2.2292 20.8529 3.14708 21.7708 4.27606 22.346C5.55953 23 7.23969 23 10.6 23H13.4C16.7603 23 18.4405 23 19.7239 22.346C20.8529 21.7708 21.7708 20.8529 22.346 19.7239C23 18.4405 23 16.7603 23 13.4V10.6C23 7.23969 23 5.55953 22.346 4.27606C21.7708 3.14708 20.8529 2.2292 19.7239 1.65396C18.4405 1 16.7603 1 13.4 1H10.6C7.23969 1 5.55953 1 4.27606 1.65396C3.14708 2.2292 2.2292 3.14708 1.65396 4.27606ZM13.4 3H10.6C8.88684 3 7.72225 3.00156 6.82208 3.0751C5.94524 3.14674 5.49684 3.27659 5.18404 3.43597C4.43139 3.81947 3.81947 4.43139 3.43597 5.18404C3.27659 5.49684 3.14674 5.94524 3.0751 6.82208C3.00156 7.72225 3 8.88684 3 10.6V13.4C3 15.1132 3.00156 16.2777 3.0751 17.1779C3.14674 18.0548 3.27659 18.5032 3.43597 18.816C3.81947 19.5686 4.43139 20.1805 5.18404 20.564C5.49684 20.7234 5.94524 20.8533 6.82208 20.9249C7.72225 20.9984 8.88684 21 10.6 21H13.4C15.1132 21 16.2777 20.9984 17.1779 20.9249C18.0548 20.8533 18.5032 20.7234 18.816 20.564C19.5686 20.1805 20.1805 19.5686 20.564 18.816C20.7234 18.5032 20.8533 18.0548 20.9249 17.1779C20.9984 16.2777 21 15.1132 21 13.4V10.6C21 8.88684 20.9984 7.72225 20.9249 6.82208C20.8533 5.94524 20.7234 5.49684 20.564 5.18404C20.1805 4.43139 19.5686 3.81947 18.816 3.43597C18.5032 3.27659 18.0548 3.14674 17.1779 3.0751C16.2777 3.00156 15.1132 3 13.4 3Z" fill="#FFFFFF"></path> </g></svg>
            </a>
            <a class="social linkdiscreto" href="https://www.linkedin.com/in/paulorobertovrc" target="_blank">
                <svg width="27px" height="27px" viewBox="0 0 21 21" fill="none" xmlns="http://www.w3.org/2000/svg"><g id="SVGRepo_bgCarrier" stroke-width="0"></g><g id="SVGRepo_tracerCarrier" stroke-linecap="round" stroke-linejoin="round"></g><g id="SVGRepo_iconCarrier"> <path d="M18.72 3.99997H5.37C5.19793 3.99191 5.02595 4.01786 4.86392 4.07635C4.70189 4.13484 4.55299 4.22471 4.42573 4.34081C4.29848 4.45692 4.19537 4.59699 4.12232 4.75299C4.04927 4.909 4.0077 5.07788 4 5.24997V18.63C4.01008 18.9901 4.15766 19.3328 4.41243 19.5875C4.6672 19.8423 5.00984 19.9899 5.37 20H18.72C19.0701 19.9844 19.4002 19.8322 19.6395 19.5761C19.8788 19.32 20.0082 18.9804 20 18.63V5.24997C20.0029 5.08247 19.9715 4.91616 19.9078 4.76122C19.8441 4.60629 19.7494 4.466 19.6295 4.34895C19.5097 4.23191 19.3672 4.14059 19.2108 4.08058C19.0544 4.02057 18.8874 3.99314 18.72 3.99997ZM9 17.34H6.67V10.21H9V17.34ZM7.89 9.12997C7.72741 9.13564 7.5654 9.10762 7.41416 9.04768C7.26291 8.98774 7.12569 8.89717 7.01113 8.78166C6.89656 8.66615 6.80711 8.5282 6.74841 8.37647C6.6897 8.22474 6.66301 8.06251 6.67 7.89997C6.66281 7.73567 6.69004 7.57169 6.74995 7.41854C6.80986 7.26538 6.90112 7.12644 7.01787 7.01063C7.13463 6.89481 7.2743 6.80468 7.42793 6.74602C7.58157 6.68735 7.74577 6.66145 7.91 6.66997C8.07259 6.66431 8.2346 6.69232 8.38584 6.75226C8.53709 6.8122 8.67431 6.90277 8.78887 7.01828C8.90344 7.13379 8.99289 7.27174 9.05159 7.42347C9.1103 7.5752 9.13699 7.73743 9.13 7.89997C9.13719 8.06427 9.10996 8.22825 9.05005 8.3814C8.99014 8.53456 8.89888 8.6735 8.78213 8.78931C8.66537 8.90513 8.5257 8.99526 8.37207 9.05392C8.21843 9.11259 8.05423 9.13849 7.89 9.12997ZM17.34 17.34H15V13.44C15 12.51 14.67 11.87 13.84 11.87C13.5822 11.8722 13.3313 11.9541 13.1219 12.1045C12.9124 12.2549 12.7546 12.4664 12.67 12.71C12.605 12.8926 12.5778 13.0865 12.59 13.28V17.34H10.29V10.21H12.59V11.21C12.7945 10.8343 13.0988 10.5225 13.4694 10.3089C13.84 10.0954 14.2624 9.98848 14.69 9.99997C16.2 9.99997 17.34 11 17.34 13.13V17.34Z" fill="#FFFFFF"></path> </g></svg>
            </a>
        </p>
</footer>
