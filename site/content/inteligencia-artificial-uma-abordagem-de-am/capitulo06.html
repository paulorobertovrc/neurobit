<!DOCTYPE html>
<html lang="pt-br">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="../../../site/style.css">
    <title>NeuroBit | @pauloroberto.dev</title>
    <link rel="shortcut icon" href="../../../imagens/favicon.png" type="image/x-icon">
</head>

<header class="breadcrumb" id="top">
    <nav>
        <a href="ia-am-faceli.html">Voltar à página anterior</a>
        <br/>
        <br/>
        <div class="header-info-container">
            <div class="container-esquerdo">
                <p class="numeracao-sequencial">2</p>
            </div>
            <div>
                <p class="titulo">Inteligência Artificial: Uma Abordagem de Aprendizado de Máquina</p>
                <p class="descricao">Katti Faceli et al.</p>
            </div>
        </div>
    </nav>
</header>

<body>
    <main>
        <h1># 2.6 Capítulo 6 (p. 78/100)</h1>
        <p><img src="https://img.shields.io/badge/Status-Estudando-grey?labelColor=31A8B8"></p>

        <article>
            <div>
                <h2>Parte 2 | Modelos preditivos (capítulos 4 a 10)</h2>
                
                <div>
                    <h3 id="subtitulo_0_nota_1_2"> 6. Métodos simbólicos</h3>
                    <p>
                        Englobam modelos cujo objetivo é representar explicitamente, através de estruturas simbólicas<sup><a class="linknotas" href="#nota_1">[1]</a></sup>, o conhecimento extraído do conjunto de dados. Esses métodos facilitam a interpretação do resultado por seres humanos, visto que asseguram "[...] uma compreensibilidade maior do processo decisório [...], estando mais alinhado[s] aos princípios de que os modelos de AM devem também ser 'explicáveis' (<em>Explainable Machine Learning</em>) para garantir maior transparência em sua operação." (FACELI et al., 2023, p. 78). Em contrapartida, se utilizados isoladamente, têm menor acurácia preditiva em comparação a outros modelos, ditos "caixa-preta"<sup><a class="linknotas" href="#nota_2">[2]</a></sup>.
                    </p>
                    <p>
                        Não obstante, é importante destacar que, "atualmente, existem algoritmos eficientes para indução de árvores de decisão ou conjuntos de regras e de aplicação eficiente, com um desempenho equivalente ao de outros modelos (como redes neurais e SVM), mas com maior grau de interpretabilidade. [...] A combinação de múltiplos modelos de árvores em comitês (<em>ensembles</em>) também tem se mostrado competitiva e é uma abordagem frequentemente empregada para aumentar o desempenho preditivo desses modelos." (FACELI et al., 2023, p. 97).
                    </p>
                </div>
                <div>
                    <h4 id="subtitulo_1_nota_3_4">6.1 Modelos baseados em árvores <sup><a class="linknotas" href="#nota_3">[3]</a></sup></h4>
                    <p>
                        Os modelos baseados em árvores utilizam a estrutura de dados homônima para solucionar problemas de classificação ou regressão, casos em que os algoritmos são respectivamente denominados <strong>árvores de decisão</strong> ou <strong>árvores de regressão</strong>. Em ambos os casos, a forma de se interpretar o modelo e de construir o algoritmo indutor da própria árvore são bastante similares e, de modo geral, o problema é abordado <strong>recursivamente</strong> por meio da estratégia da <strong>divisão e conquista</strong><sup><a class="linknotas" href="#nota_4">[4]</a></sup>.
                    </p>
                    <p>
                        Nesse sentido, "um problema complexo é dividido em problemas mais simples, aos quais recursivamente é aplicada a mesma estratégia. As soluções dos subproblemas podem ser combinadas, na forma de uma árvore, para produzir uma solução do problema complexo. A força dessa proposta vem da capacidade de dividir o espaço de instâncias em subespaços e cada subespaço é ajustado usando diferentes modelos." (FACELI et al., 2023, p. 78).
                    </p>
                    <p id="subtitulo_1_nota_5">
                        "Formalmente, uma árvore de decisão<sup><a class="linknotas" href="#nota_5">[5]</a></sup> é um grafo direcionado acíclico em que cada nó ou é um nó de divisão, com dois ou mais sucessores, ou um nó folha." (FACELI et al., 2023, p. 79). Os <strong>nós de divisão</strong> possuem testes condicionais de acordo com o valor do atributo que representam; os <strong>nós folha</strong> são funções que representam as saídas do modelo, possuindo os valores da variável alvo. Cada nó da árvore corresponde a uma região no espaço definido pelos atributos.
                    </p>
                    <p>
                        <blockquote>
                            "As regiões definidas pelas folhas da árvore são mutuamente excludentes, e a reunião dessas regiões cobre todo o espaço definido pelos atributos. A interseção das regiões abrangidas por quaisquer duas folhas é vazia. A união de todas as regiões (todas as folhas) é U. Uma árvore de decisão abrange todo o espaço de instâncias. Esse fato implica que uma árvore de decisão pode fazer predições para qualquer exemplo de entrada. [...] As condições ao longo de um ramo (um percurso entre a raiz e uma folha) são conjunções de condições e os ramos individuais são disjunções. Assim, cada ramo forma uma regra com uma parte condicional e uma conclusão. A parte condicional é uma conjunção de condições. Condições são testes que envolvem um atributo particular, operador [...] e um valor do domínio do atributo." (FACELI et al., 2023, p. 79).
                        </blockquote>
                    </p>
                    <p>
                        Para exemplificar, vejamos a imagem a seguir:
                        <div class="image-container">
                            <figure>
                                <img src="../../../imagens/21_am_faceli_arvore_de_decisao.png" alt="Árvore de decisão e regiões de decisão no espaço de objetos">
                                <figcaption class="legenda">Figura 21 - Árvore de decisão e regiões de decisão no espaço de objetos (FACELI et al., 2023, p. 79).</figcaption>
                            </figure>
                        </div>
                    </p>

                    <div>
                        <h5>6.1.1 Regras de divisão em tarefas de classificação</h5>
                        <p>
                            As regras de divisão servem para <strong>atenuar a impureza</strong> dos conjuntos de dados e balizam a construção da árvore de decisão no intuito de <strong>maximizar a homogeneidade</strong> dos subconjuntos gerados em cada recursão, de modo que garantir a congruência dos atributos no tocante à seleção daqueles que melhor discriminam cada classe (<em>goodness of split</em>). Para isso, normalmente adota-se a estratégia "olha para a frente um passo", de sorte que, uma vez tomada, a decisão não é reconsiderada pelo algoritmo. "Essa pesquisa de subida de encosta (<em>hill-climbing</em>) sem <em>backtracking</em> é suscetível aos riscos usuais de convergência a uma solução ótima localmente que não é ótima globalmente." (FACELI et al., 2023, p. 80).
                        </p>
                        <p>
                            "Uma proposta natural é rotular cada subconjunto da divisão por sua classe mais frequente e escolher a divisão que tem menores erros" (FACELI et al., 2023, p. 81), e as diversas propostas para isso convergem para a conclusão de que "[...] uma divisão que mantém a proporção de classes em todo o subconjunto não tem utilidade, e uma divisão na qual cada subconjunto contém somente exemplos de uma classe tem utilidade máxima." (FACELI et al., 2023, p. 81).
                        </p>
                        <p>
                            Assim, a melhor divisão é aquela que minimiza a impureza e, consequentemente, heterogeneidade dos subconjuntos. Em sentido logicamente contrário, portanto, é de se concluir que a divisão ideal busca maximizar a homogeneidade dos dados em cada subconjunto.
                        </p>
                        <p>
                            Conforme Martin (1997), os autores distinguem as métricas para avaliar a qualidade das divisões em subconjuntos em três grandes grupos (<strong>funções de mérito</strong>), considerando diferentes critérios: <strong>(1)</strong> baseadas na diferença entre a <strong>distribuição de dados antes e depois da divisão</strong>, que enfatizam a <strong>pureza dos subconjuntos</strong>; <strong>(2)</strong> baseadas em <strong>diferenças entre os subconjuntos</strong>, cujo enfoque é a <strong>disparidade entre os subconjuntos</strong> após a divisão; e <strong>(3)</strong> baseadas na <strong>confiabilidade dos subconjuntos</strong>, isto é, em medidas estatísticas de independência capazes de denotar que cada nó/subconjunto (atributo) é efetivamente adequado para produzir boas previsões, em que a ênfase é no <strong>peso da evidência.</strong>

                        </p>
                        <p>
                            Embora não haja consenso em relação à superioridade, a escolha por algum critério parece ser superior à divisão aleatória de atributos.
                        </p>
                        <p>
                            Nas tarefas de classificação, as regras baseadas no <strong>ganho de informação</strong> e no <strong>índice de Gini</strong> são as mais comuns.
                        </p>

                        <h6>6.1.1.1 Baseadas no ganho de informação</h6>
                        <p>
                            Esta regra é baseada no conceito de <strong>entropia</strong>, que informa a <strong>aleatoriedade de uma variável</strong> isto é, <strong>quantifica a dificuldade</strong> de predizê-la. A entropia também pode ser compreendida como uma medida da desordem ou impureza do conjunto de dados, e é medida em logaritmos na base 2. Logo, quanto maior a entropia, mais difícil será predizer o valor dessa variável aleatória, e <strong>a árvore de decisão é construída de modo a minimizar a dificuldade</strong> de predizer a variável alvo.
                        </p>
                        <p>
                            <blockquote>
                                "A cada nó de decisão, o atributo que mais reduz a aleatoriedade da variável alvo será escolhido para dividir os dados. [...] Os valores de um atributo definem partições [divisões] no conjunto de exemplos. Para cada atributo, o ganho de informação mede a redução na entropia nas partições obtidas de acordo com os valores do atributo. Informalmente, o ganho de informação é dado pela diferença entre a entropia do conjunto de exemplos e a soma ponderada da entropia das partições. A construção da árvore de decisão é guiada pelo objetivo de reduzir a entropia, isto é, a aleatoriedade (dificuldade para predizer) da variável alvo." (FACELI et al., 2023, p. 81).
                            </blockquote>
                        </p>
                        <p>
                            Em cada nó de divisão da árvore, o atributo que mais reduzir a entropia, consequentemente maximizando o ganho de informação, será escolhido para resolver o subproblema — leia-se, a divisão em subconjunto naquela etapa recursiva. Logo, é esperado que a cada divisão ocorra a diminuição da aleatoriedade — incerteza em relação à classificação correta da variável alvo — em virtude da consistência dos atributos que conformam o subconjunto como sendo aqueles que melhor discriminam as classes.
                        </p>
                        <p>
                            Por isso é que, essencialmente, o ganho de informação consiste na redução da aleatoriedade resultante da diferença entre a entropia de todo o conjunto de dados e a dos subconjuntos.
                        </p>
                        <h6 id="subtitulo_1_nota_6">6.1.1.2 Baseadas no índice de Gini <sup><a class="linknotas" href="#nota_6">[6]</a></sup></h6>
                        <p>
                            É uma métrica de <strong>impureza</strong> dos nós de decisão (subconjuntos). <strong>Avalia a probabilidade de que exemplos escolhidos ao acaso pertençam a classes diferentes, mas estejam no mesmo subconjunto.</strong> Quanto menor o índice de Gini, mais homogêneo — e menos impuro — é o subconjunto. Nesse sentido, o atributo que melhor discrimina a classe é aquele que minimiza o índice de Gini e, via de consequência, reduz a impureza e aumenta a homogeneidade dos subconjuntos.
                        </p>
                    </div>
                    <div>
                        <h5>6.1.2 Regras de divisão em tarefas de regressão</h5>
                        <p>
                            Nas tarefas de regressão, o critério mais usual é calcular a <strong>média do erro quadrático</strong> (erro quadrático médio — EQM ou <strong><em>mean squared error</em></strong> — MSE), objetivando que dessa divisão resultem subconjuntos compostos por elementos cujos valores aproximem-se entre si, consequentemente minimizando o erro. "Por esse motivo, a constante associada às folhas de uma árvore de regressão é a média dos valores do atributo alvo dos exemplos de treinamento que caem na folha." (FACELI et al., 2023, p. 85).
                        </p>
                        <p>
                            Uma forma de avaliar a qualidade das divisões foi proposta por Breiman et al. (1984) e consiste na <strong>redução do desvio padrão</strong> (<strong><em>standard deviation reduction</em></strong> - SDR), a ser calculada para cada subconjunto possível, de modo que seja o que ensejar a menor variância.
                        </p>
                    </div>
                    <div>
                        <h5>6.1.3 Valores desconhecidos</h5>
                        <p>
                            Submeter valores desconhecidos ou indeterminados ao modelo, isto é, que não foram explicitamente contemplados dentre os dados de treinamento, pode levar a resultados indesejados, "uma vez que uma árvore de decisão constitui uma hierarquia de testes [...]" (FACELI et al., 2023, p. 86).
                        </p>
                        <p>
                            Para resolver o chamado <strong>problema do valor desconhecido</strong>, há na literatura diversas propostas, como <strong>(1)</strong> atribuir-lhe o valor mais frequente; <strong>(2)</strong> considerá-lo também um valor possível para o atributo em questão; <strong>(3)</strong> associar probabilidades a todos os possíveis valores do atributo (utilizada no algoritmo C4.5); ou <strong>(4)</strong> implementar a estratégia da divisão substituta, que preconiza a utilização de atributos que gerem divisões similares àquela obtida pelo atributo selecionado para criar uma lista alternativa de atributos, possibilitando a busca por outro, que corresponda ao valor desconhecido (utilizada no algoritmo CART).
                        </p>
                    </div>
                </div>
            </div>
            <a class="emoji" href="#top">[ 🔝 ]</a>
        </article>
        <hr>
        <article>
            <h2>Principais tópicos</h2>
            <ul>
                <li>
                    <strong>Características gerais</strong>
                    <ul>
                        <li>Representação explícita do conhecimento extraído do conjunto de dados</li>
                        <li>
                            Vantagens
                            <ul>
                                <li>Favorece a interpretabilidade por seres humanos</li>
                                <li>O processo de tomada de decisão do modelo é mais compreensível e transparente (<em>explainable machine learning</em>)</li>
                                <li>Inferência lógica</li>
                            </ul>
                        </li>
                        <li>
                            Desvantagens
                            <ul>
                                <li>Menor acurácia preditiva em comparação aos modelos "caixa-preta"</li>
                            </ul>
                        </li>
                    </ul>
                </li>
                <li>
                    <strong>Modelos baseados em árvores</strong>
                    <ul>
                        <li>
                            Classificação (árvores de decisão) e regressão (árvores de regressão)
                            <ul>
                                <li>A interpretação dos modelos e implementação dos algoritmos construtores de ambas são similares</li>
                            </ul>
                        </li>
                        <li>Recursividade</li>
                        <li>Divisão e conquista</li>
                        <li>Nós de divisão (testes condicionais com base nos valores dos atributos) e folha (valores da variável alvo)</li>
                        <li>
                            <strong>Regras de divisão em tarefas de classificação</strong>
                            <ul>
                                <li>
                                    Minimizar a <strong>impureza</strong> e a heterogeneidade dos subconjuntos e, consequentemente, maximizar a homogeneidade dos dados que os compõem
                                </li>
                                <li>
                                    Seleção dos atributos que melhor discriminam cada classe (<em>goodness of split</em>)
                                </li>
                                <li>
                                    <strong>Ganho de informação</strong>
                                    <ul>
                                        <li>
                                            <strong>Entropia</strong>
                                            <ul>
                                                <li>Medida da desordem ou impureza do conjunto de dados</li>
                                                <li>A aleatoriedade de uma variável dificulta sua predição</li>
                                            </ul>
                                            <li>
                                                Exprime a redução da aleatoriedade/incerteza pela diferença da entropia de todo o conjunto de dados e a dos subconjuntos
                                            </li>
                                        </li>
                                    </ul>
                                </li>
                                <li>
                                    <strong>Índice de Gini</strong>
                                    <ul>
                                        <li>Medida do grau de desigualdade de uma distribuição estatística</li>
                                        <li>Medida de impureza dos nós de decisão (subconjuntos)</li>
                                        <li>
                                            Avalia a probabilidade de que exemplos escolhidos ao acaso pertençam a classes diferentes, mas tenham sido classificados no mesmo subconjunto
                                        </li>
                                    </ul>
                                </li>
                            </ul>
                        </li>
                        <li>
                            <strong>Regras de divisão em tarefas de regressão</strong>
                            <ul>
                                <li>Erro quadrático médio (EQM) ou <em>mean squared error</em>(MSE)</li>
                                <li>Redução do desvio padrão (<em>standard deviation reduction - SDR)</em></li>
                                <li>Subconjuntos formados por valores sejam parecidos</li>
                            </ul>
                        </li>
                        <li>Problema do valor desconhecido</li>
                    </ul>
                </li>
                <li><strong>Modelos baseados em regras</strong></li>
            </ul>
            <p><a class="emoji" href="#top">[ 🔝 ]</a></p>
        </article>
        <hr>
        <article>
            <h2>Referências complementares</h2>
            <p>
                CORMEN, Thomas H.; LEISERSON, Charles E.; RIVEST, Ronald L.; STEIN, Clifford. <strong>Algoritmos: teoria e prática</strong>. Trad. Arlete Simille Marques. 3. ed. Rio de Janeiro: Elsevier, 2012.
            </p>
            <p>
                HOFFMANN, Rodolfo. <strong>Estatística para economistas</strong>. 4. ed. São Paulo: Pioneira Thomson Learning, 2006.
            </p>
            <a class="emoji" href="#top">[ 🔝 ]</a>
        </article>
        <hr>
        <article>
            <h2>Notas</h2>
            <div>
                <p id="nota_1">[1] Neste contexto, o termo <strong>símbolo</strong> refere-se à abstração de conceitos, objetos ou relações do mundo real, que propriamente os representam ou a suas características e estados. Por conseguinte, <strong>estruturas simbólicas</strong> são agrupamentos desses elementos fundamentais, de modo a representar conhecimentos e relacionamentos mais complexos.<a class="linkdiscreto" href="#subtitulo_0_nota_1_2"> [ 🔙 ]</a></p>
                <p id="nota_2">[2] A literatura comumente utiliza a expressão "caixa-preta" (<em>black box</em>) para se referir a modelos cujo processo decisório não é facilmente inferível ou interpretável por seres humanos, como é o caso das redes neurais artificiais. A propósito, vide o fichamento <a href="../neural-newtorks-and-learning-machines-simon-haykin/introduction.html" target="_blank">1.2</a>, que contém algumas referências à terminologia no âmbito das redes neurais, em especial as notas 1 e 19.<a class="linkdiscreto" href="#subtitulo_0_nota_1_2"> [ 🔙 ]</a></p>
                <p id="nota_3">[3] Ver o <a href="../suplementos/03-ed.html">suplemento 3</a>, sobre estruturas de dados, para informações adicionais sobre <stong>árvores.</stong><a class="linkdiscreto" href="#subtitulo_1_nota_3_4"> [ 🔙 ]</a></p>
                <p id="nota_4">[4] A <strong>recursividade</strong> é uma característica de determinados algoritmos de se chamarem a si mesmos, uma ou mais vezes, a fim de fracionar um problema em tantos problemas menores quantos forem necessários, até que seja possível resolver o problema original. Normalmente, isso é feito por meio da abordagem da <strong>divisão e conquista</strong>, que em cada nível de recursão aplica três etapas: divisão, conquista e combinação (Cormen et al., 2012).<a class="linkdiscreto" href="#subtitulo_1_nota_3_4"> [ 🔙 ]</a></p>
                <p id="nota_5">[5] No livro, os autores utilizam o termo árvore de decisão para se referir, indistintamente, às árvores de decisão ou de regressão, inclusive neste caso, dado que a interpretação dos modelos e a indução da árvore são bastante similares. Todavia, ressalvam que, se necessária, haverá a devida distinção.<a class="linkdiscreto" href="#subtitulo_1_nota_3_4"> [ 🔙 ]</a></p>
                <p id="nota_6">[6] Na Economia, o <strong>índice de Gini</strong> é uma <strong>medida de desigualdade</strong> muito empregada para analisar a distribuição de renda, mas que pode ser usada para medir o grau de desigualdade de qualquer distribuição estatística, definido como a razão entre a <strong>área de desigualdade</strong>, obtida entre a <strong>linha da perfeita igualdade</strong> e a <strong>curva de Lorenz</strong>, e a área do triângulo formado pelos eixos do gráfico e a linha de perfeita igualdade (Hoffmann, 2006). Analogicamente, no contexto da aprendizagem de máquina, é possível trasladar esse raciocínio para a desigualdade - leia-se heterogeneidade - da distribuição dos elementos em um conjunto ou subconjunto de dados. <a class="linkdiscreto" href="#subtitulo_1_nota_6"> [ 🔙 ]</a></p>
            </div>
            <a class="emoji" href="#top">[ 🔝 ]</a>
        </article>
    </main>
</body>

<footer>
    <p>Projeto NeuroBit</p>
        <p class="linkdiscreto">
            <a class="social linkdiscreto" href="https://github.com/paulorobertovrc" target="_blank">
                <svg width="24px" height="24px" viewBox="0 0 20 20" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" fill="#ffffff"><g id="SVGRepo_bgCarrier" stroke-width="0"></g><g id="SVGRepo_tracerCarrier" stroke-linecap="round" stroke-linejoin="round"></g><g id="SVGRepo_iconCarrier"> <title>github [#142]</title> <desc>Created with Sketch.</desc> <defs> </defs> <g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"> <g id="Dribbble-Light-Preview" transform="translate(-140.000000, -7559.000000)" fill="#fff"> <g id="icons" transform="translate(56.000000, 160.000000)"> <path d="M94,7399 C99.523,7399 104,7403.59 104,7409.253 C104,7413.782 101.138,7417.624 97.167,7418.981 C96.66,7419.082 96.48,7418.762 96.48,7418.489 C96.48,7418.151 96.492,7417.047 96.492,7415.675 C96.492,7414.719 96.172,7414.095 95.813,7413.777 C98.04,7413.523 100.38,7412.656 100.38,7408.718 C100.38,7407.598 99.992,7406.684 99.35,7405.966 C99.454,7405.707 99.797,7404.664 99.252,7403.252 C99.252,7403.252 98.414,7402.977 96.505,7404.303 C95.706,7404.076 94.85,7403.962 94,7403.958 C93.15,7403.962 92.295,7404.076 91.497,7404.303 C89.586,7402.977 88.746,7403.252 88.746,7403.252 C88.203,7404.664 88.546,7405.707 88.649,7405.966 C88.01,7406.684 87.619,7407.598 87.619,7408.718 C87.619,7412.646 89.954,7413.526 92.175,7413.785 C91.889,7414.041 91.63,7414.493 91.54,7415.156 C90.97,7415.418 89.522,7415.871 88.63,7414.304 C88.63,7414.304 88.101,7413.319 87.097,7413.247 C87.097,7413.247 86.122,7413.234 87.029,7413.87 C87.029,7413.87 87.684,7414.185 88.139,7415.37 C88.139,7415.37 88.726,7417.2 91.508,7416.58 C91.513,7417.437 91.522,7418.245 91.522,7418.489 C91.522,7418.76 91.338,7419.077 90.839,7418.982 C86.865,7417.627 84,7413.783 84,7409.253 C84,7403.59 88.478,7399 94,7399" id="github-[#142]"> </path> </g> </g> </g> </g></svg>
            </a>
            <a class="social linkdiscreto" href="https://www.instagram.com/pauloroberto.dev" target="_blank">
                <svg width="24px" height="24px" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" stroke="#FFFFFF"><g id="SVGRepo_bgCarrier" stroke-width="0"></g><g id="SVGRepo_tracerCarrier" stroke-linecap="round" stroke-linejoin="round"></g><g id="SVGRepo_iconCarrier"> <path fill-rule="evenodd" clip-rule="evenodd" d="M12 18C15.3137 18 18 15.3137 18 12C18 8.68629 15.3137 6 12 6C8.68629 6 6 8.68629 6 12C6 15.3137 8.68629 18 12 18ZM12 16C14.2091 16 16 14.2091 16 12C16 9.79086 14.2091 8 12 8C9.79086 8 8 9.79086 8 12C8 14.2091 9.79086 16 12 16Z" fill="#FFFFFF"></path> <path d="M18 5C17.4477 5 17 5.44772 17 6C17 6.55228 17.4477 7 18 7C18.5523 7 19 6.55228 19 6C19 5.44772 18.5523 5 18 5Z" fill="#FFFFFF"></path> <path fill-rule="evenodd" clip-rule="evenodd" d="M1.65396 4.27606C1 5.55953 1 7.23969 1 10.6V13.4C1 16.7603 1 18.4405 1.65396 19.7239C2.2292 20.8529 3.14708 21.7708 4.27606 22.346C5.55953 23 7.23969 23 10.6 23H13.4C16.7603 23 18.4405 23 19.7239 22.346C20.8529 21.7708 21.7708 20.8529 22.346 19.7239C23 18.4405 23 16.7603 23 13.4V10.6C23 7.23969 23 5.55953 22.346 4.27606C21.7708 3.14708 20.8529 2.2292 19.7239 1.65396C18.4405 1 16.7603 1 13.4 1H10.6C7.23969 1 5.55953 1 4.27606 1.65396C3.14708 2.2292 2.2292 3.14708 1.65396 4.27606ZM13.4 3H10.6C8.88684 3 7.72225 3.00156 6.82208 3.0751C5.94524 3.14674 5.49684 3.27659 5.18404 3.43597C4.43139 3.81947 3.81947 4.43139 3.43597 5.18404C3.27659 5.49684 3.14674 5.94524 3.0751 6.82208C3.00156 7.72225 3 8.88684 3 10.6V13.4C3 15.1132 3.00156 16.2777 3.0751 17.1779C3.14674 18.0548 3.27659 18.5032 3.43597 18.816C3.81947 19.5686 4.43139 20.1805 5.18404 20.564C5.49684 20.7234 5.94524 20.8533 6.82208 20.9249C7.72225 20.9984 8.88684 21 10.6 21H13.4C15.1132 21 16.2777 20.9984 17.1779 20.9249C18.0548 20.8533 18.5032 20.7234 18.816 20.564C19.5686 20.1805 20.1805 19.5686 20.564 18.816C20.7234 18.5032 20.8533 18.0548 20.9249 17.1779C20.9984 16.2777 21 15.1132 21 13.4V10.6C21 8.88684 20.9984 7.72225 20.9249 6.82208C20.8533 5.94524 20.7234 5.49684 20.564 5.18404C20.1805 4.43139 19.5686 3.81947 18.816 3.43597C18.5032 3.27659 18.0548 3.14674 17.1779 3.0751C16.2777 3.00156 15.1132 3 13.4 3Z" fill="#FFFFFF"></path> </g></svg>
            </a>
            <a class="social linkdiscreto" href="https://www.linkedin.com/in/paulorobertovrc" target="_blank">
                <svg width="27px" height="27px" viewBox="0 0 21 21" fill="none" xmlns="http://www.w3.org/2000/svg"><g id="SVGRepo_bgCarrier" stroke-width="0"></g><g id="SVGRepo_tracerCarrier" stroke-linecap="round" stroke-linejoin="round"></g><g id="SVGRepo_iconCarrier"> <path d="M18.72 3.99997H5.37C5.19793 3.99191 5.02595 4.01786 4.86392 4.07635C4.70189 4.13484 4.55299 4.22471 4.42573 4.34081C4.29848 4.45692 4.19537 4.59699 4.12232 4.75299C4.04927 4.909 4.0077 5.07788 4 5.24997V18.63C4.01008 18.9901 4.15766 19.3328 4.41243 19.5875C4.6672 19.8423 5.00984 19.9899 5.37 20H18.72C19.0701 19.9844 19.4002 19.8322 19.6395 19.5761C19.8788 19.32 20.0082 18.9804 20 18.63V5.24997C20.0029 5.08247 19.9715 4.91616 19.9078 4.76122C19.8441 4.60629 19.7494 4.466 19.6295 4.34895C19.5097 4.23191 19.3672 4.14059 19.2108 4.08058C19.0544 4.02057 18.8874 3.99314 18.72 3.99997ZM9 17.34H6.67V10.21H9V17.34ZM7.89 9.12997C7.72741 9.13564 7.5654 9.10762 7.41416 9.04768C7.26291 8.98774 7.12569 8.89717 7.01113 8.78166C6.89656 8.66615 6.80711 8.5282 6.74841 8.37647C6.6897 8.22474 6.66301 8.06251 6.67 7.89997C6.66281 7.73567 6.69004 7.57169 6.74995 7.41854C6.80986 7.26538 6.90112 7.12644 7.01787 7.01063C7.13463 6.89481 7.2743 6.80468 7.42793 6.74602C7.58157 6.68735 7.74577 6.66145 7.91 6.66997C8.07259 6.66431 8.2346 6.69232 8.38584 6.75226C8.53709 6.8122 8.67431 6.90277 8.78887 7.01828C8.90344 7.13379 8.99289 7.27174 9.05159 7.42347C9.1103 7.5752 9.13699 7.73743 9.13 7.89997C9.13719 8.06427 9.10996 8.22825 9.05005 8.3814C8.99014 8.53456 8.89888 8.6735 8.78213 8.78931C8.66537 8.90513 8.5257 8.99526 8.37207 9.05392C8.21843 9.11259 8.05423 9.13849 7.89 9.12997ZM17.34 17.34H15V13.44C15 12.51 14.67 11.87 13.84 11.87C13.5822 11.8722 13.3313 11.9541 13.1219 12.1045C12.9124 12.2549 12.7546 12.4664 12.67 12.71C12.605 12.8926 12.5778 13.0865 12.59 13.28V17.34H10.29V10.21H12.59V11.21C12.7945 10.8343 13.0988 10.5225 13.4694 10.3089C13.84 10.0954 14.2624 9.98848 14.69 9.99997C16.2 9.99997 17.34 11 17.34 13.13V17.34Z" fill="#FFFFFF"></path> </g></svg>
            </a>
        </p>
</footer>
