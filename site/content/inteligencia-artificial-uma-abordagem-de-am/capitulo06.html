<!DOCTYPE html>
<html lang="pt-br">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="../../../site/style.css">
    <title>NeuroBit | @pauloroberto.dev</title>
    <link rel="shortcut icon" href="../../../imagens/favicon.png" type="image/x-icon">
</head>

<header class="breadcrumb" id="top">
    <nav>
        <a href="ia-am-faceli.html">Voltar √† p√°gina anterior</a>
        <br/>
        <br/>
        <div class="header-info-container">
            <div class="container-esquerdo">
                <p class="numeracao-sequencial">2</p>
            </div>
            <div>
                <p class="titulo">Intelig√™ncia Artificial: Uma Abordagem de Aprendizado de M√°quina</p>
                <p class="descricao">Katti Faceli et al.</p>
            </div>
        </div>
    </nav>
</header>

<body>
    <main>
        <h1># 2.6 Cap√≠tulo 6 (p. 78/100)</h1>
        <p><img src="https://img.shields.io/badge/Status-Estudando-grey?labelColor=31A8B8"></p>

        <article>
            <div>
                <h2>Parte 2 | Modelos preditivos (cap√≠tulos 4 a 10)</h2>
                
                <div>
                    <h3 id="subtitulo_0_nota_1_2"> 6. M√©todos simb√≥licos</h3>
                    <p>
                        Englobam modelos cujo objetivo √© representar explicitamente, atrav√©s de estruturas simb√≥licas<sup><a class="linknotas" href="#nota_1">[1]</a></sup>, o conhecimento extra√≠do do conjunto de dados. Esses m√©todos facilitam a interpreta√ß√£o do resultado por seres humanos, visto que asseguram "[...] uma compreensibilidade maior do processo decis√≥rio [...], estando mais alinhado[s] aos princ√≠pios de que os modelos de AM devem tamb√©m ser 'explic√°veis' (<em>Explainable Machine Learning</em>) para garantir maior transpar√™ncia em sua opera√ß√£o." (FACELI et al., 2023, p. 78). Em contrapartida, se utilizados isoladamente, t√™m menor acur√°cia preditiva em compara√ß√£o a outros modelos, ditos "caixa-preta"<sup><a class="linknotas" href="#nota_2">[2]</a></sup>.
                    </p>
                    <p>
                        N√£o obstante, √© importante destacar que, "atualmente, existem algoritmos eficientes para indu√ß√£o de √°rvores de decis√£o ou conjuntos de regras e de aplica√ß√£o eficiente, com um desempenho equivalente ao de outros modelos (como redes neurais e SVM), mas com maior grau de interpretabilidade. [...] A combina√ß√£o de m√∫ltiplos modelos de √°rvores em comit√™s (<em>ensembles</em>) tamb√©m tem se mostrado competitiva e √© uma abordagem frequentemente empregada para aumentar o desempenho preditivo desses modelos." (FACELI et al., 2023, p. 97).
                    </p>
                </div>
                <div>
                    <h4 id="subtitulo_1_nota_3_4">6.1 Modelos baseados em √°rvores <sup><a class="linknotas" href="#nota_3">[3]</a></sup></h4>
                    <p>
                        Os modelos baseados em √°rvores utilizam a estrutura de dados hom√¥nima para solucionar problemas de classifica√ß√£o ou regress√£o, casos em que os algoritmos s√£o respectivamente denominados <strong>√°rvores de decis√£o</strong> ou <strong>√°rvores de regress√£o</strong>. Em ambos os casos, a forma de se interpretar o modelo e de construir o algoritmo indutor da pr√≥pria √°rvore s√£o bastante similares e, de modo geral, o problema √© abordado <strong>recursivamente</strong> por meio da estrat√©gia da <strong>divis√£o e conquista</strong><sup><a class="linknotas" href="#nota_4">[4]</a></sup> sem <em>backtracking</em>.
                    </p>
                    <p>
                        <blockquote>
                            "Usualmente, os algoritmos exploram heur√≠sticas que localmente executam uma pesquisa olha para a frente um passo. Uma vez que uma decis√£o √© tomada, ela nunca √© reconsiderada. Essa pesquisa de subida de encosta (<em>hill-climbing</em>) sem <em>backtracking</em> √© suscet√≠vel aos riscos usuais de converg√™ncia de uma solu√ß√£o √≥tima localmente que n√£o √© √≥tima globalmente. Por outro lado, essa estrat√©gia permite construir √°rvores de decis√£o em tempo linear no n√∫mero de exemplos." (FACELI et al., 2023, p. 80).
                        </blockquote>
                    </p>
                    <p>
                        Nesse sentido, "um problema complexo √© dividido em problemas mais simples, aos quais recursivamente √© aplicada a mesma estrat√©gia. As solu√ß√µes dos subproblemas podem ser combinadas, na forma de uma √°rvore, para produzir uma solu√ß√£o do problema complexo. A for√ßa dessa proposta vem da capacidade de dividir o espa√ßo de inst√¢ncias em subespa√ßos e cada subespa√ßo √© ajustado usando diferentes modelos." (FACELI et al., 2023, p. 78).
                    </p>
                    <p id="subtitulo_1_nota_5">
                        "Formalmente, uma √°rvore de decis√£o<sup><a class="linknotas" href="#nota_5">[5]</a></sup> √© um grafo direcionado ac√≠clico em que cada n√≥ ou √© um n√≥ de divis√£o, com dois ou mais sucessores, ou um n√≥ folha." (FACELI et al., 2023, p. 79). Os <strong>n√≥s de divis√£o</strong> possuem testes condicionais de acordo com o valor do atributo que representam; os <strong>n√≥s folha</strong> s√£o fun√ß√µes que representam as sa√≠das do modelo, possuindo os valores da vari√°vel alvo. Cada n√≥ da √°rvore corresponde a uma regi√£o no espa√ßo definido pelos atributos.
                    </p>
                    <p>
                        <blockquote>
                            "As regi√µes definidas pelas folhas da √°rvore s√£o mutuamente excludentes, e a reuni√£o dessas regi√µes cobre todo o espa√ßo definido pelos atributos. A interse√ß√£o das regi√µes abrangidas por quaisquer duas folhas √© vazia. A uni√£o de todas as regi√µes (todas as folhas) √© U. Uma √°rvore de decis√£o abrange todo o espa√ßo de inst√¢ncias. Esse fato implica que uma √°rvore de decis√£o pode fazer predi√ß√µes para qualquer exemplo de entrada. [...] As condi√ß√µes ao longo de um ramo (um percurso entre a raiz e uma folha) s√£o conjun√ß√µes de condi√ß√µes e os ramos individuais s√£o disjun√ß√µes. Assim, cada ramo forma uma regra com uma parte condicional e uma conclus√£o. A parte condicional √© uma conjun√ß√£o de condi√ß√µes. Condi√ß√µes s√£o testes que envolvem um atributo particular, operador [...] e um valor do dom√≠nio do atributo." (FACELI et al., 2023, p. 79).
                        </blockquote>
                    </p>
                    <p>
                        Para exemplificar, vejamos a imagem a seguir:
                        <div class="image-container">
                            <figure>
                                <img src="../../../imagens/21_am_faceli_arvore_de_decisao.png" alt="√Årvore de decis√£o e regi√µes de decis√£o no espa√ßo de objetos">
                                <figcaption class="legenda">Figura 21 - √Årvore de decis√£o e regi√µes de decis√£o no espa√ßo de objetos (FACELI et al., 2023, p. 79).</figcaption>
                            </figure>
                        </div>
                    </p>
                    <p id="subtitulo_1_nota_6_7_8_9_10_11_12_13_14">
                        Os modelos baseados em √°rvores possuem <strong>vantagens</strong> como <strong>flexibilidade</strong> ‚Äî por serem n√£o param√©tricos, n√£o pressup√µem alguma distribui√ß√£o espec√≠fica de dados<sup><a class="linknotas" href="#nota_6">[6]</a></sup> ‚Äî, <strong>robustez</strong> ‚Äî lidam bem com transforma√ß√µes de vari√°veis que preservem a ordem dos dados, n√£o modificando a estrutura da √°rvore e a l√≥gica do processo decis√≥rio<sup><a class="linknotas" href="#nota_7">[7]</a></sup> ‚Äî, <strong>autonomia na sele√ß√£o de atributos</strong><sup><a class="linknotas" href="#nota_8">[8]</a></sup>, <strong>interpretabilidade</strong><sup><a class="linknotas" href="#nota_9">[9]</a></sup> e, em geral, <strong>efici√™ncia</strong><sup><a class="linknotas" href="#nota_10">[10]</a></sup>. Dentre as <strong>desvantagens</strong>, est√£o a <strong>replica√ß√£o</strong><sup><a class="linknotas" href="#nota_11">[11]</a></sup>, a <strong>instabilidade</strong><sup><a class="linknotas" href="#nota_12">[12]</a></sup> e a inefici√™ncia em cen√°rios espec√≠ficos, como dados com <strong>valores ausentes</strong><sup><a class="linknotas" href="#nota_13">[13]</a></sup> e <strong>atributos cont√≠nuos</strong><sup><a class="linknotas" href="#nota_14">[14]</a></sup>.
                    </p>
                    <p>
                        H√° tamb√©m modelos n√£o convencionais como as <strong>√°rvores de modelos e de op√ß√µes</strong>, alternativas que podem ser usadas em tarefas de regress√£o e classifica√ß√£o, respectivamente.
                    </p>
                    <p>
                        <blockquote>
                            "Uma <strong>√°rvore de modelos</strong> (do ingl√™s <em>model tree</em>) [...] combina √°rvore de regress√£o com equa√ß√µes de regress√£o. Esse tipo de √°rvore funciona da mesma maneira que uma √°rvore de regress√£o, por√©m <strong>os n√≥s folha cont√™m express√µes lineares em vez de valores agregados (m√©dias ou medianas).</strong> A estrutura da √°rvore divide o espa√ßo dos atributos em subespa√ßos, e os exemplos em cada um dos subespa√ßos s√£o aproximados por uma fun√ß√£o linear. A <em>model tree</em> √© <strong>menor e mais compreens√≠vel</strong> que uma √°rvore de regress√£o e, mesmo assim, apresenta um erro m√©dio menor na predi√ß√£o." (FACELI et al., 2023, p. 96, destaquei).
                        </blockquote>
                    </p>
                    <p>
                        Em tarefas de classifica√ß√£o, as <strong>√°rvores de op√ß√£o</strong> "[...] podem incluir <em>n√≥s de op√ß√£o</em>, que trocam o usual teste no valor de um atributo por um conjunto de testes, cada um dos quais sobre o valor de um atributo. Um n√≥ de op√ß√£o √© como um n√≥ <em>ou</em> em √°rvores <em>e-ou</em>. <strong>Na constru√ß√£o da √°rvore, em vez de selecionar o <em>melhor</em> atributo, s√£o selecionados todos os atributos promissores, aqueles com maior valor do ganho de informa√ß√£o.</strong> Para cada atributo selecionado, uma √°rvore de decis√£o √© constru√≠da. √â de salientar que uma √°rvore de op√ß√£o pode ter tr√™s tipos de n√≥s: n√≥s com somente um atributo teste - <em>n√≥s de decis√£o</em>; n√≥s com disjun√ß√µes dos atributos de teste - <em>n√≥s de op√ß√£o</em>; e n√≥s folhas." (FACELI et al., 2023, p. 97, destaquei). Em contrapartida, o consumo de recursos computacionais, notadamente mem√≥ria e espa√ßo para armazenamento, √© substancialmente maior.
                    </p>

                    <div>
                        <h5>6.1.1 Regras de divis√£o em tarefas de classifica√ß√£o</h5>
                        <p>
                            As regras de divis√£o servem para <strong>atenuar a impureza</strong> dos conjuntos de dados e balizam a constru√ß√£o da √°rvore de decis√£o no intuito de <strong>maximizar a homogeneidade</strong> dos subconjuntos gerados em cada recurs√£o, de modo que garantir a congru√™ncia dos atributos no tocante √† sele√ß√£o daqueles que melhor discriminam cada classe (<em>goodness of split</em>).
                        </p>
                        <p>
                            "Uma proposta natural √© rotular cada subconjunto da divis√£o por sua classe mais frequente e escolher a divis√£o que tem menores erros" (FACELI et al., 2023, p. 81), e as diversas propostas para isso convergem para a conclus√£o de que "[...] uma divis√£o que mant√©m a propor√ß√£o de classes em todo o subconjunto n√£o tem utilidade, e uma divis√£o na qual cada subconjunto cont√©m somente exemplos de uma classe tem utilidade m√°xima." (FACELI et al., 2023, p. 81).
                        </p>
                        <p>
                            Assim, a melhor divis√£o √© aquela que minimiza a impureza e, consequentemente, heterogeneidade dos subconjuntos. Em sentido logicamente contr√°rio, portanto, √© de se concluir que a divis√£o ideal busca maximizar a homogeneidade dos dados em cada subconjunto.
                        </p>
                        <p>
                            Conforme Martin (1997), os autores distinguem as m√©tricas para avaliar a qualidade das divis√µes em subconjuntos em tr√™s grandes grupos (<strong>fun√ß√µes de m√©rito</strong>), considerando diferentes crit√©rios: <strong>(1)</strong> baseadas na diferen√ßa entre a <strong>distribui√ß√£o de dados antes e depois da divis√£o</strong>, que enfatizam a <strong>pureza dos subconjuntos</strong>; <strong>(2)</strong> baseadas em <strong>diferen√ßas entre os subconjuntos</strong>, cujo enfoque √© a <strong>disparidade entre os subconjuntos</strong> ap√≥s a divis√£o; e <strong>(3)</strong> baseadas na <strong>confiabilidade dos subconjuntos</strong>, isto √©, em medidas estat√≠sticas de independ√™ncia capazes de denotar que cada n√≥/subconjunto (atributo) √© efetivamente adequado para produzir boas previs√µes, em que a √™nfase √© no <strong>peso da evid√™ncia.</strong>

                        </p>
                        <p>
                            Embora n√£o haja consenso em rela√ß√£o √† superioridade, a escolha por algum crit√©rio parece ser superior √† divis√£o aleat√≥ria de atributos.
                        </p>
                        <p>
                            Nas tarefas de classifica√ß√£o, as regras baseadas no <strong>ganho de informa√ß√£o</strong> e no <strong>√≠ndice de Gini</strong> s√£o as mais comuns.
                        </p>

                        <h6>6.1.1.1 Baseadas no ganho de informa√ß√£o</h6>
                        <p>
                            Esta regra √© baseada no conceito de <strong>entropia</strong>, que informa a <strong>aleatoriedade de uma vari√°vel</strong> isto √©, <strong>quantifica a dificuldade</strong> de prediz√™-la. A entropia tamb√©m pode ser compreendida como uma medida da desordem ou impureza do conjunto de dados, e √© medida em logaritmos na base 2. Logo, quanto maior a entropia, mais dif√≠cil ser√° predizer o valor dessa vari√°vel aleat√≥ria, e <strong>a √°rvore de decis√£o √© constru√≠da de modo a minimizar a dificuldade</strong> de predizer a vari√°vel alvo.
                        </p>
                        <p>
                            <blockquote>
                                "A cada n√≥ de decis√£o, o atributo que mais reduz a aleatoriedade da vari√°vel alvo ser√° escolhido para dividir os dados. [...] Os valores de um atributo definem parti√ß√µes [leia-se divis√µes] no conjunto de exemplos. Para cada atributo, o ganho de informa√ß√£o mede a redu√ß√£o na entropia nas parti√ß√µes obtidas de acordo com os valores do atributo. Informalmente, o ganho de informa√ß√£o √© dado pela diferen√ßa entre a entropia do conjunto de exemplos e a soma ponderada da entropia das parti√ß√µes. A constru√ß√£o da √°rvore de decis√£o √© guiada pelo objetivo de reduzir a entropia, isto √©, a aleatoriedade (dificuldade para predizer) da vari√°vel alvo." (FACELI et al., 2023, p. 81).
                            </blockquote>
                        </p>
                        <p>
                            Em cada n√≥ de divis√£o da √°rvore, o atributo que mais reduzir a entropia, consequentemente maximizando o ganho de informa√ß√£o, ser√° escolhido para resolver o subproblema ‚Äî leia-se, a divis√£o em subconjunto naquela etapa recursiva. Logo, √© esperado que a cada divis√£o ocorra a diminui√ß√£o da aleatoriedade ‚Äî incerteza em rela√ß√£o √† classifica√ß√£o correta da vari√°vel alvo ‚Äî em virtude da consist√™ncia dos atributos que conformam o subconjunto como sendo aqueles que melhor discriminam as classes.
                        </p>
                        <p>
                            Por isso √© que, essencialmente, o ganho de informa√ß√£o consiste na redu√ß√£o da aleatoriedade resultante da diferen√ßa entre a entropia de todo o conjunto de dados e a dos subconjuntos.
                        </p>
                        <h6 id="subtitulo_1_nota_15">6.1.1.2 Baseadas no √≠ndice de Gini <sup><a class="linknotas" href="#nota_15">[15]</a></sup></h6>
                        <p>
                            √â uma m√©trica de <strong>impureza</strong> dos n√≥s de decis√£o (subconjuntos). <strong>Avalia a probabilidade de que exemplos escolhidos ao acaso perten√ßam a classes diferentes, mas estejam no mesmo subconjunto.</strong> Quanto menor o √≠ndice de Gini, mais homog√™neo ‚Äî e menos impuro ‚Äî √© o subconjunto. Nesse sentido, o atributo que melhor discrimina a classe √© aquele que minimiza o √≠ndice de Gini e, via de consequ√™ncia, reduz a impureza e aumenta a homogeneidade dos subconjuntos.
                        </p>
                    </div>
                    <div>
                        <h5>6.1.2 Regras de divis√£o em tarefas de regress√£o</h5>
                        <p>
                            Nas tarefas de regress√£o, o crit√©rio mais usual √© calcular a <strong>m√©dia do erro quadr√°tico</strong> (erro quadr√°tico m√©dio ‚Äî EQM ou <strong><em>mean squared error</em></strong> ‚Äî MSE), objetivando que dessa divis√£o resultem subconjuntos compostos por elementos cujos valores aproximem-se entre si, consequentemente minimizando o erro. "Por esse motivo, a constante associada √†s folhas de uma √°rvore de regress√£o √© a m√©dia dos valores do atributo alvo dos exemplos de treinamento que caem na folha." (FACELI et al., 2023, p. 85).
                        </p>
                        <p>
                            Uma forma de avaliar a qualidade das divis√µes foi proposta por Breiman et al. (1984) e consiste na <strong>redu√ß√£o do desvio padr√£o</strong> (<strong><em>standard deviation reduction</em></strong> - SDR), a ser calculada para cada subconjunto poss√≠vel, de modo que seja o que ensejar a menor vari√¢ncia.
                        </p>
                    </div>
                    <div>
                        <h5>6.1.3 Valores desconhecidos</h5>
                        <p>
                            Submeter valores desconhecidos ou indeterminados ao modelo, isto √©, que n√£o foram explicitamente contemplados dentre os dados de treinamento, pode levar a resultados indesejados, "uma vez que uma √°rvore de decis√£o constitui uma hierarquia de testes [...]" (FACELI et al., 2023, p. 86).
                        </p>
                        <p>
                            Para resolver o chamado <strong>problema do valor desconhecido</strong>, h√° na literatura diversas propostas, como <strong>(1)</strong> atribuir-lhe o valor mais frequente; <strong>(2)</strong> consider√°-lo tamb√©m um valor poss√≠vel para o atributo em quest√£o; <strong>(3)</strong> associar probabilidades a todos os poss√≠veis valores do atributo (utilizada no algoritmo C4.5); ou <strong>(4)</strong> implementar a estrat√©gia da divis√£o substituta, que preconiza a utiliza√ß√£o de atributos que gerem divis√µes similares √†quela obtida pelo atributo selecionado para criar uma lista alternativa de atributos, possibilitando a busca por outro, que corresponda ao valor desconhecido (utilizada no algoritmo CART).
                        </p>
                    </div>
                    <div>
                        <h5 id="subtitulo_1_nota_16">6.1.4 Estrat√©gias de poda</h5>
                        <p>
                            A poda de uma √°rvore de decis√£o (<em>decision tree pruning</em>) consiste na diminui√ß√£o de seu tamanho, substituindo por folhas os n√≥s demasiadamente profundos<sup><a class="linknotas" href="#nota_16">[16]</a></sup> (elimina√ß√£o de ramos ou sub√°rvores), com o objetivo de aumentar a <strong>confiabilidade</strong> do modelo e tornar o processo decis√≥rio ainda mais <strong>compreens√≠vel</strong>. O procedimento tende a melhorar a capacidade de <strong>generaliza√ß√£o</strong> do modelo e √© de especial import√¢ncia em cen√°rios com dados ruidosos.
                        </p>
                        <p>
                            <blockquote>
                                "Dados ruidosos levantam dois problemas. O primeiro √© que as √°rvores induzidas classificam novos objetos em um modo n√£o confi√°vel. Estat√≠sticas calculadas nos n√≥s mais profundos de uma √°rvore t√™m baixos n√≠veis de import√¢ncia em fun√ß√£o do pequeno n√∫mero de exemplos que chegam nesses n√≥s. N√≥s mais profundos refletem mais o conjunto de treinamento (superajuste) e aumentam o erro em raz√£o da vari√¢ncia do classificador. O segundo √© que a √°rvore induzida tende a ser grande e, portanto, dif√≠cil para compreender." (FACELI et al., 2023, p. 86).
                            </blockquote>
                        </p>
                        <p>
                            √â poss√≠vel realizar a poda concomitantemente √† constru√ß√£o da √°rvore, interrompendo o processo conforme algum crit√©rio predeterminado, e outras que aguardam o t√©rmino, respectivamente denominadas <strong>pr√©-poda</strong> e <strong>p√≥s-poda</strong>. N√£o obstante, "tal como as regras de divis√£o, a poda √© um dom√≠nio em que nenhuma proposta existente √© a melhor para todos os casos. A poda √© um enviesamento em dire√ß√£o √† simplicidade. Se o dom√≠nio do problema admite solu√ß√µes simples, ent√£o a poda √© uma op√ß√£o eficiente (Schaffer, 1993)." (FACELI et al., 2023, p. 88).
                        </p>
                        <p>
                            A <strong>pr√©-poda</strong> implementa regras que interrompem a constru√ß√£o de ramos que aparentemente n√£o contribuiriam para incrementar a acur√°cia preditiva da √°rvore, evitando desde o in√≠cio a cria√ß√£o de n√≥s considerados in√∫teis, o que economiza tempo e recursos computacionais. Embora a literatura enumere diversas regras poss√≠veis, s√£o majoritariamente aceitas a assun√ß√£o de que <strong>(1)</strong> todos os objetos que alcancem um determinado n√≥ s√£o da mesma classe e/ou de que <strong>(2)</strong> todos os elementos que o alcancem possuem caracter√≠sticas id√™nticas, embora n√£o necessariamente perten√ßam √† mesma classe.
                        </p>
                        <p id="subtitulo_1_nota_17">
                            Todavia, as estrat√©gias de <strong>p√≥s-poda</strong> s√£o mais comuns e resultam em modelos mais confi√°veis, embora o processo construtivo seja mais demorado porque "uma √°rvore completa, superajustada aos dados de treinamento, √© gerada e podada posteriormente." (FACELI et al., 2023, p. 87). Logicamente que, por esse motivo, h√° maior consumo de tempo e recursos computacionais. Dentre os m√©todos de p√≥s-poda, s√£o elencados os <strong>(1)</strong> baseados nas medidas de <strong>erro est√°tico e erro de <em>backed-up</em></strong>; <strong>(2)</strong> a poda <strong>custo de complexidade</strong>, um dos mais utilizados, introduzido por Breiman et al. (1984) no algoritmo CART; e <strong>(3)</strong> a poda <strong>pessimista</strong>, apresentada e adotada por Quinlan (1988) no algoritmo C4.5<sup><a class="linknotas" href="#nota_17">[17]</a></sup>.
                        </p>
                    </div>
                </div>
                <div>
                    <h4>6.2 Modelos baseados em regras</h4>
                    <p>
                        Uma regra de decis√£o compara logicamente um atributo e os valores conhecidos do dom√≠nio e, assim como nas √°rvores de decis√£o, seu espa√ßo de hip√≥teses √© dado sob a forma disjuntiva.
                    </p>
                    <p>
                        A despeito da grande semelhan√ßa, as regras de decis√£o flexibilizam algumas caracter√≠sticas inerentes √†s √°rvores de decis√£o que podem ser desvantajosas, como a replica√ß√£o e a instabilidade, e tornam o processo decis√≥rio modular, visto que as regras podem ser avaliadas isoladamente, sem que modifica√ß√µes realizadas em determinada regra condicional afetem as subsequentes.
                    </p>
                    <p>
                        <blockquote>
                            "Regras de decis√£o e √°rvores de decis√£o s√£o bastante similares em suas formas de representa√ß√£o para expressar generaliza√ß√µes dos exemplos. Ambas definem superf√≠cies de decis√£o similares. [...] <strong>Como as √°rvores de decis√£o cobrem todo o espa√ßo de inst√¢ncias, a vantagem √© que qualquer exemplo √© classificado por uma √°rvore de decis√£o. Entretanto, cada teste em um n√≥ tem um contexto definido por testes anteriores, definidos nos n√≥s no caminho, que podem ser problem√°ticos se levarmos em conta a interpretabilidade. Por outro lado, as regras s√£o modulares, ou seja, podem ser interpretadas isoladamente.</strong> Cada regra cobre uma regi√£o espec√≠fica do espa√ßo de inst√¢ncias. A uni√£o de todas as regras pode ser menor que o Universo." (FACELI et al., 2023, p. 90, destaquei).
                        </blockquote>
                    </p>
                    <p>
                        Para exemplificar, vejamos a imagem a seguir:
                        <div class="image-container">
                            <figure>
                                <img src="../../../imagens/22_am_faceli_regras_de_decisao.png" alt="Exemplos de superf√≠cies de decis√£o desenhadas por um conjunto de regras">
                                <figcaption class="legenda">Figura 22 - Exemplos de superf√≠cies de decis√£o desenhadas por um conjunto de regras (FACELI et al., 2023, p. 90).</figcaption>
                            </figure>
                        </div>
                    </p>
                    <p>
                        A similaridade permite a convers√£o de √°rvores em conjuntos ou listas de regras de decis√£o, tal que cada folha da √°rvore corresponda a uma regra. Embora a regra contemple o percurso por toda a altura da √°rvore ‚Äî da raiz √† folha ‚Äî, √© poss√≠vel otimizar a representa√ß√£o, simplificando-a por meio da remo√ß√£o de condi√ß√µes redundantes ou irrelevantes. Justifica-se essa abordagem:
                        <blockquote>
                            "<strong>√Årvores de decis√£o extensas s√£o de dif√≠cil compreens√£o porque o teste de decis√£o em cada n√≥ aparece em um contexto espec√≠fico, definido pelo resultado de todos os testes nos n√≥s antecedentes.</strong> O trabalho desenvolvido por Rivest (1987) apresenta as <em>listas de decis√£o</em>, uma nova representa√ß√£o para a generaliza√ß√£o de exemplos que estende as √°rvores de decis√£o. A grande vantagem dessa representa√ß√£o √© a modularidade do modelo de decis√£o e, consequentemente, a sua interpretabilidade: cada regra √© independente das outras regras, e pode ser interpretada isoladamente das outras regras. Como consequ√™ncia, <strong>a representa√ß√£o utilizando regras de decis√£o permite eliminar um teste em uma regra, mas reter o teste em outra regra.</strong> Al√©m disso, como a conjun√ß√£o de condi√ß√µes √© comutativa, <strong>a distin√ß√£o entre testes perto da raiz e testes perto das folhas desaparece.</strong>" (FACELI et al., 2023, p. 91, destaquei).
                        </blockquote>
                    </p>
                    <p>
                        Nesse sentido, o <strong>algoritmo de cobertura</strong> √© capaz de aprender regras de decis√£o baseadas em exemplos e a qualidade das regras pode ser avaliada pela quantidade de casos cobertos, tenham ou n√£o sido corretamente classificados.
                    </p>
                    <p>
                        <blockquote>
                            "O algoritmo da cobertura define o processo de aprendizado como um processo de procura: dados um conjunto de exemplos classificados e uma linguagem para representar generaliza√ß√µes dos exemplos, o algoritmo procede, para cada classe, a uma procura heur√≠stica. Tipicamente, o algoritmo procura regras da forma: se <em>Atributo<sub>i</sub> = Valor<sub>j</sub> e Atributo<sub>l</sub> = Valor<sub>k</sub> ... ent√£o Classe<sub>z</sub></em>. A procura pode proceder a partir da regra mais geral, ou seja, uma regra sem parte condicional, para regras mais espec√≠ficas, acrescentando condi√ß√µes <strong>[busca <em>top-down</em> orientada pelo modelo]</strong>; ou a partir de regras muito espec√≠ficas [...] para regras mais gerais, eliminando restri√ß√µes <strong>[busca <em>bottom-up</em> orientada pelos dados]</strong>. O processo de procura √© guiado por uma fun√ß√£o de avalia√ß√£o das hip√≥teses. Essa fun√ß√£o estima a qualidade das regras que s√£o geradas durante o processo. [...] <strong>Dado um conjunto de exemplos de classes diferentes, o <em>algoritmo de cobertura</em> consiste em aprender uma regra para uma das classes, removendo o conjunto de exemplos cobertos pela regra (ou o conjunto de exemplos positivos)</strong>, e repetir o processo. O processo termina quando s√≥ h√° exemplos de uma √∫nica classe." (FACELI et al., 2023, p. 91/92, destaquei).
                        </blockquote>
                    </p>
                    <p>
                        Pode haver conflito entre duas ou mais regras, caso em que ser√° necess√°rio estabelecer algum crit√©rio de escolha. √â importante observar que, diferentemente dos m√©todos <em>bottom-up</em>, os <em>top-down</em> induzem conjuntos ordenados de regras. Portanto, nestes, a execu√ß√£o do algoritmo √© interrompida diante da primeira regra que satisfa√ßa √† condi√ß√£o de parada, enquanto naqueles, todas as regras aplic√°veis s√£o testadas e, normalmente, o resultado ser√° ponderado pela qualidade de cada uma. Por esse motivo, √© comum que algoritmos orientados por processos <em>top-down</em> contenham uma regra que espec√≠fica para a classifica√ß√£o de exemplos desconhecidos.
                    </p>
                </div>
            </div>
            <a class="emoji" href="#top">[ üîù ]</a>
        </article>
        <hr>
        <article>
            <h2>Principais t√≥picos</h2>
            <ul>
                <li>
                    <strong>Caracter√≠sticas gerais</strong>
                    <ul>
                        <li>Representa√ß√£o expl√≠cita do conhecimento extra√≠do do conjunto de dados</li>
                        <li>
                            Vantagens
                            <ul>
                                <li>Favorece a interpretabilidade por seres humanos</li>
                                <li>O processo de tomada de decis√£o do modelo √© mais compreens√≠vel e transparente (<em>explainable machine learning</em>)</li>
                                <li>Infer√™ncia l√≥gica</li>
                            </ul>
                        </li>
                        <li>
                            Desvantagens
                            <ul>
                                <li>Menor acur√°cia preditiva em compara√ß√£o aos modelos "caixa-preta"</li>
                            </ul>
                        </li>
                    </ul>
                </li>
                <li>
                    <strong>Modelos baseados em √°rvores</strong>
                    <ul>
                        <li>
                            Classifica√ß√£o (√°rvores de decis√£o) e regress√£o (√°rvores de regress√£o)
                            <ul>
                                <li>A interpreta√ß√£o dos modelos e implementa√ß√£o dos algoritmos construtores de ambas s√£o similares</li>
                            </ul>
                        </li>
                        <li>Recursividade</li>
                        <li>Divis√£o e conquista</li>
                        <li>N√≥s de divis√£o (testes condicionais com base nos valores dos atributos) e folha (valores da vari√°vel alvo)</li>
                        <li>
                            <strong>Vantagens</strong>
                            <ul>
                                <li>Flexibilidade</li>
                                <li>Robustez</li>
                                <li>Autonomia na sele√ß√£o de atributos</li>
                                <li>Interpretabilidade</li>
                                <li>Efici√™ncia (em geral)</li>
                            </ul>
                        </li>
                        <li>
                            <strong>Desvantagens</strong>
                            <ul>
                                <li>Replica√ß√£o</li>
                                <li>Instabilidade</li>
                                <li>Inefici√™ncia perante valores ausentes e atributos cont√≠nuos</li>
                            </ul>
                        </li>
                        <li>
                            Modelos n√£o convencionais
                            <ul>
                                <li>√Årvores de modelos (regress√£o) e √°rvores de op√ß√µes (classifica√ß√£o)</li>
                            </ul>
                        </li>
                        <li>
                            <strong>Regras de divis√£o em tarefas de classifica√ß√£o</strong>
                            <ul>
                                <li>
                                    Minimizar a <strong>impureza</strong> e a heterogeneidade dos subconjuntos e, consequentemente, maximizar a homogeneidade dos dados que os comp√µem
                                </li>
                                <li>
                                    Sele√ß√£o dos atributos que melhor discriminam cada classe (<em>goodness of split</em>)
                                </li>
                                <li>
                                    <strong>Ganho de informa√ß√£o</strong>
                                    <ul>
                                        <li>
                                            <strong>Entropia</strong>
                                            <ul>
                                                <li>Medida da desordem ou impureza do conjunto de dados</li>
                                                <li>A aleatoriedade de uma vari√°vel dificulta sua predi√ß√£o</li>
                                            </ul>
                                            <li>
                                                Exprime a redu√ß√£o da aleatoriedade/incerteza pela diferen√ßa da entropia de todo o conjunto de dados e a dos subconjuntos
                                            </li>
                                        </li>
                                    </ul>
                                </li>
                                <li>
                                    <strong>√çndice de Gini</strong>
                                    <ul>
                                        <li>Medida do grau de desigualdade de uma distribui√ß√£o estat√≠stica</li>
                                        <li>Medida de impureza dos n√≥s de decis√£o (subconjuntos)</li>
                                        <li>
                                            Avalia a probabilidade de que exemplos escolhidos ao acaso perten√ßam a classes diferentes, mas tenham sido classificados no mesmo subconjunto
                                        </li>
                                    </ul>
                                </li>
                            </ul>
                        </li>
                        <li>
                            <strong>Regras de divis√£o em tarefas de regress√£o</strong>
                            <ul>
                                <li>Erro quadr√°tico m√©dio (EQM) ou <em>mean squared error</em>(MSE)</li>
                                <li>Redu√ß√£o do desvio padr√£o (<em>standard deviation reduction - SDR)</em></li>
                                <li>Subconjuntos formados por valores sejam parecidos</li>
                            </ul>
                        </li>
                        <li><strong>Problema do valor desconhecido</strong></li>
                        <li>
                            <strong>Estrat√©gias de poda</strong>
                            <ul>
                                <li>Redu√ß√£o da profundidade da √°rvore</li>
                                <li>Aumento da compreensibilidade e confiabilidade do modelo</li>
                                <li>
                                    Melhora da capacidade de generaliza√ß√£o, pois n√≥s muito profundos tendem ao superajuste aos dados de treinamento (<em>overfitting</em>)
                                    <ul>
                                        <li>Equil√≠brio entre a complexidade e a capacidade de generaliza√ß√£o da √°rvore</li>
                                    </ul>
                                    <li>
                                        <strong>Pr√©-proda</strong>
                                        <ul>
                                            <li>Interrompe a constru√ß√£o da √°rvore conforme algum crit√©rio de parada</li>
                                            <li>Evita a cria√ß√£o desnecess√°ria de ramos/sub√°rvores</li>
                                        </ul>
                                    </li>
                                    <li>
                                        <strong>P√≥s-proda</strong>
                                        <ul>
                                            <li>Mais comum e confi√°vel</li>
                                            <li>A √°rvore √© completamente constru√≠da e podada ao final, ap√≥s ter atingido sua complexidade m√°xima</li>
                                        </ul>
                                    </li>
                                </li>
                            </ul>
                        </li>
                    </ul>
                </li>
                <li>
                    <strong>Modelos baseados em regras</strong>
                    <ul>
                        <li>Compara√ß√£o l√≥gica entre um atributo e os valores do dom√≠nio</li>
                        <li>
                            Modularidade
                            <ul>
                                <li>As regras s√£o modulares e podem ser interpretadas de maneira isolada, independentemente do resultado de testes anteriores</li>
                            </ul>
                        </li>
                        <li>Maior interpretabilidade</li>
                        <li>N√£o h√° perda da acur√°cia conforme se aproxime dos n√≥s folha</li>
                        <li>√Årvores de decis√£o podem ser transformadas em conjuntos ou listas de regras de decis√£o</li>
                        <li>
                            Algoritmo de cobertura
                            <ul>
                                <li>O aprendizado das regras √© um processo de procura</li>
                                <li>Busca <em>top-down</em> (orientada pelo modelo): come√ßa abrangente e acrescenta condi√ß√µes</li>
                                <li>Busca <em>bottom-up</em> (orientada pelos dados): come√ßa espec√≠fica e remove restri√ß√µes</li>
                            </ul>
                        </li>
                    </ul>
                </li>
            </ul>
            <p><a class="emoji" href="#top">[ üîù ]</a></p>
        </article>
        <hr>
        <article>
            <h2>Refer√™ncias complementares</h2>
            <p>
                BROOKSHEAR, J. Glenn. <strong>Ci√™ncia da computa√ß√£o: uma vis√£o abrangente</strong>. Trad. Eduardo Kessler Piveta. 11. ed. Porto Alegre: Bookman, 2013.
            </p>
            <p>
                CORMEN, Thomas H.; LEISERSON, Charles E.; RIVEST, Ronald L.; STEIN, Clifford. <strong>Algoritmos: teoria e pr√°tica</strong>. Trad. Arlete Simille Marques. 3. ed. Rio de Janeiro: Elsevier, 2012.
            </p>
            <p>
                HOFFMANN, Rodolfo. <strong>Estat√≠stica para economistas</strong>. 4. ed. S√£o Paulo: Pioneira Thomson Learning, 2006.
            </p>
            <a class="emoji" href="#top">[ üîù ]</a>
        </article>
        <hr>
        <article>
            <h2>Notas</h2>
            <div>
                <p id="nota_1">[1] Neste contexto, o termo <strong>s√≠mbolo</strong> refere-se √† abstra√ß√£o de conceitos, objetos ou rela√ß√µes do mundo real, que propriamente os representam ou a suas caracter√≠sticas e estados. Por conseguinte, <strong>estruturas simb√≥licas</strong> s√£o agrupamentos desses elementos fundamentais, de modo a representar conhecimentos e relacionamentos mais complexos.<a class="linkdiscreto" href="#subtitulo_0_nota_1_2"> [ üîô ]</a></p>
                <p id="nota_2">[2] A literatura comumente utiliza a express√£o "caixa-preta" (<em>black box</em>) para se referir a modelos cujo processo decis√≥rio n√£o √© facilmente infer√≠vel ou interpret√°vel por seres humanos, como √© o caso das redes neurais artificiais. A prop√≥sito, vide o fichamento <a href="../neural-newtorks-and-learning-machines-simon-haykin/introduction.html" target="_blank">1.2</a>, que cont√©m algumas refer√™ncias √† terminologia no √¢mbito das redes neurais, em especial as notas 1 e 19.<a class="linkdiscreto" href="#subtitulo_0_nota_1_2"> [ üîô ]</a></p>
                <p id="nota_3">[3] Ver o <a href="../suplementos/03-ed.html">suplemento 3</a>, sobre estruturas de dados, para informa√ß√µes adicionais sobre <stong>√°rvores.</stong><a class="linkdiscreto" href="#subtitulo_1_nota_3_4"> [ üîô ]</a></p>
                <p id="nota_4">[4] A <strong>recursividade</strong> √© uma caracter√≠stica de determinados algoritmos de se chamarem a si mesmos, uma ou mais vezes, a fim de fracionar um problema em tantos problemas menores quantos forem necess√°rios, at√© que seja poss√≠vel resolver o problema original. Normalmente, isso √© feito por meio da abordagem da <strong>divis√£o e conquista</strong>, que em cada n√≠vel de recurs√£o aplica tr√™s etapas: divis√£o, conquista e combina√ß√£o (Cormen et al., 2012).<a class="linkdiscreto" href="#subtitulo_1_nota_3_4"> [ üîô ]</a></p>
                <p id="nota_5">[5] No livro, os autores utilizam o termo √°rvore de decis√£o para se referir, indistintamente, √†s √°rvores de decis√£o ou de regress√£o, inclusive neste caso, dado que a interpreta√ß√£o dos modelos e a indu√ß√£o da √°rvore s√£o bastante similares. Todavia, ressalvam que, se necess√°ria, haver√° a devida distin√ß√£o.<a class="linkdiscreto" href="#subtitulo_1_nota_3_4"> [ üîô ]</a></p>
                <p id="nota_6">[6] "O espa√ßo de objetos √© dividido em subespa√ßos e cada subespa√ßo √© ajustado com diferentes modelos. Uma √°rvore de decis√£o fornece uma cobertura exaustiva do espa√ßo de inst√¢ncias." (FACELI et al., 2023, p. 89). <a class="linkdiscreto" href="#subtitulo_1_nota_6_7_8_9_10_11_12_13_14"> [ üîô ]</a></p>
                <p id="nota_7">[7] "√Årvores univari√°veis s√£o invariantes a transforma√ß√µes (estritamente) mon√≥tonas de vari√°veis de entrada. [...] Como consequ√™ncia dessa invari√¢ncia, a sensibilidade a distribui√ß√µes com grande cauda e <em>outliers</em> √© tamb√©m reduzida (Friedman, 1999)." (FACELI et al., 2023, p. 89). <a class="linkdiscreto" href="#subtitulo_1_nota_6_7_8_9_10_11_12_13_14"> [ üîô ]</a></p>
                <p id="nota_8">[8] "O processo de constru√ß√£o de uma √°rvore de decis√£o seleciona os atributos a usar no modelo de decis√£o. Essa sele√ß√£o de atributos produz modelos que tendem a ser bastante robustos contra a adi√ß√£o de atributos irrelevantes e redundantes." (FACELI et al., 2023, p. 89). <a class="linkdiscreto" href="#subtitulo_1_nota_6_7_8_9_10_11_12_13_14"> [ üîô ]</a></p>
                <p id="nota_9">[9] "Decis√µes complexas e globais podem ser aproximadas por uma s√©rie de decis√µes mais simples e locais. Todas as decis√µes s√£o baseadas nos valores dos atributos usados para descrever o problema." (FACELI et al., 2023, p. 89). <a class="linkdiscreto" href="#subtitulo_1_nota_6_7_8_9_10_11_12_13_14"> [ üîô ]</a></p>
                <p id="nota_10">[10] "O algoritmo para aprendizado de √°rvore de decis√£o √© um algoritmo guloso que √© constru√≠do de cima para baixo (<em>top-down</em>), usando uma estrat√©gia dividir para conquistar sem <em>backtracking</em>. Sua complexidade de tempo √© linear com o n√∫mero de exemplos." (FACELI et al., 2023, p. 89). <a class="linkdiscreto" href="#subtitulo_1_nota_6_7_8_9_10_11_12_13_14"> [ üîô ]</a></p>
                <p id="nota_11">[11] "O termo refere-se √† duplica√ß√£o de uma sequ√™ncia de testes em diferentes ramos de uma √°rvore de decis√£o, levando a uma representa√ß√£o n√£o concisa, que tamb√©m tende a ter baixa acur√°ria preditiva [...]." (FACELI et al., 2023, p. 89). <a class="linkdiscreto" href="#subtitulo_1_nota_6_7_8_9_10_11_12_13_14"> [ üîô ]</a></p>
                <p id="nota_12">[12] "Pequenas varia√ß√µes no conjunto de treinamento podem produzir grandes varia√ß√µes na √°rvore final [...]. A estrat√©gia da parti√ß√£o recursiva implica que a cada divis√£o que √© feita o dado √© dividido com base no atributo de teste. Depois de algumas divis√µes, h√° usualmente muito poucos dados nos quais a decis√£o se baseia. H√° uma forte tend√™ncia a infer√™ncias feitas pr√≥ximo das folhas serem menos confi√°veis que aquelas feitas pr√≥ximas da raiz." (FACELI et al., 2023, p. 90). <a class="linkdiscreto" href="#subtitulo_1_nota_6_7_8_9_10_11_12_13_14"> [ üîô ]</a></p>
                <p id="nota_13">[13] "Uma √°rvore de decis√£o √© uma hierarquia de teses. Se o valor de um atributo √© desconhecido, isso causa problemas em decidir que ramo seguir." (FACELI et al., 2023, p. 89). <a class="linkdiscreto" href="#subtitulo_1_nota_6_7_8_9_10_11_12_13_14"> [ üîô ]</a></p>
                <p id="nota_14">[14] "Nesse caso, uma opera√ß√£o de ordena√ß√£o √© solicitada para cada atributo cont√≠nuo de cada n√≥ de decis√£o." (FACELI et al., 2023, p. 89). <a class="linkdiscreto" href="#subtitulo_1_nota_6_7_8_9_10_11_12_13_14"> [ üîô ]</a></p>
                <p id="nota_15">[15] Na Economia, o <strong>√≠ndice de Gini</strong> √© uma <strong>medida de desigualdade</strong> muito empregada para analisar a distribui√ß√£o de renda, mas que pode ser usada para medir o grau de desigualdade de qualquer distribui√ß√£o estat√≠stica, definido como a raz√£o entre a <strong>√°rea de desigualdade</strong>, obtida entre a <strong>linha da perfeita igualdade</strong> e a <strong>curva de Lorenz</strong>, e a √°rea do tri√¢ngulo formado pelos eixos do gr√°fico e a linha de perfeita igualdade (Hoffmann, 2006). Analogicamente, no contexto da aprendizagem de m√°quina, √© poss√≠vel trasladar esse racioc√≠nio para a desigualdade - leia-se heterogeneidade - da distribui√ß√£o dos elementos em um conjunto ou subconjunto de dados. <a class="linkdiscreto" href="#subtitulo_1_nota_15"> [ üîô ]</a></p>
                <p id="nota_16">[16] O caminho mais longo entre as extremidades determina a <strong>altura</strong> da √°rvore, enquanto a <strong>profundidade</strong> √© a quantidade de n√≥s ou camadas horizontais existentes nesse caminho (Brookshear, 2013). <a class="linkdiscreto" href="#subtitulo_1_nota_16"> [ üîô ]</a></p>
                <p id="nota_17">[17] Especialmente os m√©todos do custo de complexidade e da poda pessimista s√£o abordados em detalhe na se√ß√£o 6.3.2 do livro (p. 87/88). <a class="linkdiscreto" href="#subtitulo_1_nota_17"> [ üîô ]</a></p>
            </div>
            <a class="emoji" href="#top">[ üîù ]</a>
        </article>
    </main>
</body>

<footer>
    <p>Projeto NeuroBit</p>
        <p class="linkdiscreto">
            <a class="social linkdiscreto" href="https://github.com/paulorobertovrc" target="_blank">
                <svg width="24px" height="24px" viewBox="0 0 20 20" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" fill="#ffffff"><g id="SVGRepo_bgCarrier" stroke-width="0"></g><g id="SVGRepo_tracerCarrier" stroke-linecap="round" stroke-linejoin="round"></g><g id="SVGRepo_iconCarrier"> <title>github [#142]</title> <desc>Created with Sketch.</desc> <defs> </defs> <g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"> <g id="Dribbble-Light-Preview" transform="translate(-140.000000, -7559.000000)" fill="#fff"> <g id="icons" transform="translate(56.000000, 160.000000)"> <path d="M94,7399 C99.523,7399 104,7403.59 104,7409.253 C104,7413.782 101.138,7417.624 97.167,7418.981 C96.66,7419.082 96.48,7418.762 96.48,7418.489 C96.48,7418.151 96.492,7417.047 96.492,7415.675 C96.492,7414.719 96.172,7414.095 95.813,7413.777 C98.04,7413.523 100.38,7412.656 100.38,7408.718 C100.38,7407.598 99.992,7406.684 99.35,7405.966 C99.454,7405.707 99.797,7404.664 99.252,7403.252 C99.252,7403.252 98.414,7402.977 96.505,7404.303 C95.706,7404.076 94.85,7403.962 94,7403.958 C93.15,7403.962 92.295,7404.076 91.497,7404.303 C89.586,7402.977 88.746,7403.252 88.746,7403.252 C88.203,7404.664 88.546,7405.707 88.649,7405.966 C88.01,7406.684 87.619,7407.598 87.619,7408.718 C87.619,7412.646 89.954,7413.526 92.175,7413.785 C91.889,7414.041 91.63,7414.493 91.54,7415.156 C90.97,7415.418 89.522,7415.871 88.63,7414.304 C88.63,7414.304 88.101,7413.319 87.097,7413.247 C87.097,7413.247 86.122,7413.234 87.029,7413.87 C87.029,7413.87 87.684,7414.185 88.139,7415.37 C88.139,7415.37 88.726,7417.2 91.508,7416.58 C91.513,7417.437 91.522,7418.245 91.522,7418.489 C91.522,7418.76 91.338,7419.077 90.839,7418.982 C86.865,7417.627 84,7413.783 84,7409.253 C84,7403.59 88.478,7399 94,7399" id="github-[#142]"> </path> </g> </g> </g> </g></svg>
            </a>
            <a class="social linkdiscreto" href="https://www.instagram.com/pauloroberto.dev" target="_blank">
                <svg width="24px" height="24px" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" stroke="#FFFFFF"><g id="SVGRepo_bgCarrier" stroke-width="0"></g><g id="SVGRepo_tracerCarrier" stroke-linecap="round" stroke-linejoin="round"></g><g id="SVGRepo_iconCarrier"> <path fill-rule="evenodd" clip-rule="evenodd" d="M12 18C15.3137 18 18 15.3137 18 12C18 8.68629 15.3137 6 12 6C8.68629 6 6 8.68629 6 12C6 15.3137 8.68629 18 12 18ZM12 16C14.2091 16 16 14.2091 16 12C16 9.79086 14.2091 8 12 8C9.79086 8 8 9.79086 8 12C8 14.2091 9.79086 16 12 16Z" fill="#FFFFFF"></path> <path d="M18 5C17.4477 5 17 5.44772 17 6C17 6.55228 17.4477 7 18 7C18.5523 7 19 6.55228 19 6C19 5.44772 18.5523 5 18 5Z" fill="#FFFFFF"></path> <path fill-rule="evenodd" clip-rule="evenodd" d="M1.65396 4.27606C1 5.55953 1 7.23969 1 10.6V13.4C1 16.7603 1 18.4405 1.65396 19.7239C2.2292 20.8529 3.14708 21.7708 4.27606 22.346C5.55953 23 7.23969 23 10.6 23H13.4C16.7603 23 18.4405 23 19.7239 22.346C20.8529 21.7708 21.7708 20.8529 22.346 19.7239C23 18.4405 23 16.7603 23 13.4V10.6C23 7.23969 23 5.55953 22.346 4.27606C21.7708 3.14708 20.8529 2.2292 19.7239 1.65396C18.4405 1 16.7603 1 13.4 1H10.6C7.23969 1 5.55953 1 4.27606 1.65396C3.14708 2.2292 2.2292 3.14708 1.65396 4.27606ZM13.4 3H10.6C8.88684 3 7.72225 3.00156 6.82208 3.0751C5.94524 3.14674 5.49684 3.27659 5.18404 3.43597C4.43139 3.81947 3.81947 4.43139 3.43597 5.18404C3.27659 5.49684 3.14674 5.94524 3.0751 6.82208C3.00156 7.72225 3 8.88684 3 10.6V13.4C3 15.1132 3.00156 16.2777 3.0751 17.1779C3.14674 18.0548 3.27659 18.5032 3.43597 18.816C3.81947 19.5686 4.43139 20.1805 5.18404 20.564C5.49684 20.7234 5.94524 20.8533 6.82208 20.9249C7.72225 20.9984 8.88684 21 10.6 21H13.4C15.1132 21 16.2777 20.9984 17.1779 20.9249C18.0548 20.8533 18.5032 20.7234 18.816 20.564C19.5686 20.1805 20.1805 19.5686 20.564 18.816C20.7234 18.5032 20.8533 18.0548 20.9249 17.1779C20.9984 16.2777 21 15.1132 21 13.4V10.6C21 8.88684 20.9984 7.72225 20.9249 6.82208C20.8533 5.94524 20.7234 5.49684 20.564 5.18404C20.1805 4.43139 19.5686 3.81947 18.816 3.43597C18.5032 3.27659 18.0548 3.14674 17.1779 3.0751C16.2777 3.00156 15.1132 3 13.4 3Z" fill="#FFFFFF"></path> </g></svg>
            </a>
            <a class="social linkdiscreto" href="https://www.linkedin.com/in/paulorobertovrc" target="_blank">
                <svg width="27px" height="27px" viewBox="0 0 21 21" fill="none" xmlns="http://www.w3.org/2000/svg"><g id="SVGRepo_bgCarrier" stroke-width="0"></g><g id="SVGRepo_tracerCarrier" stroke-linecap="round" stroke-linejoin="round"></g><g id="SVGRepo_iconCarrier"> <path d="M18.72 3.99997H5.37C5.19793 3.99191 5.02595 4.01786 4.86392 4.07635C4.70189 4.13484 4.55299 4.22471 4.42573 4.34081C4.29848 4.45692 4.19537 4.59699 4.12232 4.75299C4.04927 4.909 4.0077 5.07788 4 5.24997V18.63C4.01008 18.9901 4.15766 19.3328 4.41243 19.5875C4.6672 19.8423 5.00984 19.9899 5.37 20H18.72C19.0701 19.9844 19.4002 19.8322 19.6395 19.5761C19.8788 19.32 20.0082 18.9804 20 18.63V5.24997C20.0029 5.08247 19.9715 4.91616 19.9078 4.76122C19.8441 4.60629 19.7494 4.466 19.6295 4.34895C19.5097 4.23191 19.3672 4.14059 19.2108 4.08058C19.0544 4.02057 18.8874 3.99314 18.72 3.99997ZM9 17.34H6.67V10.21H9V17.34ZM7.89 9.12997C7.72741 9.13564 7.5654 9.10762 7.41416 9.04768C7.26291 8.98774 7.12569 8.89717 7.01113 8.78166C6.89656 8.66615 6.80711 8.5282 6.74841 8.37647C6.6897 8.22474 6.66301 8.06251 6.67 7.89997C6.66281 7.73567 6.69004 7.57169 6.74995 7.41854C6.80986 7.26538 6.90112 7.12644 7.01787 7.01063C7.13463 6.89481 7.2743 6.80468 7.42793 6.74602C7.58157 6.68735 7.74577 6.66145 7.91 6.66997C8.07259 6.66431 8.2346 6.69232 8.38584 6.75226C8.53709 6.8122 8.67431 6.90277 8.78887 7.01828C8.90344 7.13379 8.99289 7.27174 9.05159 7.42347C9.1103 7.5752 9.13699 7.73743 9.13 7.89997C9.13719 8.06427 9.10996 8.22825 9.05005 8.3814C8.99014 8.53456 8.89888 8.6735 8.78213 8.78931C8.66537 8.90513 8.5257 8.99526 8.37207 9.05392C8.21843 9.11259 8.05423 9.13849 7.89 9.12997ZM17.34 17.34H15V13.44C15 12.51 14.67 11.87 13.84 11.87C13.5822 11.8722 13.3313 11.9541 13.1219 12.1045C12.9124 12.2549 12.7546 12.4664 12.67 12.71C12.605 12.8926 12.5778 13.0865 12.59 13.28V17.34H10.29V10.21H12.59V11.21C12.7945 10.8343 13.0988 10.5225 13.4694 10.3089C13.84 10.0954 14.2624 9.98848 14.69 9.99997C16.2 9.99997 17.34 11 17.34 13.13V17.34Z" fill="#FFFFFF"></path> </g></svg>
            </a>
        </p>
</footer>
