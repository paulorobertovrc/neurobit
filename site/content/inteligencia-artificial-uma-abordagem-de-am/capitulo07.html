<!DOCTYPE html>
<html lang="pt-br">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="../../../site/style.css">
    <title>NeuroBit | @pauloroberto.dev</title>
    <link rel="shortcut icon" href="../../../imagens/favicon.png" type="image/x-icon">
</head>

<header class="breadcrumb" id="top">
    <nav>
        <a href="ia-am-faceli.html">Voltar √† p√°gina anterior</a>
        <br/>
        <br/>
        <div class="header-info-container">
            <div class="container-esquerdo">
                <p class="numeracao-sequencial">2</p>
            </div>
            <div>
                <p class="titulo">Intelig√™ncia Artificial: Uma Abordagem de Aprendizado de M√°quina</p>
                <p class="descricao">Katti Faceli et al.</p>
            </div>
        </div>
    </nav>
</header>

<body>
    <main>
        <h1># 2.7 Cap√≠tulo 7 (p. 101/116)</h1>
        <p><img src="https://img.shields.io/badge/Status-Estudando-grey?labelColor=31A8B8"></p>

        <article>
            <div>                
                <h3 id="subtitulo_0_nota_1"> 7. M√©todos conexionistas</h3>
                <p>
                    S√£o m√©todos inspirados em modelos biol√≥gicos, que buscam n√£o apenas <strong>simular o funcionamento</strong> do sistema nervoso humano e o modo como o c√©rebro <strong>adquire novos conhecimentos</strong> ‚Äî ou seja, seu <strong>processo de aprendizado</strong> ‚Äî, mas alcan√ßar <strong>capacidade de processamento</strong> semelhante e obter <strong>m√°quinas inteligentes ou que se comportem de maneira aparentemente inteligente.</strong> Assim como o c√©rebro √© composto por uma grande quantidade de neur√¥nios interconectados, perfazendo redes neurais que funcionam em paralelo e trocam informa√ß√µes atrav√©s de sinapses<sup><a class="linknotas" href="#nota_1">[1]</a></sup>, as redes neurais artificiais (RNAs) s√£o formadas por unidades que implementam fun√ß√µes matem√°ticas a fim de simular a atividade neuronal e, de modo geral, <strong>abstraem a compreens√£o da fisiologia do c√©rebro e dos processos biol√≥gicos de aprendizagem.</strong>
                </p>
            </div>
            <div>
                <h4>7.1 Redes neurais artificiais</h4>
                <p id="subtitulo_0_nota_2">
                    <blockquote>
                        "A procura por modelos computacionais ou matem√°ticos do sistema nervoso teve in√≠cio na mesma √©poca em que foram desenvolvidos os primeiros computadores eletr√¥nicos, na d√©cada de 1940. Os estudos pioneiros na √°rea foram realizados por McCulloch e Pitts (1943)<sup><a class="linknotas" href="#nota_2">[2]</a></sup>. Em 1943, eles propuseram um modelo matem√°tico de neur√¥nio artificial, a unidade l√≥gica com limiar (LTU, do ingl√™s <em>Logic Threshold Unit</em>), que podia executar fun√ß√µes l√≥gicas simples. McCulloch e Pitts mostraram que a combina√ß√£o de v√°rios neur√¥nios artificiais em sistemas neurais tem um elevado poder computacional, pois pode implementar qualquer fun√ß√£o obtida pela combina√ß√£o de fun√ß√µes l√≥gicas. Entretanto, redes de LTUs n√£o possu√≠am capacidade de aprendizado. [...] Na d√©cada de 1970, houve um resfriamento das pesquisas em RNAs, principalmente com a [...] limita√ß√£o da rede Perceptron a problemas linearmente separ√°veis. Na d√©cada de 1980, o aumento da capacidade de processamento, as pesquisas em processamento paralelo e, principalmente, a proposta de novas arquiteturas de RNAs com maior capacidade de representa√ß√£o e de algoritmos de aprendizado mais sofisticados levaram ao ressurgimento da √°rea." (FACELI et al., 2023, p. 102).
                    </blockquote>
                </p>
                <p>
                    √â poss√≠vel definir as redes neurais artificiais como
                    <blockquote>
                        "[...] sistemas computacionais distribu√≠dos compostos de unidades de processamento simples, densamente interconectadas [...], conhecidas como neur√¥nios artificiais, [que] computam fun√ß√µes matem√°ticas [...]. As unidades [neur√¥nios artificiais] s√£o dispostas em uma ou mais camadas e interligadas por um grande n√∫mero de conex√µes, geralmente unidirecionais. Na maioria das arquiteturas, essas conex√µes, que simulam as sinapses biol√≥gicas, possuem pesos associados, que ponderam a entrada recebida por cada neur√¥nio da rede [...] [e] podem assumir valores positivos ou negativos, dependendo de o comportamento da conex√£o ser excitat√≥rio ou inibit√≥rio, respectivamente. Os pesos t√™m seus valores ajustados em um processo de aprendizado e codificam o conhecimento adquirido pela rede (Braga et al., 2007)." (FACELI et al., 2023, p. 103).
                    </blockquote>
                </p>
                <div>
                    <h5>7.1.1 Componentes b√°sicos de uma RNA</h5>
                    <p>
                        Seus <strong>componentes b√°sicos</strong> s√£o <strong>arquitetura</strong> e e <strong>aprendizado</strong>. "Enquanto a arquitetura est√° relacionada com o tipo, o n√∫mero de unidades de processamento e a forma como os neur√¥nios est√£o conectados, o aprendizado diz respeito √†s regras utilizadas para ajustar os pesos da rede e √† informa√ß√£o que √© utilizada por essas regras." (FACELI et al., 2023, p. 103).
                    </p>
                    <p>
                        <strong>O neur√¥nio artificial √© a unidade de processamento e componente fundamental da arquitetura de uma RNA.</strong> Ele possui <strong>terminais de entrada</strong> que recebem os valores, uma <strong>fun√ß√£o de ativa√ß√£o</strong>, que √© uma fun√ß√£o matem√°tica que realiza o processamento desses valores j√° ponderados, e um terminal de sa√≠da que corresponde √† resposta do neur√¥nio, alusivos, respectivamente, aos dendritos, corpo celular e ax√¥nios de um neur√¥nio biol√≥gico. A cada terminal de entrada corresponde um <strong>peso sin√°ptico</strong>, sendo a <strong>entrada total</strong>, sobre a qual a fun√ß√£o de ativa√ß√£o √© aplicada, definida pelo somat√≥rio de cada um dos valores de entrada multiplicado pelo peso vinculado √† conex√£o respectiva. Os terminais de entrada podem ter pesos positivos, negativos ou zero, neste caso indicativo de que nenhuma conex√£o foi associada.
                    </p>
                    <p>
                        A seguir, imagem ilustrativa de um neur√¥nio artificial:
                        <div class="image-container">
                            <figure>
                                <img src="../../../imagens/23_am_faceli_neuronio_artificial.png" alt="Ilustra√ß√£o de um neur√¥nio artificial simples">
                                <figcaption class="legenda">Figura 23 - Ilustra√ß√£o de um neur√¥nio artificial simples (FACELI et al., 2023, p. 103).</figcaption>
                            </figure>
                        </div>
                    </p>
                    <p>
                        Em rela√ß√£o √† <strong>fun√ß√£o de ativa√ß√£o</strong>, dentre as propostas mais comuns, destacam-se as seguintes: linear, limiar, sigmoidal, tangente hiperb√≥lica, gaussiana e linear retificada (ReLU). Em qualquer caso, ela receber√° a entrada total e retornar√° um valor que ser√° a sa√≠da do neur√¥nio, definindo, por conseguinte, se ele ser√° ou n√£o ativado.
                    </p>
                    <p>
                        <blockquote>
                            "O uso da <strong>fun√ß√£o linear</strong> identidade [...] implica retornar como sa√≠da o valor de <math><ms>u</ms></math> [ou seja, a pr√≥pria entrada total]. Na <strong>fun√ß√£o limiar</strong> [...], o valor do limiar define quando o resultado da fun√ß√£o limiar ser√° igual a 1 ou 0 (alternativamente, [...] -1). Quando a soma das entradas recebidas ultrapassa o limiar estabelecido, o neur√¥nio torna-se ativo (sa√≠da +1). Quanto maior o valor do limiar, maior tem que ser o valor da entrada total para que o valor de sa√≠da do neur√¥nio seja igual a 1. A <strong>fun√ß√£o sigmoidal</strong> [...] representa uma aproxima√ß√£o cont√≠nua e diferenci√°vel da fun√ß√£o limiar. A sua sa√≠da √© um valor no intervalo aberto (0, 1), podendo apresentar diferentes inclina√ß√µes. A <strong>fun√ß√£o tangente hiperb√≥lica</strong> [...] √© uma varia√ß√£o da fun√ß√£o sigmoidal que utiliza o intervalo aberto (-1, +1) para o valor de sa√≠da. Outra fun√ß√£o utilizada com frequ√™ncia, tamb√©m cont√≠nua e diferenci√°vel, √© a <strong>fun√ß√£o gaussiana</strong> [...]. Mais recentemente, com a populariza√ß√£o das redes profundas, passou a ser cada vez mais utilizada a <strong>fun√ß√£o linear retificada</strong>, tamb√©m conhecida como ReLU (do ingl√™s <em>Rectified Linear Unit</em>) [...]. Essa fun√ß√£o retorna 0 se recebe um valor negativo ou o pr√≥prio valor, no caso contr√°rio. Junto com suas varia√ß√µes, ela tem apresentado bons resultados em v√°rias aplica√ß√µes." (FACELI et al., 2023, p. 104, destaquei).
                        </blockquote>
                    </p>
                    <p>
                        A seguir, imagens ilustrativas do c√°lculo da entrada total de um neur√¥nio artificial e das precitadas fun√ß√µes de ativa√ß√£o:
                        <div class="image-container">
                            <figure>
                                <img src="../../../imagens/24_am_faceli_entrada_total_do_neuronio_artificial.png" alt="Entrada total de um neur√¥nio artificial">
                                <figcaption class="legenda">Figura 24 - Entrada total de um neur√¥nio artificial (FACELI et al., 2023, p. 104).</figcaption>
                            </figure>
                        </div>
                        <div class="image-container">
                            <figure>
                                <img src="../../../imagens/25_am_faceli_funcoes_de_ativacao.png" alt="Fun√ß√µes de ativa√ß√£o">
                                <figcaption class="legenda">Figura 25 - Fun√ß√µes de ativa√ß√£o (FACELI et al., 2023, p. 104).</figcaption>
                            </figure>
                        </div>
                    </p>
                </div>
                <div>
                    <h6>7.1.1.1 Arquitetura</h6>
                    <p>
                        As redes neurais artificiais tem seus neur√¥nios organizados em <strong>camadas</strong> que definem o <strong>padr√£o arquitetural</strong> da rede. Na forma mais simples, composta por <strong>uma √∫nica camada</strong>, os neur√¥nios recebem os dados diretamente em seus terminais de entrada, correspondendo ela pr√≥pria √† camada de sa√≠da. Nas redes <strong>multicamadas</strong>, que possuem camadas <strong>intermedi√°rias, escondidas ou ocultas</strong>, o fluxo da informa√ß√£o entre as camadas pode ser unidirecional (redes <strong><em>feed-forward</em></strong>) ou com <strong>retroalimenta√ß√£o (<em>feedback</em>)</strong> (redes <strong>recorrentes ou com retropropaga√ß√£o</strong>), isto √©, um ou mais terminais de entrada de um ou mais neur√¥nios recebem a sa√≠da de neur√¥nios da mesma camada, de camada posterior ou mesmo a sua pr√≥pria sa√≠da. "O n√∫mero de camadas, o n√∫mero de neur√¥nios em cada camada, o grau de conectividade e a presen√ßa ou n√£o de conex√µes com retropropaga√ß√£o definem a topologia de uma RNA." (FACELI et al., 2023, p. 106).
                    </p>
                    <p>
                        Ilustrando arquiteturas multicamadas sem e com retroalimenta√ß√£o, vejamos a imagem a seguir:
                        <div class="image-container">
                            <figure>
                                <img src="../../../imagens/26_am_faceli_rna_multicamadas.png" alt="RNA multicamadas sem e com retroalimenta√ß√£o">
                                <figcaption class="legenda">Figura 26 ‚Äî RNA multicamadas sem e com retroalimenta√ß√£o (FACELI et al., 2023, p. 106).</figcaption>
                            </figure>
                        </div>
                    </p>
                </div>
                <div>
                    <h6>7.1.1.2 Aprendizado</h6>
                    <p>
                        Essencialmente, a capacidade de aprendizado est√° relacionada com o <strong>ajuste dos par√¢metros</strong> da RNA, isto √©, "[...] a <strong>defini√ß√£o dos valores dos pesos associados √†s conex√µes da rede</strong> que fazem com que o modelo obtenha melhor desempenho, geralmente medido pela acur√°cia preditiva." (FACELI et al., 2023, p. 106, destaquei). Isso √© feito atrav√©s de "[...] um <strong>conjunto de regras bem definidas</strong> que especificam quando e como deve ser alterado o valor de cada peso" (FACELI et al., 2023, p. 106, destaquei), implementadas por <strong>algoritmos de treinamento</strong>.
                    </p>
                    <p>
                        Os principais algoritmos s√£o os seguintes: <strong>(1) corre√ß√£o de erro</strong> o ajuste √© feito para minimizar os erros, geralmente, mediante o aprendizado supervisionado; <strong>(2) Hebbiano</strong> inspirado na aprendizagem Hebbiana ‚Äî n√£o supervisionada ‚Äî, que preconiza o fortalecimento de conex√µes que, com frequ√™ncia, s√£o ativadas simultaneamente; <strong>(3) competitivo</strong>: os neur√¥nios competem entre si (aprendizado n√£o supervisionado); e <strong>(4) termodin√¢mico</strong>: inspirado na aprendizagem de Boltzmann, √© um algoritmo estoc√°stico que se baseia em princ√≠pios da F√≠sica (termodin√¢mica) e busca o "equil√≠brio t√©rmico" da rede.
                    </p>
                </div>
            </div>
            <a class="emoji" href="#top">[ üîù ]</a>
        </article>
        <hr>
        <article>
            <h2>Principais t√≥picos</h2>
            <ul>
                <li>
                    <strong>No√ß√µes gerais</strong>
                    <ul>
                        <li>
                            Inspira√ß√£o biol√≥gica
                            <ul>
                                <li>Simulam o funcionamento de uma rede neural biol√≥gica para alcan√ßar capacidade computacional similar √† do c√©rebro humano, com o objetivo de obter m√°quinas inteligentes e/ou capazes de emitir comportamentos aparentemente inteligentes</li>
                            </ul>
                        </li>
                        <li>Unidades de processamento simples implementam fun√ß√µes matem√°ticas a fim de resolver problemas complexos</li>
                    </ul>
                </li>
                <li>
                    <strong>Redes neurais artificiais</strong>
                    <ul>
                        <li>
                            Modelo de McCulloch e Pitts (1943)
                            <ul>
                                <li>Primeiro modelo matem√°tico de um neur√¥nio artificial</li>
                                <li>A fun√ß√£o de ativa√ß√£o √© uma fun√ß√£o limiar</li>
                                <li>
                                    Unidade l√≥gica com limiar (<em>Logic Threshold Unit</em>, LTU)
                                    <ul>
                                        <li>Executava fun√ß√µes l√≥gicas simples</li>
                                        <li>Redes de LTUs tinham elevado poder computacional para a √©poca, pois podiam implementar qualquer fun√ß√£o obtida pela combina√ß√£o de fun√ß√µes l√≥gicas, mas n√£o possu√≠am capacidade de aprendizado
                                        </li>
                                    </ul>
                                </li>
                            </ul>
                        </li>
                        <li>
                            <strong>Neur√¥nio artificial</strong>
                            <ul>
                                <li>Unidade de processamento e principal componente arquitetural</li>
                                <li>Terminais de entrada, pesos, fun√ß√£o de ativa√ß√£o e terminal de sa√≠da</li>
                                <li><strong>Entrada total</strong>: soma ponderada de todos os valores de entrada, sobre a qual √© aplicada a fun√ß√£o de ativa√ß√£o</li>
                                <li><strong>Fun√ß√£o de ativa√ß√£o</strong>: recebe a entrada total e retorna um valor correspondente √† sa√≠da do neur√¥nio, que define se ele ser√° ou n√£o ativado</li>
                            </ul>
                        </li>
                        <li>
                            <strong>Componentes b√°sicos de uma RNA</strong>
                            <ul>
                                <li>
                                    <strong>Arquitetura</strong>
                                    <ul>
                                        <li>Estrutura da rede</li>
                                        <li>
                                            Topologia
                                            <ul>
                                                <li>N√∫mero de camadas e de neur√¥nios por camada</li>
                                                <li>Grau de conectividade</li>
                                                <li>Presen√ßa ou n√£o de retropropaga√ß√£o</li>
                                            </ul>
                                        </li>
                                    </ul>
                                </li>
                                <li>
                                    <strong>Aprendizado</strong>
                                    <ul>
                                        <li>
                                            Ajuste dos par√¢metros da rede
                                            <ul>
                                                <li>Defini√ß√£o dos valores dos pesos associados √†s conex√µes da rede</li>
                                            </ul>
                                        </li>
                                        <li>
                                            Algoritmos
                                            <ul>
                                                <li>Corre√ß√£o de erro</li>
                                                <li>Hebbiano</li>
                                                <li>Competitivo</li>
                                                <li>Termodin√¢mico</li>
                                            </ul>
                                        </li>
                                    </ul>
                                </li>
                            </ul>
                        </li>
                    </ul>
                </li>
            </ul>
            <p><a class="emoji" href="#top">[ üîù ]</a></p>
        </article>
        <hr>
        <article>
            <h2>Refer√™ncias complementares</h2>
            <p>
                BOCCATO, Levy; ATTUX, Romis. <strong>T√≥pico 6 ‚Äî parte 1: redes neurais artificiais</strong>. Departamento de Engenharia de Computa√ß√£o e Automa√ß√£o. Faculdade de Engenharia El√©trica e de Computa√ß√£o. Universidade Estadual de Campinas. Dispon√≠vel em <https://www.dca.fee.unicamp.br/~lboccato/topico_6.1_redes_neurais.pdf>. Acesso em 19 jul. 2024.
            </p>
            <p>
                LEMES, Nelson H. T. <strong>Neur√¥nio de McCulloch-Pitts</strong>. Instituto de Qu√≠mica. Universidade Federal de Alfenas. Dispon√≠vel em <https://pessoas.unifal-mg.edu.br/nelsonlemes/neuronio-de-mcculloch-pitts/>. Acesso em 19 jul. 2024.
            </p>
            <p>
                McCULLOCH, Warren S.; PITTS, Walter. <strong>A logical calculus of ideas immanent in nervous activity</strong>. <em>Reprinted from the Bulletin of Mathematical Biophysics</em>, v. 5, p. 115-133 (1943). <em>Bulletin of Mathematical Biology</em>, v. 52, n. 1/2, p. 99/115. <em>Great Britain</em>: Pergamon Press PLC, 1990.
            </p>
            <a class="emoji" href="#top">[ üîù ]</a>
        </article>
        <hr>
        <article>
            <h2>Notas</h2>
            <div>
                <p id="nota_1">[1] "O principal bloco de constru√ß√£o do c√©rebro √© o neur√¥nio. Os principais componentes de um neur√¥nio s√£o: dendritos, corpo celular e ax√¥nio. [...] Os dendritos s√£o prolongamentos dos neur√¥nio especializados na recep√ß√£o de est√≠mulos nervosos provenientes de outros neur√¥nios ou do ambiente. Esses est√≠mulos s√£o ent√£o transmitidos para o corpo celular ou soma. O soma coleta as informa√ß√µes recebidas dos dendritos, as combina e processa. De acordo com a intensidade e frequ√™ncia dos est√≠mulos recebidos, o corpo celular gera um novo impulso, que √© enviado para o ax√¥nio. O ax√¥nio √© um prolongamento dos neur√¥nios, respons√°vel pela condu√ß√£o dos impulsos el√©tricos produzidos no corpo celular at√© outro local mais distante [...]. O contato entre a termina√ß√£o de um ax√¥nio e o dendrito de outro neur√¥nio √© denominado sinapse. As sinapses s√£o, portanto, as unidades que medeiam as intera√ß√µes entre os neur√¥nios [...] e podem ser excitat√≥rias ou inibit√≥rias." (FACELI et al., 2023, p. 102/103).<a class="linkdiscreto" href="#subtitulo_0_nota_1"> [ üîô ]</a></p>
                
                <p id="nota_2">[2] No artigo intitulado "<em>A Logical Calculus of Ideas Immanent in Nervous Activity</em>", publicado em 1943 no <em>Bulletin of Mathematical Biophysics</em>, Warren McCulloch e Walter Pitts, ainda que simplificadamente, propuseram o primeiro modelo computacional/matem√°tico de um neur√¥nio biol√≥gico. √Ä luz do conhecimento cient√≠fico que se tinha √†quele momento sobre a estrutura e o funcionamento das c√©lulas nervosas, o trabalho foi norteado pelo estabelecimento das seguintes premissas: <strong>(1) a atividade do neur√¥nio √© um processo "tudo ou nada"</strong> (<em>"all-or-none" process</em>); <strong>(2) a ativa√ß√£o do neur√¥nio exige que impulsos el√©tricos sejam recebidos em um per√≠odo (lat√™ncia) por uma determinada quantidade de sinapses</strong>, que n√£o se altera em raz√£o da atividade pr√©via e/ou do estado da c√©lula; <strong>(3) o atraso sin√°ptico foi o √∫nico intervalo de tempo considerado para induzir o modelo</strong>, desprezando-se outros inerentes √† gera√ß√£o e/ou transmiss√£o de impulsos pelo sistema nervoso, pois irrelevantes; <strong>(4) sinapses inibit√≥rias impedem a ativa√ß√£o do neur√¥nio naquele instante</strong> (per√≠odo refrat√°rio); e <strong>(5) a estrutura da rede neural √© invariante ao tempo</strong>, assim como seus componentes. Noutras palavras, partiram da assun√ß√£o de que o processo computacional √© bin√°rio, admitindo apenas dois estados n√£o concomitantes ‚Äî ativo/ligado e inativo/desligado ‚Äî, mas invariante ao mero decurso do tempo na medida em que mudam de estado t√£o somente se verificada determinada condi√ß√£o, de modo que a qualquer instante o limiar de ativa√ß√£o, que √© determinado pelo pr√≥prio neur√¥nio e n√£o por quaisquer caracter√≠sticas inerentes ao est√≠mulo recebido, deve ser excedido em uma janela de lat√™ncia para que o neur√¥nio seja ativado. Os autores demonstraram que seu modelo era capaz de representar quaisquer proposi√ß√µes elementares e que, implementados em rede, conseguiriam calcular l√≥gicas complexas. N√£o obstante, observaram limita√ß√µes diante de disjun√ß√µes exclusivas e que a incapacidade de modificar, extinguir ou criar sinapses impedia o processo de aprendizagem. Como se v√™, o modelo constitui uma abstra√ß√£o da verdadeira fisiologia do sistema nervoso biol√≥gico.<a class="linkdiscreto" href="#subtitulo_0_nota_2"> [ üîô ]</a></p>
            </div>
            <a class="emoji" href="#top">[ üîù ]</a>
        </article>
    </main>
</body>

<footer>
    <p>Projeto NeuroBit</p>
        <p class="linkdiscreto">
            <a class="social linkdiscreto" href="https://github.com/paulorobertovrc" target="_blank">
                <svg width="24px" height="24px" viewBox="0 0 20 20" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" fill="#ffffff"><g id="SVGRepo_bgCarrier" stroke-width="0"></g><g id="SVGRepo_tracerCarrier" stroke-linecap="round" stroke-linejoin="round"></g><g id="SVGRepo_iconCarrier"> <title>github [#142]</title> <desc>Created with Sketch.</desc> <defs> </defs> <g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"> <g id="Dribbble-Light-Preview" transform="translate(-140.000000, -7559.000000)" fill="#fff"> <g id="icons" transform="translate(56.000000, 160.000000)"> <path d="M94,7399 C99.523,7399 104,7403.59 104,7409.253 C104,7413.782 101.138,7417.624 97.167,7418.981 C96.66,7419.082 96.48,7418.762 96.48,7418.489 C96.48,7418.151 96.492,7417.047 96.492,7415.675 C96.492,7414.719 96.172,7414.095 95.813,7413.777 C98.04,7413.523 100.38,7412.656 100.38,7408.718 C100.38,7407.598 99.992,7406.684 99.35,7405.966 C99.454,7405.707 99.797,7404.664 99.252,7403.252 C99.252,7403.252 98.414,7402.977 96.505,7404.303 C95.706,7404.076 94.85,7403.962 94,7403.958 C93.15,7403.962 92.295,7404.076 91.497,7404.303 C89.586,7402.977 88.746,7403.252 88.746,7403.252 C88.203,7404.664 88.546,7405.707 88.649,7405.966 C88.01,7406.684 87.619,7407.598 87.619,7408.718 C87.619,7412.646 89.954,7413.526 92.175,7413.785 C91.889,7414.041 91.63,7414.493 91.54,7415.156 C90.97,7415.418 89.522,7415.871 88.63,7414.304 C88.63,7414.304 88.101,7413.319 87.097,7413.247 C87.097,7413.247 86.122,7413.234 87.029,7413.87 C87.029,7413.87 87.684,7414.185 88.139,7415.37 C88.139,7415.37 88.726,7417.2 91.508,7416.58 C91.513,7417.437 91.522,7418.245 91.522,7418.489 C91.522,7418.76 91.338,7419.077 90.839,7418.982 C86.865,7417.627 84,7413.783 84,7409.253 C84,7403.59 88.478,7399 94,7399" id="github-[#142]"> </path> </g> </g> </g> </g></svg>
            </a>
            <a class="social linkdiscreto" href="https://www.instagram.com/pauloroberto.dev" target="_blank">
                <svg width="24px" height="24px" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" stroke="#FFFFFF"><g id="SVGRepo_bgCarrier" stroke-width="0"></g><g id="SVGRepo_tracerCarrier" stroke-linecap="round" stroke-linejoin="round"></g><g id="SVGRepo_iconCarrier"> <path fill-rule="evenodd" clip-rule="evenodd" d="M12 18C15.3137 18 18 15.3137 18 12C18 8.68629 15.3137 6 12 6C8.68629 6 6 8.68629 6 12C6 15.3137 8.68629 18 12 18ZM12 16C14.2091 16 16 14.2091 16 12C16 9.79086 14.2091 8 12 8C9.79086 8 8 9.79086 8 12C8 14.2091 9.79086 16 12 16Z" fill="#FFFFFF"></path> <path d="M18 5C17.4477 5 17 5.44772 17 6C17 6.55228 17.4477 7 18 7C18.5523 7 19 6.55228 19 6C19 5.44772 18.5523 5 18 5Z" fill="#FFFFFF"></path> <path fill-rule="evenodd" clip-rule="evenodd" d="M1.65396 4.27606C1 5.55953 1 7.23969 1 10.6V13.4C1 16.7603 1 18.4405 1.65396 19.7239C2.2292 20.8529 3.14708 21.7708 4.27606 22.346C5.55953 23 7.23969 23 10.6 23H13.4C16.7603 23 18.4405 23 19.7239 22.346C20.8529 21.7708 21.7708 20.8529 22.346 19.7239C23 18.4405 23 16.7603 23 13.4V10.6C23 7.23969 23 5.55953 22.346 4.27606C21.7708 3.14708 20.8529 2.2292 19.7239 1.65396C18.4405 1 16.7603 1 13.4 1H10.6C7.23969 1 5.55953 1 4.27606 1.65396C3.14708 2.2292 2.2292 3.14708 1.65396 4.27606ZM13.4 3H10.6C8.88684 3 7.72225 3.00156 6.82208 3.0751C5.94524 3.14674 5.49684 3.27659 5.18404 3.43597C4.43139 3.81947 3.81947 4.43139 3.43597 5.18404C3.27659 5.49684 3.14674 5.94524 3.0751 6.82208C3.00156 7.72225 3 8.88684 3 10.6V13.4C3 15.1132 3.00156 16.2777 3.0751 17.1779C3.14674 18.0548 3.27659 18.5032 3.43597 18.816C3.81947 19.5686 4.43139 20.1805 5.18404 20.564C5.49684 20.7234 5.94524 20.8533 6.82208 20.9249C7.72225 20.9984 8.88684 21 10.6 21H13.4C15.1132 21 16.2777 20.9984 17.1779 20.9249C18.0548 20.8533 18.5032 20.7234 18.816 20.564C19.5686 20.1805 20.1805 19.5686 20.564 18.816C20.7234 18.5032 20.8533 18.0548 20.9249 17.1779C20.9984 16.2777 21 15.1132 21 13.4V10.6C21 8.88684 20.9984 7.72225 20.9249 6.82208C20.8533 5.94524 20.7234 5.49684 20.564 5.18404C20.1805 4.43139 19.5686 3.81947 18.816 3.43597C18.5032 3.27659 18.0548 3.14674 17.1779 3.0751C16.2777 3.00156 15.1132 3 13.4 3Z" fill="#FFFFFF"></path> </g></svg>
            </a>
            <a class="social linkdiscreto" href="https://www.linkedin.com/in/paulorobertovrc" target="_blank">
                <svg width="27px" height="27px" viewBox="0 0 21 21" fill="none" xmlns="http://www.w3.org/2000/svg"><g id="SVGRepo_bgCarrier" stroke-width="0"></g><g id="SVGRepo_tracerCarrier" stroke-linecap="round" stroke-linejoin="round"></g><g id="SVGRepo_iconCarrier"> <path d="M18.72 3.99997H5.37C5.19793 3.99191 5.02595 4.01786 4.86392 4.07635C4.70189 4.13484 4.55299 4.22471 4.42573 4.34081C4.29848 4.45692 4.19537 4.59699 4.12232 4.75299C4.04927 4.909 4.0077 5.07788 4 5.24997V18.63C4.01008 18.9901 4.15766 19.3328 4.41243 19.5875C4.6672 19.8423 5.00984 19.9899 5.37 20H18.72C19.0701 19.9844 19.4002 19.8322 19.6395 19.5761C19.8788 19.32 20.0082 18.9804 20 18.63V5.24997C20.0029 5.08247 19.9715 4.91616 19.9078 4.76122C19.8441 4.60629 19.7494 4.466 19.6295 4.34895C19.5097 4.23191 19.3672 4.14059 19.2108 4.08058C19.0544 4.02057 18.8874 3.99314 18.72 3.99997ZM9 17.34H6.67V10.21H9V17.34ZM7.89 9.12997C7.72741 9.13564 7.5654 9.10762 7.41416 9.04768C7.26291 8.98774 7.12569 8.89717 7.01113 8.78166C6.89656 8.66615 6.80711 8.5282 6.74841 8.37647C6.6897 8.22474 6.66301 8.06251 6.67 7.89997C6.66281 7.73567 6.69004 7.57169 6.74995 7.41854C6.80986 7.26538 6.90112 7.12644 7.01787 7.01063C7.13463 6.89481 7.2743 6.80468 7.42793 6.74602C7.58157 6.68735 7.74577 6.66145 7.91 6.66997C8.07259 6.66431 8.2346 6.69232 8.38584 6.75226C8.53709 6.8122 8.67431 6.90277 8.78887 7.01828C8.90344 7.13379 8.99289 7.27174 9.05159 7.42347C9.1103 7.5752 9.13699 7.73743 9.13 7.89997C9.13719 8.06427 9.10996 8.22825 9.05005 8.3814C8.99014 8.53456 8.89888 8.6735 8.78213 8.78931C8.66537 8.90513 8.5257 8.99526 8.37207 9.05392C8.21843 9.11259 8.05423 9.13849 7.89 9.12997ZM17.34 17.34H15V13.44C15 12.51 14.67 11.87 13.84 11.87C13.5822 11.8722 13.3313 11.9541 13.1219 12.1045C12.9124 12.2549 12.7546 12.4664 12.67 12.71C12.605 12.8926 12.5778 13.0865 12.59 13.28V17.34H10.29V10.21H12.59V11.21C12.7945 10.8343 13.0988 10.5225 13.4694 10.3089C13.84 10.0954 14.2624 9.98848 14.69 9.99997C16.2 9.99997 17.34 11 17.34 13.13V17.34Z" fill="#FFFFFF"></path> </g></svg>
            </a>
        </p>
</footer>
