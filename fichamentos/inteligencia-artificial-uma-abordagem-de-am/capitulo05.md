# # 2.5 Capítulo 5 (p. 64/77)

![Static Badge](https://img.shields.io/badge/Status-Estudando-grey?labelColor=31A8B8)

## Parte 2 | Modelos preditivos (capítulos 4 a 10)

### 5. Métodos probabilísticos

#### 5.1 Brevíssimas considerações sobre probabilidade[^1]

A probabilidade diz respeito a formas de quantificar a incerteza associada à previsão de resultados de eventos aleatórios. Sob uma perspectiva mais ampla, também à quantidade de surpresa esperada ao observar tais eventos e à quantidade de informação intrínseca àqueles eventos, referida como a entropia[^2] de uma variável aleatória (Ross, 2010).

Consoante a teoria da informação, a entropia de um sistema pode ser compreendida como uma métrica da [im]previsibilidade da ocorrência de determinado evento, tal que quanto maior a quantidade de informações disponíveis para ajustar a expectativa relacionada à observação, tão menor será a entropia e a incerteza associadas; consequentemente, por outro lado, se houver poucas informações prévias para auxiliar na valoração da probabilidade, o aumento da entropia consequentemente importará em maior incerteza.

Isso acontece porque a entropia é inversamente proporcional à interpretabilidade dos dados: cenários de maior entropia reduzem o valor informacional porque aumentam a desorganização (confusão), enquanto os de menor entropia favorecem a organização — e reduzem a confusão — dos dados, tornando-os mais interpretáveis e previsíveis.

De fato, "*uncertainty represents the reliability of our inferences*" (DAVIS et al., 2020, p. 3).

## Principais tópicos

## Referências complementares

AUDY, Jorge L N.; ANDRADE, Gilberto K.; CIDRAL, Alexandre. **Fundamentos de sistemas de informação**. E-book. Porto Alegre: Bookman, 2007.

DAVIS, Josiah; ZHU, Jason; OLDFATHER, Jeremy; MACDONALD, Samual; TRZASKOWSKI, Maciej; KELSEN, Max. 2020. **Quantifying uncertainty in deep learning systems**. Disponível em <https://d1.awsstatic.com/APG/quantifying-uncertainty-in-deep-learning-systems.pdf>. Acesso em 18 mai. 2024.

ROSS, Sheldon. **Probabilidade: um curso moderno com aplicações**. Trad. Alberto Resende De Conti. 8 ed. Porto Alegre: Bookman, 2010.

## Notas

[^1]: Este tópico é autoral, portanto, não consta no texto original do livro.

[^2]: Na teoria da informação, a entropia é uma propriedade que "[...] mede o grau de desordem de um sistema, e a forma de combater essa desordem se dá através da informação." (AUDY; CIDRAL, 2007, p. 37).
